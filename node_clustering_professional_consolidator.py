"""
node_clustering_professional_consolidator.py - å¢å¼ºç‰ˆï¼šé›†æˆæ–°ClothoidCubicæ›²çº¿æ‹Ÿåˆ (å®Œæ•´ä¿®å¤ç‰ˆ)
ä¼˜åŒ–ç‰ˆæœ¬ï¼š
1. å®Œç¾é›†æˆå¢å¼ºç‰ˆClothoidCubic.py
2. å®ç°èµ·ç‚¹â†’å…³é”®èŠ‚ç‚¹åºåˆ—â†’ç»ˆç‚¹çš„å®Œæ•´æ›²çº¿æ‹Ÿåˆ
3. å¢å¼ºçš„èŠ‚ç‚¹åºåˆ—æ’åºå’ŒéªŒè¯
4. ä¸¥æ ¼çš„è½¦è¾†åŠ¨åŠ›å­¦çº¦æŸæ‰§è¡Œ
5. è¯¦ç»†çš„è´¨é‡è¯„ä¼°å’Œç»Ÿè®¡æŠ¥å‘Š
6. ä¿®å¤äº†æ‰€æœ‰å·²çŸ¥çš„åˆå§‹åŒ–å’Œå˜é‡å®šä¹‰é—®é¢˜
7. âœ¨ æ–°å¢ï¼šä¼˜åŒ–ç«¯ç‚¹èšç±»é€»è¾‘ - å¯¹æ‰€æœ‰ç«¯ç‚¹è¿›è¡ŒåŠå¾„ä¸º2çš„èšç±»
"""

import math
import time
import numpy as np
from collections import defaultdict, OrderedDict
from typing import Dict, List, Tuple, Optional, Any, Set
from dataclasses import dataclass, field
from enum import Enum

# å¯¼å…¥eastar.pyä¸­çš„ç®—æ³•ç”¨äºè·¯å¾„é‡å»º
try:
    from eastar import HybridAStarPlanner, MiningOptimizedReedShepp
    EASTAR_AVAILABLE = True
    print("âœ… æˆåŠŸå¯¼å…¥eastar.pyç”¨äºè·¯å¾„é‡å»º")
except ImportError as e:
    EASTAR_AVAILABLE = False
    print(f"âš ï¸ æ— æ³•å¯¼å…¥eastar.py: {e}")

# å¯¼å…¥å¢å¼ºç‰ˆClothoidCubicæ¨¡å— - å®Œæ•´ä¿®å¤ç‰ˆ
ENHANCED_CURVE_FITTING_AVAILABLE = False
CURVE_FITTING_AVAILABLE = False

try:
    from ClothoidCubic import (
        EnhancedClothoidCubicFitter, 
        BackbonePathFitter, 
        VehicleDynamicsConfig,
        CurveSegment,
        CurveType,
        PathQuality
    )
    ENHANCED_CURVE_FITTING_AVAILABLE = True
    CURVE_FITTING_AVAILABLE = True  # ä¿®å¤ï¼šå¢å¼ºç‰ˆå¯ç”¨æ—¶ä¹Ÿè®¾ç½®ä¼ ç»Ÿç‰ˆæœ¬æ ‡è®°
    print("âœ… æˆåŠŸå¯¼å…¥å¢å¼ºç‰ˆClothoidCubicæ¨¡å—")
except ImportError as e:
    ENHANCED_CURVE_FITTING_AVAILABLE = False
    print(f"âš ï¸ æ— æ³•å¯¼å…¥å¢å¼ºç‰ˆClothoidCubic: {e}")
    
    # å›é€€åˆ°åŸç‰ˆ
    try:
        from ClothoidCubic import BackbonePathFitter
        CURVE_FITTING_AVAILABLE = True
        print("âœ… å›é€€åˆ°åŸç‰ˆClothoidCubicæ¨¡å—")
    except ImportError:
        CURVE_FITTING_AVAILABLE = False
        print("âŒ ClothoidCubicæ¨¡å—å®Œå…¨ä¸å¯ç”¨")

class RoadClass(Enum):
    """é“è·¯ç­‰çº§"""
    PRIMARY = "primary"        # ä¸»å¹²é“
    SECONDARY = "secondary"    # æ¬¡å¹²é“  
    SERVICE = "service"        # ä½œä¸šé“

class NodeType(Enum):
    """èŠ‚ç‚¹ç±»å‹"""
    ENDPOINT = "endpoint"      # ç«¯ç‚¹ï¼ˆä¸å¯èšç±»ï¼‰
    WAYPOINT = "waypoint"      # è·¯å¾„ç‚¹ï¼ˆå¯èšç±»ï¼‰
    KEY_NODE = "key_node"      # å…³é”®èŠ‚ç‚¹

@dataclass
class KeyNode:
    """å…³é”®èŠ‚ç‚¹ - å¢å¼ºç‰ˆ"""
    node_id: str
    position: Tuple[float, float, float]
    cluster_center: Tuple[float, float, float]
    
    # èŠ‚ç‚¹å±æ€§
    node_type: NodeType = NodeType.KEY_NODE
    is_endpoint: bool = False
    endpoint_info: Dict = field(default_factory=dict)
    
    # èŠ‚ç‚¹å±æ€§ç»§æ‰¿
    original_nodes: List[Tuple] = field(default_factory=list)
    path_memberships: Set[str] = field(default_factory=set)
    node_importance: float = 1.0
    
    # å·¥ç¨‹å±æ€§
    road_class: RoadClass = RoadClass.SECONDARY
    traffic_capacity: int = 80
    safety_rating: float = 1.0
    
    # è¿æ¥ä¿¡æ¯
    connected_nodes: Set[str] = field(default_factory=set)
    backbone_segments: List[str] = field(default_factory=list)
    
    # èšç±»ä¿¡æ¯
    cluster_info: Dict = field(default_factory=dict)
    
    # å¢å¼ºï¼šæ›²çº¿æ‹Ÿåˆç›¸å…³
    curve_fitting_quality: float = 0.0     # æ›²çº¿æ‹Ÿåˆè´¨é‡
    dynamics_compliance: bool = True        # åŠ¨åŠ›å­¦åˆè§„æ€§
    smoothness_score: float = 0.0          # å¹³æ»‘åº¦åˆ†æ•°
    
    def add_original_node(self, node: Tuple, path_id: str, is_endpoint: bool = False):
        """æ·»åŠ åŸå§‹èŠ‚ç‚¹ä¿¡æ¯"""
        if node not in self.original_nodes:
            self.original_nodes.append(node)
        self.path_memberships.add(path_id)
        
        if is_endpoint:
            self.is_endpoint = True
            self.node_type = NodeType.ENDPOINT
        
        # æ›´æ–°èŠ‚ç‚¹é‡è¦æ€§
        self.node_importance = len(self.path_memberships)
        if self.is_endpoint:
            self.node_importance *= 2
        
        # æ ¹æ®è·¯å¾„æ•°é‡ç¡®å®šé“è·¯ç­‰çº§
        if self.is_endpoint or len(self.path_memberships) >= 4:
            self.road_class = RoadClass.PRIMARY
            self.traffic_capacity = 120
        elif len(self.path_memberships) >= 2:
            self.road_class = RoadClass.SECONDARY
            self.traffic_capacity = 80
        else:
            self.road_class = RoadClass.SERVICE
            self.traffic_capacity = 40
    
    def get_average_position(self) -> Tuple[float, float, float]:
        """è·å–æ‰€æœ‰åŸå§‹èŠ‚ç‚¹çš„å¹³å‡ä½ç½®"""
        if self.is_endpoint:
            return self.position
        
        if not self.original_nodes:
            return self.position
        
        sum_x = sum(node[0] for node in self.original_nodes)
        sum_y = sum(node[1] for node in self.original_nodes)
        sum_z = sum(node[2] if len(node) > 2 else 0 for node in self.original_nodes)
        count = len(self.original_nodes)
        
        return (sum_x / count, sum_y / count, sum_z / count)

@dataclass
class EndpointCluster:
    """ç«¯ç‚¹èšç±»ä¿¡æ¯"""
    cluster_id: str
    representative_position: Tuple[float, float, float]
    original_endpoints: List[Dict]  # åŸå§‹ç«¯ç‚¹ä¿¡æ¯
    endpoint_type: str  # 'start' or 'end'
    cluster_importance: float = 1.0
    merged_path_ids: Set[str] = field(default_factory=set)
    
    def calculate_representative_position(self):
        """è®¡ç®—ä»£è¡¨æ€§ä½ç½®ï¼ˆåŠ æƒå¹³å‡ï¼‰"""
        if not self.original_endpoints:
            return self.representative_position
        
        total_weight = 0
        sum_x = sum_y = sum_z = 0.0
        
        for endpoint in self.original_endpoints:
            # æƒé‡åŸºäºè·¯å¾„æ•°é‡
            weight = len(endpoint.get('paths', {endpoint.get('path_id', '')}))
            
            pos = endpoint['position']
            sum_x += pos[0] * weight
            sum_y += pos[1] * weight
            sum_z += (pos[2] if len(pos) > 2 else 0) * weight
            total_weight += weight
            
            # æ”¶é›†è·¯å¾„ID
            if 'path_id' in endpoint:
                self.merged_path_ids.add(endpoint['path_id'])
            if 'paths' in endpoint:
                self.merged_path_ids.update(endpoint['paths'])
        
        if total_weight > 0:
            self.representative_position = (
                sum_x / total_weight, 
                sum_y / total_weight, 
                sum_z / total_weight
            )
            self.cluster_importance = total_weight
        
        return self.representative_position

@dataclass
class EnhancedConsolidatedBackbonePath:
    """å¢å¼ºçš„æ•´åˆåéª¨å¹²è·¯å¾„"""
    path_id: str
    original_path_id: str
    key_nodes: List[str]  # å…³é”®èŠ‚ç‚¹IDåºåˆ—
    
    # è·¯å¾„å±æ€§
    path_length: float = 0.0
    road_class: RoadClass = RoadClass.SECONDARY
    quality_score: float = 0.7
    
    # åŸå§‹è·¯å¾„ä¿¡æ¯ä¿ç•™
    original_endpoints: Tuple = None
    original_quality: float = 0.7
    endpoint_nodes: Dict = field(default_factory=dict)
    
    # å¢å¼ºï¼šæ›²çº¿æ‹Ÿåˆç»“æœ
    reconstructed_path: List[Tuple] = field(default_factory=list)
    reconstruction_success: bool = False
    curve_segments: List = field(default_factory=list)  # ä¿®å¤ï¼šç§»é™¤CurveSegmentç±»å‹çº¦æŸé¿å…å¯¼å…¥é—®é¢˜
    
    # å¢å¼ºï¼šè´¨é‡å’Œæ€§èƒ½æŒ‡æ ‡
    curve_fitting_method: str = "none"           # æ‹Ÿåˆæ–¹æ³•
    curve_quality_score: float = 0.0             # æ›²çº¿è´¨é‡åˆ†æ•°
    dynamics_compliance_rate: float = 0.0        # åŠ¨åŠ›å­¦åˆè§„ç‡
    smoothness_score: float = 0.0                # å¹³æ»‘åº¦åˆ†æ•°
    safety_score: float = 0.0                    # å®‰å…¨æ€§åˆ†æ•°
    efficiency_score: float = 0.0                # æ•ˆç‡åˆ†æ•°
    
    # å¢å¼ºï¼šè½¦è¾†åŠ¨åŠ›å­¦ç»Ÿè®¡
    max_curvature: float = 0.0                   # æœ€å¤§æ›²ç‡
    avg_curvature: float = 0.0                   # å¹³å‡æ›²ç‡
    max_grade: float = 0.0                       # æœ€å¤§å¡åº¦
    turning_radius_compliance: bool = True       # è½¬å¼¯åŠå¾„åˆè§„
    grade_compliance: bool = True                # å¡åº¦åˆè§„
    
    def calculate_path_properties(self, key_nodes_dict: Dict[str, KeyNode]):
        """è®¡ç®—è·¯å¾„å±æ€§ - å¢å¼ºç‰ˆ"""
        if len(self.key_nodes) < 2:
            return
        
        # è®¡ç®—æ€»é•¿åº¦
        total_length = 0.0
        for i in range(len(self.key_nodes) - 1):
            node1 = key_nodes_dict.get(self.key_nodes[i])
            node2 = key_nodes_dict.get(self.key_nodes[i + 1])
            
            if node1 and node2:
                pos1 = node1.position
                pos2 = node2.position
                distance = math.sqrt(
                    (pos2[0] - pos1[0])**2 + 
                    (pos2[1] - pos1[1])**2
                )
                total_length += distance
        
        self.path_length = total_length
        
        # ç¡®å®šé“è·¯ç­‰çº§ï¼ˆåŸºäºå…³é”®èŠ‚ç‚¹çš„æœ€é«˜ç­‰çº§ï¼‰
        max_importance = 0
        for node_id in self.key_nodes:
            node = key_nodes_dict.get(node_id)
            if node and node.node_importance > max_importance:
                max_importance = node.node_importance
                self.road_class = node.road_class
    
    def update_curve_fitting_results(self, curve_segments: List):
        """æ›´æ–°æ›²çº¿æ‹Ÿåˆç»“æœ - ä¿®å¤ç‰ˆ"""
        if not curve_segments:
            return
        
        self.curve_segments = curve_segments
        
        # è®¡ç®—ç»¼åˆè´¨é‡æŒ‡æ ‡
        total_quality = sum(getattr(seg, 'quality_score', 0.7) for seg in curve_segments)
        self.curve_quality_score = total_quality / len(curve_segments)
        
        # è®¡ç®—ç»¼åˆæ€§èƒ½æŒ‡æ ‡
        total_smoothness = sum(getattr(seg, 'smoothness_score', 0.8) for seg in curve_segments)
        self.smoothness_score = total_smoothness / len(curve_segments)
        
        # è®¡ç®—æ›²ç‡ç»Ÿè®¡
        all_curvatures = []
        for seg in curve_segments:
            if hasattr(seg, 'max_curvature'):
                all_curvatures.append(seg.max_curvature)
        
        if all_curvatures:
            self.max_curvature = max(all_curvatures)
            self.avg_curvature = sum(all_curvatures) / len(all_curvatures)
        
        # æ£€æŸ¥åˆè§„æ€§
        self.dynamics_compliance_rate = sum(
            1 for seg in curve_segments if getattr(seg, 'dynamics_compliance', True)
        ) / len(curve_segments)
        
        self.turning_radius_compliance = all(getattr(seg, 'dynamics_compliance', True) for seg in curve_segments)
        self.grade_compliance = all(getattr(seg, 'grade_compliance', True) for seg in curve_segments)
        
        # ç¡®å®šæ‹Ÿåˆæ–¹æ³•
        if curve_segments:
            self.curve_fitting_method = getattr(curve_segments[0], 'curve_type', 'unknown')

class EnhancedNodeClusteringConsolidator:
    """å¢å¼ºç‰ˆåŸºäºèŠ‚ç‚¹èšç±»çš„ä¸“ä¸šé“è·¯ç½‘ç»œæ•´åˆå™¨"""
    
    def __init__(self, env, config: Dict = None):
        self.env = env
        
        # å¢å¼ºé…ç½®
        self.config = {
            # å¤šè½®èšç±»é…ç½®
            'multi_round_clustering': True,
            'clustering_rounds': [
                {'radius': 6.0, 'name': 'ç¬¬ä¸€è½®'},
                {'radius': 6.0, 'name': 'ç¬¬äºŒè½®'},
                {'radius': 3.0, 'name': 'ç¬¬ä¸‰è½®'}
            ],
            
            # ç«¯ç‚¹ä¿æŠ¤å’Œèšç±»é…ç½® - âœ¨ æ–°å¢ä¼˜åŒ–
            'protect_endpoints': True,
            'endpoint_buffer_radius': 3.0,
            'enable_endpoint_clustering': True,     # âœ¨ å¯ç”¨ç«¯ç‚¹èšç±»
            'endpoint_clustering_radius': 2.0,      # âœ¨ ç«¯ç‚¹èšç±»åŠå¾„
            'min_endpoint_cluster_size': 1,         # âœ¨ ç«¯ç‚¹èšç±»æœ€å°å°ºå¯¸
            'endpoint_merge_threshold': 2.5,        # âœ¨ ç«¯ç‚¹åˆå¹¶é˜ˆå€¼
            
            # èšç±»å‚æ•°
            'min_cluster_size': 1,
            'importance_threshold': 1.5,
            
            # å¢å¼ºï¼šæ›²çº¿æ‹Ÿåˆå‚æ•°
            'enable_enhanced_curve_fitting': ENHANCED_CURVE_FITTING_AVAILABLE,
            'curve_fitting_quality_threshold': 0.7,
            'prefer_complete_curve': True,      # ä¼˜å…ˆä½¿ç”¨å®Œæ•´æ›²çº¿
            'enable_segmented_fallback': True,  # å¯ç”¨åˆ†æ®µå›é€€
            'force_vehicle_dynamics': True,    # å¼ºåˆ¶è½¦è¾†åŠ¨åŠ›å­¦çº¦æŸ
            
            # è½¦è¾†åŠ¨åŠ›å­¦é…ç½® - ä¿®å¤ç‰ˆï¼šåŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ
            'vehicle_dynamics': {
                'vehicle_length': 6.0,
                'vehicle_width': 3.0,
                'turning_radius': 8.0,
                'max_steering_angle': 45.0,
                'max_acceleration': 1.5,
                'max_deceleration': 2.0,
                'max_speed': 15.0,
                'max_grade': 0.15,
                'comfort_lateral_accel': 1.0,
                'safety_margin': 1.5
            },
            
            # è´¨é‡æ§åˆ¶
            'min_reconstruction_quality': 0.5,
            'preserve_original_on_failure': True,
            'enable_quality_reporting': True,
        }
        
        if config:
            self.config.update(config)
        
        # æ ¸å¿ƒæ•°æ®ç»“æ„
        self.original_paths = {}
        self.key_nodes = {}
        self.consolidated_paths = {}
        self.node_clusters = []
        
        # âœ¨ æ–°å¢ï¼šç«¯ç‚¹èšç±»ç›¸å…³æ•°æ®ç»“æ„
        self.original_endpoint_nodes = {}       # åŸå§‹ç«¯ç‚¹ä¿¡æ¯
        self.endpoint_clusters = {}             # ç«¯ç‚¹èšç±»ç»“æœ
        self.clustered_endpoint_nodes = {}      # èšç±»åçš„ç«¯ç‚¹èŠ‚ç‚¹
        self.endpoint_cluster_mapping = {}      # åŸå§‹ç«¯ç‚¹åˆ°èšç±»çš„æ˜ å°„
        
        # ç«¯ç‚¹ä¿¡æ¯ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        self.endpoint_nodes = {}
        self.protected_positions = set()
        
        # å¢å¼ºï¼šæ›²çº¿æ‹Ÿåˆå™¨
        self.enhanced_curve_fitter = None
        self.traditional_path_fitter = None
        self._initialize_curve_fitters()
        
        # å¢å¼ºï¼šè¯¦ç»†ç»Ÿè®¡
        self.consolidation_stats = {
            'original_nodes_count': 0,
            'endpoint_nodes_count': 0,
            'clusterable_nodes_count': 0,
            'key_nodes_count': 0,
            'node_reduction_ratio': 0.0,
            'clustering_time': 0.0,
            'reconstruction_time': 0.0,
            'paths_reconstructed': 0,
            'reconstruction_success_rate': 0.0,
            
            # âœ¨ æ–°å¢ï¼šç«¯ç‚¹èšç±»ç»Ÿè®¡
            'original_endpoints_count': 0,
            'endpoint_clusters_count': 0,
            'endpoint_reduction_ratio': 0.0,
            'endpoint_clustering_time': 0.0,
            'merged_endpoint_paths': 0,
            
            # å¢å¼ºï¼šæ›²çº¿æ‹Ÿåˆç»Ÿè®¡
            'enhanced_curve_fitting_used': 0,
            'complete_curve_success': 0,
            'segmented_curve_success': 0,
            'fallback_reconstruction': 0,
            'avg_curve_quality': 0.0,
            'dynamics_compliance_rate': 0.0,
            'turning_radius_violations': 0,
            'grade_violations': 0,
        }
        
        print(f"ğŸ”§ å¢å¼ºç‰ˆåŸºäºèŠ‚ç‚¹èšç±»çš„ä¸“ä¸šé“è·¯æ•´åˆå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  å¢å¼ºæ›²çº¿æ‹Ÿåˆ: {'âœ…' if self.config['enable_enhanced_curve_fitting'] else 'âŒ'}")
        print(f"  è½¦è¾†åŠ¨åŠ›å­¦çº¦æŸ: {'âœ…' if self.config['force_vehicle_dynamics'] else 'âŒ'}")
        print(f"  âœ¨ ç«¯ç‚¹æ™ºèƒ½èšç±»: {'âœ…' if self.config['enable_endpoint_clustering'] else 'âŒ'}")
        print(f"  ç«¯ç‚¹èšç±»åŠå¾„: {self.config['endpoint_clustering_radius']}m")
        print(f"  è½¬å¼¯åŠå¾„: {self.config['vehicle_dynamics']['turning_radius']}m")
        print(f"  æœ€å¤§å¡åº¦: {self.config['vehicle_dynamics']['max_grade']:.1%}")
    
    def _initialize_curve_fitters(self):
        """åˆå§‹åŒ–æ›²çº¿æ‹Ÿåˆå™¨ - å®Œæ•´ä¿®å¤ç‰ˆ"""
        if self.config['enable_enhanced_curve_fitting'] and ENHANCED_CURVE_FITTING_AVAILABLE:
            try:
                # åˆ›å»ºè½¦è¾†åŠ¨åŠ›å­¦é…ç½® - ä¿®å¤ç‰ˆï¼šç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½æœ‰å€¼
                vehicle_dynamics = self.config['vehicle_dynamics']
                
                vehicle_config = VehicleDynamicsConfig(
                    vehicle_length=vehicle_dynamics.get('vehicle_length', 6.0),
                    vehicle_width=vehicle_dynamics.get('vehicle_width', 3.0),
                    turning_radius=vehicle_dynamics.get('turning_radius', 8.0),
                    max_steering_angle=vehicle_dynamics.get('max_steering_angle', 45.0),
                    max_acceleration=vehicle_dynamics.get('max_acceleration', 1.5),
                    max_deceleration=vehicle_dynamics.get('max_deceleration', 2.0),
                    max_speed=vehicle_dynamics.get('max_speed', 15.0),
                    max_grade=vehicle_dynamics.get('max_grade', 0.15),
                    comfort_lateral_accel=vehicle_dynamics.get('comfort_lateral_accel', 1.0),
                    safety_margin=vehicle_dynamics.get('safety_margin', 1.5)
                )
                
                # åˆ›å»ºå¢å¼ºæ‹Ÿåˆå™¨ - ä¿®å¤ç‰ˆï¼šä½¿ç”¨ä½ç½®å‚æ•°
                self.enhanced_curve_fitter = EnhancedClothoidCubicFitter(
                    vehicle_config,  # ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°
                    self.env        # ç¬¬äºŒä¸ªä½ç½®å‚æ•°
                )
                
                print("âœ… å¢å¼ºæ›²çº¿æ‹Ÿåˆå™¨åˆå§‹åŒ–æˆåŠŸ")
                
            except Exception as e:
                print(f"âš ï¸ å¢å¼ºæ›²çº¿æ‹Ÿåˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                self.config['enable_enhanced_curve_fitting'] = False
        
        # å›é€€åˆ°ä¼ ç»Ÿæ‹Ÿåˆå™¨ - ä¿®å¤ç‰ˆï¼šç¡®ä¿CURVE_FITTING_AVAILABLEå·²å®šä¹‰
        if CURVE_FITTING_AVAILABLE:
            try:
                self.traditional_path_fitter = BackbonePathFitter(env=self.env)
                print("âœ… ä¼ ç»Ÿæ›²çº¿æ‹Ÿåˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ ä¼ ç»Ÿæ›²çº¿æ‹Ÿåˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def consolidate_backbone_network_professional(self, backbone_network):
        """æ‰§è¡Œå¢å¼ºç‰ˆä¸“ä¸šæ•´åˆ"""
        print(f"\nğŸ”§ å¼€å§‹å¢å¼ºç‰ˆåŸºäºèŠ‚ç‚¹èšç±»çš„ä¸“ä¸šé“è·¯ç½‘ç»œæ•´åˆ...")
        start_time = time.time()
        
        try:
            # é˜¶æ®µ1: æå–å’Œåˆ†æåŸå§‹è·¯å¾„
            print(f"\nğŸ“Š é˜¶æ®µ1: æå–å’Œåˆ†æåŸå§‹è·¯å¾„")
            if not self._extract_original_paths(backbone_network):
                print(f"âŒ åŸå§‹è·¯å¾„æå–å¤±è´¥")
                return False
            
            # é˜¶æ®µ2: âœ¨ ä¼˜åŒ–çš„ç«¯ç‚¹è¯†åˆ«å’Œèšç±»
            print(f"\nğŸ”’ é˜¶æ®µ2: âœ¨ ä¼˜åŒ–çš„ç«¯ç‚¹è¯†åˆ«å’Œæ™ºèƒ½èšç±»")
            if not self._identify_and_cluster_endpoints_optimized():
                print(f"âŒ ä¼˜åŒ–ç«¯ç‚¹èšç±»å¤±è´¥")
                return False
            
            # é˜¶æ®µ3: å¤šè½®èŠ‚ç‚¹èšç±»
            print(f"\nğŸ¯ é˜¶æ®µ3: æ‰§è¡Œå¤šè½®èŠ‚ç‚¹èšç±»")
            clustering_start = time.time()
            if not self._perform_multi_round_clustering():
                print(f"âŒ èŠ‚ç‚¹èšç±»å¤±è´¥")
                return False
            self.consolidation_stats['clustering_time'] = time.time() - clustering_start
            
            # é˜¶æ®µ4: ç”Ÿæˆå…³é”®èŠ‚ç‚¹
            print(f"\nâ­ é˜¶æ®µ4: ç”Ÿæˆå…³é”®èŠ‚ç‚¹")
            if not self._generate_key_nodes():
                print(f"âŒ å…³é”®èŠ‚ç‚¹ç”Ÿæˆå¤±è´¥")
                return False
            
            # é˜¶æ®µ5: å¢å¼ºè·¯å¾„é‡å»ºï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰
            print(f"\nğŸ›¤ï¸ é˜¶æ®µ5: å¢å¼ºè·¯å¾„é‡å»º")
            reconstruction_start = time.time()
            if not self._enhanced_reconstruct_backbone_paths():
                print(f"âŒ å¢å¼ºè·¯å¾„é‡å»ºå¤±è´¥")
                return False
            self.consolidation_stats['reconstruction_time'] = time.time() - reconstruction_start
            
            # é˜¶æ®µ6: åº”ç”¨æ•´åˆç»“æœ
            print(f"\nâœ… é˜¶æ®µ6: åº”ç”¨æ•´åˆç»“æœ")
            if not self._apply_consolidation_to_backbone(backbone_network):
                print(f"âŒ æ•´åˆç»“æœåº”ç”¨å¤±è´¥")
                return False
            
            total_time = time.time() - start_time
            self._generate_enhanced_consolidation_report(total_time)
            
            print(f"ğŸ‰ å¢å¼ºç‰ˆä¸“ä¸šé“è·¯ç½‘ç»œæ•´åˆå®Œæˆ!")
            return True
            
        except Exception as e:
            print(f"âŒ å¢å¼ºç‰ˆä¸“ä¸šæ•´åˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    # ==================== âœ¨ æ–°å¢ï¼šä¼˜åŒ–çš„ç«¯ç‚¹èšç±»æ–¹æ³• ====================
    
    def _identify_and_cluster_endpoints_optimized(self) -> bool:
        """âœ¨ ä¼˜åŒ–çš„ç«¯ç‚¹è¯†åˆ«å’Œæ™ºèƒ½èšç±»"""
        print(f"   ğŸ” è¯†åˆ«å’Œåˆ†ææ‰€æœ‰åŸå§‹ç«¯ç‚¹...")
        
        # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰åŸå§‹ç«¯ç‚¹
        start_endpoints = []  # èµ·ç‚¹åˆ—è¡¨
        end_endpoints = []    # ç»ˆç‚¹åˆ—è¡¨
        
        for path_id, path_info in self.original_paths.items():
            nodes = path_info['nodes']
            
            if len(nodes) < 2:
                continue
            
            # èµ·ç‚¹ä¿¡æ¯
            start_point = nodes[0]
            start_endpoint = {
                'position': start_point,
                'type': 'start',
                'path_id': path_id,
                'paths': {path_id},
                'original_id': f"endpoint_start_{path_id}"
            }
            start_endpoints.append(start_endpoint)
            
            # ç»ˆç‚¹ä¿¡æ¯
            end_point = nodes[-1]
            end_endpoint = {
                'position': end_point,
                'type': 'end',
                'path_id': path_id,
                'paths': {path_id},
                'original_id': f"endpoint_end_{path_id}"
            }
            end_endpoints.append(end_endpoint)
        
        total_original_endpoints = len(start_endpoints) + len(end_endpoints)
        self.consolidation_stats['original_endpoints_count'] = total_original_endpoints
        
        print(f"   ğŸ“ æ”¶é›†åˆ°åŸå§‹ç«¯ç‚¹: èµ·ç‚¹{len(start_endpoints)}ä¸ª, ç»ˆç‚¹{len(end_endpoints)}ä¸ª")
        
        # ç¬¬äºŒæ­¥ï¼šåˆ†åˆ«å¯¹èµ·ç‚¹å’Œç»ˆç‚¹è¿›è¡Œèšç±»
        endpoint_clustering_start = time.time()
        
        if self.config['enable_endpoint_clustering']:
            print(f"   ğŸ¯ æ‰§è¡Œç«¯ç‚¹æ™ºèƒ½èšç±» (åŠå¾„: {self.config['endpoint_clustering_radius']}m)...")
            
            # å¯¹èµ·ç‚¹è¿›è¡Œèšç±»
            start_clusters = self._cluster_endpoints(start_endpoints, 'start')
            print(f"   âœ… èµ·ç‚¹èšç±»å®Œæˆ: {len(start_endpoints)} -> {len(start_clusters)} ä¸ªèšç±»")
            
            # å¯¹ç»ˆç‚¹è¿›è¡Œèšç±»
            end_clusters = self._cluster_endpoints(end_endpoints, 'end')
            print(f"   âœ… ç»ˆç‚¹èšç±»å®Œæˆ: {len(end_endpoints)} -> {len(end_clusters)} ä¸ªèšç±»")
            
            # åˆå¹¶èšç±»ç»“æœ
            self.endpoint_clusters.update(start_clusters)
            self.endpoint_clusters.update(end_clusters)
            
            # ç”Ÿæˆèšç±»åçš„ç«¯ç‚¹èŠ‚ç‚¹
            self._generate_clustered_endpoint_nodes()
            
        else:
            print(f"   âš ï¸ ç«¯ç‚¹èšç±»å·²ç¦ç”¨ï¼Œä½¿ç”¨åŸå§‹ç«¯ç‚¹")
            # å¦‚æœç¦ç”¨èšç±»ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ç«¯ç‚¹
            self._use_original_endpoints_as_clusters(start_endpoints + end_endpoints)
        
        endpoint_clustering_time = time.time() - endpoint_clustering_start
        self.consolidation_stats['endpoint_clustering_time'] = endpoint_clustering_time
        
        # ç¬¬ä¸‰æ­¥ï¼šè®¾ç½®ä¿æŠ¤åŒºåŸŸ
        self._setup_endpoint_protection_zones()
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        clustered_endpoints_count = len(self.clustered_endpoint_nodes)
        self.consolidation_stats['endpoint_clusters_count'] = clustered_endpoints_count
        
        if total_original_endpoints > 0:
            self.consolidation_stats['endpoint_reduction_ratio'] = (
                1.0 - clustered_endpoints_count / total_original_endpoints
            )
        
        print(f"   ğŸ“Š ç«¯ç‚¹èšç±»ç»Ÿè®¡:")
        print(f"      åŸå§‹ç«¯ç‚¹: {total_original_endpoints} ä¸ª")
        print(f"      èšç±»åç«¯ç‚¹: {clustered_endpoints_count} ä¸ª")
        print(f"      ç«¯ç‚¹å‡å°‘ç‡: {self.consolidation_stats['endpoint_reduction_ratio']:.1%}")
        print(f"      èšç±»è€—æ—¶: {endpoint_clustering_time:.2f}s")
        
        return True
    
    def _cluster_endpoints(self, endpoints: List[Dict], endpoint_type: str) -> Dict[str, EndpointCluster]:
        """å¯¹æŒ‡å®šç±»å‹çš„ç«¯ç‚¹è¿›è¡Œèšç±»"""
        if not endpoints:
            return {}
        
        clustering_radius = self.config['endpoint_clustering_radius']
        clusters = {}
        visited = set()
        cluster_counter = 0
        
        for i, endpoint in enumerate(endpoints):
            if i in visited:
                continue
            
            # åˆ›å»ºæ–°çš„ç«¯ç‚¹èšç±»
            cluster_id = f"endpoint_cluster_{endpoint_type}_{cluster_counter}"
            endpoint_cluster = EndpointCluster(
                cluster_id=cluster_id,
                representative_position=endpoint['position'],
                original_endpoints=[endpoint],
                endpoint_type=endpoint_type
            )
            
            visited.add(i)
            
            # æŸ¥æ‰¾èšç±»åŠå¾„å†…çš„å…¶ä»–ç«¯ç‚¹
            for j, other_endpoint in enumerate(endpoints):
                if j in visited:
                    continue
                
                distance = self._calculate_distance(endpoint['position'], other_endpoint['position'])
                
                if distance <= clustering_radius:
                    endpoint_cluster.original_endpoints.append(other_endpoint)
                    visited.add(j)
            
            # è®¡ç®—ä»£è¡¨æ€§ä½ç½®å’Œé‡è¦æ€§
            endpoint_cluster.calculate_representative_position()
            
            clusters[cluster_id] = endpoint_cluster
            cluster_counter += 1
            
            # è¾“å‡ºèšç±»ä¿¡æ¯
            if len(endpoint_cluster.original_endpoints) > 1:
                print(f"      ğŸ”— {endpoint_type}èšç±» {cluster_id}: åˆå¹¶äº† {len(endpoint_cluster.original_endpoints)} ä¸ªç«¯ç‚¹")
                self.consolidation_stats['merged_endpoint_paths'] += len(endpoint_cluster.original_endpoints) - 1
        
        return clusters
    
    def _generate_clustered_endpoint_nodes(self):
        """ç”Ÿæˆèšç±»åçš„ç«¯ç‚¹èŠ‚ç‚¹"""
        self.clustered_endpoint_nodes = {}
        self.endpoint_cluster_mapping = {}
        
        for cluster_id, endpoint_cluster in self.endpoint_clusters.items():
            # åˆ›å»ºèšç±»ç«¯ç‚¹èŠ‚ç‚¹
            clustered_endpoint = {
                'id': cluster_id,
                'position': endpoint_cluster.representative_position,
                'type': endpoint_cluster.endpoint_type,
                'cluster_info': {
                    'original_endpoints_count': len(endpoint_cluster.original_endpoints),
                    'merged_path_ids': list(endpoint_cluster.merged_path_ids),
                    'cluster_importance': endpoint_cluster.cluster_importance
                },
                'paths': endpoint_cluster.merged_path_ids.copy(),
                'is_clustered_endpoint': True
            }
            
            self.clustered_endpoint_nodes[cluster_id] = clustered_endpoint
            
            # å»ºç«‹åŸå§‹ç«¯ç‚¹åˆ°èšç±»çš„æ˜ å°„
            for original_endpoint in endpoint_cluster.original_endpoints:
                original_id = original_endpoint['original_id']
                self.endpoint_cluster_mapping[original_id] = cluster_id
        
        # ä¿æŒå‘åå…¼å®¹æ€§ï¼šæ›´æ–°endpoint_nodes
        self.endpoint_nodes = self.clustered_endpoint_nodes.copy()
    
    def _use_original_endpoints_as_clusters(self, all_endpoints: List[Dict]):
        """å¦‚æœç¦ç”¨èšç±»ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ç«¯ç‚¹"""
        self.clustered_endpoint_nodes = {}
        self.endpoint_cluster_mapping = {}
        
        for endpoint in all_endpoints:
            original_id = endpoint['original_id']
            self.clustered_endpoint_nodes[original_id] = endpoint
            self.endpoint_cluster_mapping[original_id] = original_id
        
        # ä¿æŒå‘åå…¼å®¹æ€§
        self.endpoint_nodes = self.clustered_endpoint_nodes.copy()
    
    def _setup_endpoint_protection_zones(self):
        """è®¾ç½®ç«¯ç‚¹ä¿æŠ¤åŒºåŸŸ"""
        self.protected_positions.clear()
        
        for endpoint_node in self.clustered_endpoint_nodes.values():
            position = endpoint_node['position']
            self.protected_positions.add(position)
        
        print(f"   ğŸ›¡ï¸ è®¾ç½®äº† {len(self.protected_positions)} ä¸ªç«¯ç‚¹ä¿æŠ¤åŒºåŸŸ")
    
    # ==================== å¢å¼ºç‰ˆè·¯å¾„é‡å»ºï¼ˆå·²ä¿®å¤ï¼‰ ====================
    
    def _enhanced_reconstruct_backbone_paths(self) -> bool:
        """å¢å¼ºç‰ˆéª¨å¹²è·¯å¾„é‡å»º - æ ¸å¿ƒå®ç°"""
        print(f"   ğŸ¯ å¼€å§‹å¢å¼ºç‰ˆè·¯å¾„é‡å»º...")
        
        self.consolidated_paths = {}
        reconstruction_success_count = 0
        enhanced_fitting_count = 0
        
        for path_id, path_info in self.original_paths.items():
            print(f"\n     ğŸ›¤ï¸ é‡å»ºè·¯å¾„: {path_id}")
            
            # æ„å»ºå®Œæ•´çš„å…³é”®èŠ‚ç‚¹åºåˆ—
            complete_node_sequence = self._build_enhanced_key_node_sequence(path_id)
            
            if not complete_node_sequence or len(complete_node_sequence) < 2:
                print(f"       âŒ æ— æ³•æ„å»ºæœ‰æ•ˆçš„å…³é”®èŠ‚ç‚¹åºåˆ—")
                self._preserve_original_path_as_consolidated(path_id, path_info)
                continue
            
            print(f"       ğŸ“ å…³é”®èŠ‚ç‚¹åºåˆ—: {len(complete_node_sequence)} ä¸ªèŠ‚ç‚¹")
            for i, (node_id, position) in enumerate(complete_node_sequence):
                node_type = "èµ·ç‚¹" if i == 0 else "ç»ˆç‚¹" if i == len(complete_node_sequence)-1 else "å…³é”®èŠ‚ç‚¹"
                print(f"         {i+1}. {node_type} {node_id}: ({position[0]:.1f}, {position[1]:.1f})")
            
            # åˆ›å»ºå¢å¼ºæ•´åˆè·¯å¾„å¯¹è±¡
            consolidated_path = EnhancedConsolidatedBackbonePath(
                path_id=f"enhanced_{path_id}",
                original_path_id=path_id,
                key_nodes=[node_id for node_id, pos in complete_node_sequence],
                original_endpoints=path_info['endpoints'],
                original_quality=path_info['quality']
            )
            
            # è®¡ç®—è·¯å¾„å±æ€§
            consolidated_path.calculate_path_properties(self.key_nodes)
            
            # å°è¯•å¢å¼ºæ›²çº¿æ‹Ÿåˆ
            success = self._attempt_enhanced_curve_fitting(consolidated_path, complete_node_sequence)
            
            if success:
                reconstruction_success_count += 1
                if consolidated_path.curve_fitting_method.startswith('enhanced'):
                    enhanced_fitting_count += 1
                    self.consolidation_stats['enhanced_curve_fitting_used'] += 1
                
                print(f"       âœ… é‡å»ºæˆåŠŸ: æ–¹æ³•={consolidated_path.curve_fitting_method}, "
                      f"è´¨é‡={consolidated_path.curve_quality_score:.2f}")
            else:
                print(f"       âŒ é‡å»ºå¤±è´¥")
            
            self.consolidated_paths[consolidated_path.path_id] = consolidated_path
        
        # æ›´æ–°ç»Ÿè®¡
        self.consolidation_stats['paths_reconstructed'] = reconstruction_success_count
        if len(self.original_paths) > 0:
            self.consolidation_stats['reconstruction_success_rate'] = (
                reconstruction_success_count / len(self.original_paths)
            )
        
        # è®¡ç®—å¹³å‡è´¨é‡
        if self.consolidated_paths:
            total_quality = sum(p.curve_quality_score for p in self.consolidated_paths.values())
            self.consolidation_stats['avg_curve_quality'] = total_quality / len(self.consolidated_paths)
        
        print(f"\n   ğŸ“Š å¢å¼ºé‡å»ºç»Ÿè®¡:")
        print(f"      æ€»è·¯å¾„: {len(self.original_paths)}")
        print(f"      é‡å»ºæˆåŠŸ: {reconstruction_success_count}")
        print(f"      å¢å¼ºæ‹Ÿåˆ: {enhanced_fitting_count}")
        print(f"      æˆåŠŸç‡: {reconstruction_success_count/len(self.original_paths):.1%}")
        print(f"      å¹³å‡è´¨é‡: {self.consolidation_stats['avg_curve_quality']:.2f}")
        
        return reconstruction_success_count > 0
    
    def _build_enhanced_key_node_sequence(self, path_id: str) -> List[Tuple[str, Tuple]]:
        """æ„å»ºå¢å¼ºçš„å…³é”®èŠ‚ç‚¹åºåˆ—"""
        path_info = self.original_paths[path_id]
        path_nodes = path_info['nodes']
        
        if len(path_nodes) < 2:
            return []
        
        sequence = []
        
        # 1. æ‰¾åˆ°èµ·ç‚¹å’Œç»ˆç‚¹çš„å…³é”®èŠ‚ç‚¹ï¼ˆä½¿ç”¨èšç±»åçš„ç«¯ç‚¹ï¼‰
        start_key_node = self._find_closest_clustered_endpoint(path_nodes[0], 'start', path_id)
        end_key_node = self._find_closest_clustered_endpoint(path_nodes[-1], 'end', path_id)
        
        if not start_key_node or not end_key_node:
            print(f"       âš ï¸ æ— æ³•æ‰¾åˆ°ç«¯ç‚¹å¯¹åº”çš„èšç±»ç«¯ç‚¹èŠ‚ç‚¹")
            return []
        
        sequence.append((start_key_node, self.key_nodes[start_key_node].position))
        
        # 2. æ‰¾åˆ°å¹¶æ’åºä¸­é—´å…³é”®èŠ‚ç‚¹
        middle_nodes_with_position = []
        
        for key_node_id, key_node in self.key_nodes.items():
            if (key_node_id != start_key_node and 
                key_node_id != end_key_node and
                path_id in key_node.path_memberships):
                
                # æ‰¾åˆ°è¯¥å…³é”®èŠ‚ç‚¹åœ¨åŸè·¯å¾„ä¸Šçš„æœ€ä½³ä½ç½®ç´¢å¼•
                best_index = self._find_best_path_position(key_node.position, path_nodes)
                if best_index >= 0:
                    middle_nodes_with_position.append((best_index, key_node_id, key_node.position))
        
        # 3. æŒ‰åœ¨åŸè·¯å¾„ä¸Šçš„ä½ç½®æ’åº
        middle_nodes_with_position.sort(key=lambda x: x[0])
        
        # 4. æ·»åŠ ä¸­é—´èŠ‚ç‚¹åˆ°åºåˆ—
        for index, node_id, position in middle_nodes_with_position:
            sequence.append((node_id, position))
        
        # 5. æ·»åŠ ç»ˆç‚¹
        sequence.append((end_key_node, self.key_nodes[end_key_node].position))
        
        # 6. éªŒè¯åºåˆ—çš„ç©ºé—´åˆç†æ€§
        validated_sequence = self._validate_node_sequence(sequence)
        
        return validated_sequence
    
    def _find_closest_clustered_endpoint(self, position: Tuple, endpoint_type: str, path_id: str) -> Optional[str]:
        """æ‰¾åˆ°æœ€æ¥è¿‘ä½ç½®çš„èšç±»ç«¯ç‚¹èŠ‚ç‚¹"""
        min_distance = float('inf')
        closest_node_id = None
        
        for key_node_id, key_node in self.key_nodes.items():
            if not key_node.is_endpoint:
                continue
            
            # æ£€æŸ¥ç«¯ç‚¹ç±»å‹åŒ¹é…
            endpoint_info = key_node.endpoint_info
            if endpoint_info.get('type') != endpoint_type:
                continue
            
            # æ£€æŸ¥è·¯å¾„å…³è”ï¼ˆå¯¹äºèšç±»ç«¯ç‚¹ï¼Œæ£€æŸ¥merged_path_idsï¼‰
            if 'cluster_info' in endpoint_info:
                cluster_info = endpoint_info.get('cluster_info', {})
                merged_path_ids = cluster_info.get('merged_path_ids', [])
                if path_id not in merged_path_ids:
                    continue
            elif path_id not in key_node.path_memberships:
                continue
            
            distance = self._calculate_distance(position, key_node.position)
            if distance < min_distance:
                min_distance = distance
                closest_node_id = key_node_id
        
        # è®¾ç½®åˆç†çš„è·ç¦»é˜ˆå€¼
        threshold = 8.0  # å¢åŠ é˜ˆå€¼ä»¥é€‚åº”èšç±»åçš„ç«¯ç‚¹
        return closest_node_id if min_distance < threshold else None
    
    def _find_best_path_position(self, target_position: Tuple, path_nodes: List[Tuple]) -> int:
        """æ‰¾åˆ°å…³é”®èŠ‚ç‚¹åœ¨åŸè·¯å¾„ä¸Šçš„æœ€ä½³ä½ç½®ç´¢å¼•"""
        min_distance = float('inf')
        best_index = -1
        
        for i, path_node in enumerate(path_nodes):
            distance = self._calculate_distance(target_position, path_node)
            if distance < min_distance:
                min_distance = distance
                best_index = i
        
        # è®¾ç½®åˆç†çš„è·ç¦»é˜ˆå€¼
        return best_index if min_distance < 15.0 else -1
    
    def _validate_node_sequence(self, sequence: List[Tuple[str, Tuple]]) -> List[Tuple[str, Tuple]]:
        """éªŒè¯å¹¶ä¿®å¤èŠ‚ç‚¹åºåˆ— - å·²å–æ¶ˆè·³è¿‡è¿‡è¿‘èŠ‚ç‚¹çš„é€»è¾‘"""
        if len(sequence) < 2:
            return sequence
        
        validated = [sequence[0]]  # èµ·ç‚¹
        
        for i in range(1, len(sequence)):
            current_node = sequence[i]
            prev_node = validated[-1]
            
            # è®¡ç®—è·ç¦»ï¼ˆä»…ç”¨äºä¿¡æ¯è¾“å‡ºï¼Œä¸å†ç”¨äºè·³è¿‡èŠ‚ç‚¹ï¼‰
            distance = self._calculate_distance(prev_node[1], current_node[1])
            
            # åŸæ¥çš„è·³è¿‡è¿‡è¿‘èŠ‚ç‚¹é€»è¾‘å·²è¢«ç§»é™¤
            # ç°åœ¨ä¿ç•™æ‰€æœ‰èŠ‚ç‚¹ï¼ŒåŒ…æ‹¬è¿‡è¿‘çš„èŠ‚ç‚¹
            print(f"       ğŸ“ ä¿ç•™èŠ‚ç‚¹: {current_node[0]} (è·ç¦»: {distance:.2f}m)")
            
            # æ£€æŸ¥è§’åº¦å˜åŒ–çš„åˆç†æ€§ï¼ˆä»…ç”¨äºä¿¡æ¯è¾“å‡ºï¼‰
            if len(validated) >= 2:
                angle_change = self._calculate_angle_change(
                    validated[-2][1], validated[-1][1], current_node[1]
                )
                
                # å¦‚æœè§’åº¦å˜åŒ–è¿‡å¤§ï¼Œè¾“å‡ºè­¦å‘Šä½†ä¸è·³è¿‡
                if angle_change > math.pi * 0.8:  # 144åº¦
                    print(f"       âš ï¸ å¤§è§’åº¦è½¬å¼¯: {current_node[0]} (è§’åº¦: {math.degrees(angle_change):.1f}Â°)")
            
            # æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹åˆ°éªŒè¯åºåˆ—ä¸­
            validated.append(current_node)
        
        print(f"       âœ… èŠ‚ç‚¹åºåˆ—éªŒè¯å®Œæˆ: ä¿ç•™äº†æ‰€æœ‰ {len(validated)} ä¸ªèŠ‚ç‚¹")
        return validated
    
    def _calculate_angle_change(self, p1: Tuple, p2: Tuple, p3: Tuple) -> float:
        """è®¡ç®—ä¸‰ç‚¹çš„è§’åº¦å˜åŒ–"""
        v1 = (p2[0] - p1[0], p2[1] - p1[1])
        v2 = (p3[0] - p2[0], p3[1] - p2[1])
        
        len1 = math.sqrt(v1[0]**2 + v1[1]**2)
        len2 = math.sqrt(v2[0]**2 + v2[1]**2)
        
        if len1 < 1e-6 or len2 < 1e-6:
            return 0.0
        
        cos_angle = (v1[0]*v2[0] + v1[1]*v2[1]) / (len1 * len2)
        cos_angle = max(-1, min(1, cos_angle))
        
        return math.acos(cos_angle)
    
    def _attempt_enhanced_curve_fitting(self, consolidated_path: EnhancedConsolidatedBackbonePath, 
                                      node_sequence: List[Tuple[str, Tuple]]) -> bool:
        """å°è¯•å¢å¼ºæ›²çº¿æ‹Ÿåˆ"""
        
        # æå–ä½ç½®å’ŒIDåˆ—è¡¨
        positions = [pos for node_id, pos in node_sequence]
        node_ids = [node_id for node_id, pos in node_sequence]
        
        # ç¡®ä¿3Dåæ ‡
        positions_3d = []
        for pos in positions:
            if len(pos) >= 3:
                positions_3d.append(pos)
            else:
                positions_3d.append((pos[0], pos[1], 0.0))
        
        # ç­–ç•¥1: å¢å¼ºç‰ˆæ›²çº¿æ‹Ÿåˆ
        if self.config['enable_enhanced_curve_fitting'] and self.enhanced_curve_fitter:
            print(f"       ğŸ¨ å°è¯•å¢å¼ºç‰ˆæ›²çº¿æ‹Ÿåˆ...")
            
            try:
                # ç¡®å®šé“è·¯ç­‰çº§
                road_class = consolidated_path.road_class.value if consolidated_path.road_class else 'secondary'
                
                # æ‰§è¡Œæ‹Ÿåˆ
                curve_segments = self.enhanced_curve_fitter.fit_path_between_nodes(
                    key_nodes=positions_3d,
                    key_node_ids=node_ids,
                    road_class=road_class
                )
                
                if curve_segments:
                    # å¤„ç†æ‹Ÿåˆç»“æœ
                    success = self._process_enhanced_fitting_result(consolidated_path, curve_segments)
                    
                    if success:
                        print(f"       âœ… å¢å¼ºæ›²çº¿æ‹ŸåˆæˆåŠŸ")
                        self.consolidation_stats['complete_curve_success'] += 1
                        return True
                    else:
                        print(f"       âš ï¸ å¢å¼ºæ‹Ÿåˆè´¨é‡ä¸è¾¾æ ‡")
                
            except Exception as e:
                print(f"       âš ï¸ å¢å¼ºæ›²çº¿æ‹Ÿåˆå¼‚å¸¸: {e}")
        
        # ç­–ç•¥2: ä¼ ç»Ÿæ›²çº¿æ‹Ÿåˆå›é€€
        if self.traditional_path_fitter:
            print(f"       ğŸ”„ å›é€€åˆ°ä¼ ç»Ÿæ›²çº¿æ‹Ÿåˆ...")
            
            try:
                road_class = consolidated_path.road_class.value if consolidated_path.road_class else 'secondary'
                
                fitted_path = self.traditional_path_fitter.reconstruct_path_with_curve_fitting(
                    key_node_positions=positions_3d,
                    key_node_ids=node_ids,
                    road_class=road_class,
                    path_quality=consolidated_path.original_quality
                )
                
                if fitted_path and len(fitted_path) >= 2:
                    consolidated_path.reconstructed_path = fitted_path
                    consolidated_path.reconstruction_success = True
                    consolidated_path.curve_fitting_method = "traditional_curve_fitting"
                    consolidated_path.curve_quality_score = self._evaluate_path_quality(fitted_path)
                    
                    print(f"       âœ… ä¼ ç»Ÿæ›²çº¿æ‹ŸåˆæˆåŠŸ")
                    self.consolidation_stats['segmented_curve_success'] += 1
                    return True
                
            except Exception as e:
                print(f"       âš ï¸ ä¼ ç»Ÿæ›²çº¿æ‹Ÿåˆå¼‚å¸¸: {e}")
        
        # ç­–ç•¥3: é«˜è´¨é‡ç›´çº¿è¿æ¥å›é€€
        print(f"       ğŸ†˜ ä½¿ç”¨é«˜è´¨é‡ç›´çº¿è¿æ¥...")
        self._create_high_quality_fallback_path(consolidated_path, positions_3d)
        self.consolidation_stats['fallback_reconstruction'] += 1
        
        return True
    
    def _process_enhanced_fitting_result(self, consolidated_path: EnhancedConsolidatedBackbonePath, 
                                       curve_segments: List) -> bool:
        """å¤„ç†å¢å¼ºæ‹Ÿåˆç»“æœ"""
        if not curve_segments:
            return False
        
        # æ›´æ–°æ›²çº¿æ‹Ÿåˆç»“æœ
        consolidated_path.update_curve_fitting_results(curve_segments)
        
        # è½¬æ¢ä¸ºè·¯å¾„æ ¼å¼
        complete_path = self.enhanced_curve_fitter.convert_to_path_format(curve_segments)
        consolidated_path.reconstructed_path = complete_path
        consolidated_path.reconstruction_success = True
        
        # è®¾ç½®æ‹Ÿåˆæ–¹æ³•
        if len(curve_segments) == 1:
            consolidated_path.curve_fitting_method = f"enhanced_complete_{getattr(curve_segments[0], 'curve_type', 'unknown')}"
        else:
            consolidated_path.curve_fitting_method = f"enhanced_segmented_{getattr(curve_segments[0], 'curve_type', 'unknown')}"
        
        # è´¨é‡æ£€æŸ¥
        quality_threshold = self.config['curve_fitting_quality_threshold']
        if consolidated_path.curve_quality_score >= quality_threshold:
            return True
        else:
            print(f"       âš ï¸ è´¨é‡ä¸è¾¾æ ‡: {consolidated_path.curve_quality_score:.2f} < {quality_threshold}")
            return False
    
    def _evaluate_path_quality(self, path: List[Tuple]) -> float:
        """è¯„ä¼°è·¯å¾„è´¨é‡"""
        if not path or len(path) < 2:
            return 0.0
        
        # ç®€åŒ–çš„è´¨é‡è¯„ä¼°
        # 1. è·¯å¾„æ•ˆç‡
        path_length = sum(
            math.sqrt((path[i][0] - path[i-1][0])**2 + (path[i][1] - path[i-1][1])**2)
            for i in range(1, len(path))
        )
        
        direct_distance = math.sqrt(
            (path[-1][0] - path[0][0])**2 + (path[-1][1] - path[0][1])**2
        )
        
        efficiency = direct_distance / path_length if path_length > 0 else 0
        
        # 2. å¹³æ»‘åº¦ï¼ˆç®€åŒ–ï¼‰
        smoothness = 0.8  # é»˜è®¤å€¼
        
        return min(1.0, efficiency * 0.6 + smoothness * 0.4)
    
    def _create_high_quality_fallback_path(self, consolidated_path: EnhancedConsolidatedBackbonePath, 
                                         positions: List[Tuple]):
        """åˆ›å»ºé«˜è´¨é‡çš„å›é€€è·¯å¾„"""
        if len(positions) < 2:
            return
        
        fallback_path = []
        
        for i in range(len(positions) - 1):
            start_pos = positions[i]
            end_pos = positions[i + 1]
            
            # è®¡ç®—è·ç¦»
            distance = self._calculate_distance(start_pos, end_pos)
            
            # é«˜å¯†åº¦é‡‡æ ·ï¼ˆæ¯0.5ç±³ä¸€ä¸ªç‚¹ï¼‰
            num_points = max(3, int(distance / 0.5))
            
            for j in range(num_points):
                if i > 0 and j == 0:
                    continue  # è·³è¿‡é‡å¤èµ·ç‚¹
                
                t = j / (num_points - 1) if num_points > 1 else 0
                
                x = start_pos[0] + t * (end_pos[0] - start_pos[0])
                y = start_pos[1] + t * (end_pos[1] - start_pos[1])
                z = start_pos[2] + t * (end_pos[2] - start_pos[2]) if len(start_pos) > 2 else 0
                
                fallback_path.append((x, y, z))
        
        consolidated_path.reconstructed_path = fallback_path
        consolidated_path.reconstruction_success = True
        consolidated_path.curve_fitting_method = "high_quality_fallback"
        consolidated_path.curve_quality_score = 0.6  # åŸºç¡€è´¨é‡åˆ†æ•°
    
    # ==================== ä»åŸç‰ˆç»§æ‰¿çš„æ ¸å¿ƒæ–¹æ³• ====================
    
    def _extract_original_paths(self, backbone_network) -> bool:
        """æå–åŸå§‹è·¯å¾„æ•°æ®"""
        print(f"   æå–åŸå§‹éª¨å¹²è·¯å¾„...")
        
        if not hasattr(backbone_network, 'bidirectional_paths'):
            print(f"   âŒ éª¨å¹²ç½‘ç»œç¼ºå°‘bidirectional_pathså±æ€§")
            return False
        
        self.original_paths = {}
        original_nodes_count = 0
        
        for path_id, path_data in backbone_network.bidirectional_paths.items():
            if not hasattr(path_data, 'forward_path') or not path_data.forward_path:
                print(f"   âš ï¸ è·¯å¾„ {path_id} ç¼ºå°‘forward_path")
                continue
            
            # æå–è·¯å¾„èŠ‚ç‚¹
            forward_path = path_data.forward_path
            processed_nodes = []
            
            for node in forward_path:
                if len(node) >= 3:
                    processed_nodes.append((float(node[0]), float(node[1]), float(node[2])))
                elif len(node) == 2:
                    processed_nodes.append((float(node[0]), float(node[1]), 0.0))
                else:
                    continue
            
            if len(processed_nodes) < 2:
                print(f"   âš ï¸ è·¯å¾„ {path_id} æœ‰æ•ˆèŠ‚ç‚¹å¤ªå°‘")
                continue
            
            # å­˜å‚¨åŸå§‹è·¯å¾„ä¿¡æ¯
            self.original_paths[path_id] = {
                'path_id': path_id,
                'nodes': processed_nodes,
                'quality': getattr(path_data, 'quality', 0.7),
                'path_data': path_data,
                'endpoints': (processed_nodes[0], processed_nodes[-1])
            }
            
            original_nodes_count += len(processed_nodes)
            
        self.consolidation_stats['original_nodes_count'] = original_nodes_count
        
        print(f"   âœ… æå–å®Œæˆ: {len(self.original_paths)} æ¡è·¯å¾„, {original_nodes_count} ä¸ªèŠ‚ç‚¹")
        return len(self.original_paths) > 0
    
    def _perform_multi_round_clustering(self) -> bool:
        """æ‰§è¡Œå¤šè½®èŠ‚ç‚¹èšç±»"""
        print(f"   å¼€å§‹å¤šè½®èŠ‚ç‚¹èšç±»åˆ†æ...")
        
        # æ”¶é›†æ‰€æœ‰å¯èšç±»èŠ‚ç‚¹
        clusterable_nodes = []
        
        for path_id, path_info in self.original_paths.items():
            nodes = path_info['nodes']
            
            # è·³è¿‡èµ·ç‚¹å’Œç»ˆç‚¹ï¼Œåªå¤„ç†ä¸­é—´èŠ‚ç‚¹
            for i, node in enumerate(nodes):
                if i == 0 or i == len(nodes) - 1:
                    continue
                
                # æ£€æŸ¥ä¿æŠ¤åŠå¾„
                if self._is_near_protected_position(node, self.config['endpoint_buffer_radius']):
                    continue
                
                clusterable_nodes.append({
                    'position': node,
                    'path_id': path_id,
                    'node_index': i,
                    'paths': {path_id}
                })
        
        self.consolidation_stats['clusterable_nodes_count'] = len(clusterable_nodes)
        print(f"   å¯èšç±»èŠ‚ç‚¹æ•°: {len(clusterable_nodes)}")
        
        if len(clusterable_nodes) == 0:
            self.node_clusters = []
            return True
        
        # æ‰§è¡Œå¤šè½®èšç±»
        current_nodes = clusterable_nodes
        
        for round_idx, round_config in enumerate(self.config['clustering_rounds']):
            radius = round_config['radius']
            round_name = round_config['name']
            
            print(f"\n   === {round_name} (åŠå¾„: {radius}m) ===")
            print(f"   è¾“å…¥èŠ‚ç‚¹æ•°: {len(current_nodes)}")
            
            # æ‰§è¡Œæœ¬è½®èšç±»
            round_clusters = self._perform_single_round_clustering(current_nodes, radius)
            
            print(f"   ç”Ÿæˆèšç±»æ•°: {len(round_clusters)}")
            
            # å¦‚æœæ˜¯æœ€åä¸€è½®ï¼Œä¿å­˜ç»“æœ
            if round_idx == len(self.config['clustering_rounds']) - 1:
                self.node_clusters = round_clusters
            else:
                # å°†èšç±»è½¬æ¢ä¸ºä¸‹ä¸€è½®çš„è¾“å…¥
                current_nodes = self._convert_clusters_to_nodes(round_clusters)
                print(f"   è¾“å‡ºèŠ‚ç‚¹æ•°: {len(current_nodes)}")
        
        print(f"\n   âœ… å¤šè½®èšç±»å®Œæˆ: {len(self.node_clusters)} ä¸ªæœ€ç»ˆèšç±»")
        return True
    
    def _is_near_protected_position(self, position: Tuple, radius: float = None) -> bool:
        """æ£€æŸ¥ä½ç½®æ˜¯å¦åœ¨ä¿æŠ¤åŒºåŸŸå†…"""
        if radius is None:
            radius = self.config['endpoint_buffer_radius']
        
        for protected_pos in self.protected_positions:
            distance = self._calculate_distance(position, protected_pos)
            if distance < radius:
                return True
        
        return False
    
    def _perform_single_round_clustering(self, nodes: List[Dict], radius: float) -> List[Dict]:
        """æ‰§è¡Œå•è½®èšç±»"""
        clusters = []
        visited = set()
        
        for i, node in enumerate(nodes):
            if i in visited:
                continue
            
            # åˆ›å»ºæ–°èšç±»
            cluster = {
                'center': node['position'],
                'nodes': [node],
                'paths': node['paths'].copy()
            }
            visited.add(i)
            
            # æŸ¥æ‰¾èšç±»åŠå¾„å†…çš„å…¶ä»–èŠ‚ç‚¹
            for j, other_node in enumerate(nodes):
                if j in visited:
                    continue
                
                distance = self._calculate_distance(node['position'], other_node['position'])
                
                if distance <= radius:
                    cluster['nodes'].append(other_node)
                    cluster['paths'].update(other_node['paths'])
                    visited.add(j)
            
            clusters.append(cluster)
        
        return clusters
    
    def _convert_clusters_to_nodes(self, clusters: List[Dict]) -> List[Dict]:
        """å°†èšç±»è½¬æ¢ä¸ºèŠ‚ç‚¹"""
        nodes = []
        
        for cluster_idx, cluster in enumerate(clusters):
            # è®¡ç®—èšç±»ä¸­å¿ƒ
            center = self._calculate_weighted_cluster_center(cluster['nodes'])
            
            # åˆå¹¶è·¯å¾„ä¿¡æ¯
            all_paths = set()
            for node in cluster['nodes']:
                all_paths.update(node['paths'])
            
            # åˆ›å»ºä»£è¡¨èŠ‚ç‚¹
            representative_node = {
                'position': center,
                'paths': all_paths,
                'original_cluster': cluster,
                'cluster_size': len(cluster['nodes']),
                'is_intersection': len(all_paths) > 1,
                'cluster_id': f"cluster_{cluster_idx}"
            }
            
            nodes.append(representative_node)
        
        return nodes
    
    def _calculate_weighted_cluster_center(self, nodes: List[Dict]) -> Tuple[float, float, float]:
        """è®¡ç®—åŠ æƒèšç±»ä¸­å¿ƒ"""
        if not nodes:
            return (0.0, 0.0, 0.0)
        
        sum_x = 0.0
        sum_y = 0.0
        sum_z = 0.0
        total_weight = 0
        
        for node in nodes:
            pos = node['position']
            weight = len(node['paths'])
            
            sum_x += pos[0] * weight
            sum_y += pos[1] * weight
            sum_z += (pos[2] if len(pos) > 2 else 0) * weight
            total_weight += weight
        
        if total_weight > 0:
            return (sum_x / total_weight, sum_y / total_weight, sum_z / total_weight)
        else:
            sum_x = sum(node['position'][0] for node in nodes)
            sum_y = sum(node['position'][1] for node in nodes)
            sum_z = sum(node['position'][2] if len(node['position']) > 2 else 0 for node in nodes)
            count = len(nodes)
            return (sum_x / count, sum_y / count, sum_z / count)
    
    def _generate_key_nodes(self) -> bool:
        """ç”Ÿæˆå…³é”®èŠ‚ç‚¹"""
        print(f"   ç”Ÿæˆå…³é”®èŠ‚ç‚¹...")
        
        self.key_nodes = {}
        node_id_counter = 0
        
        # 1. é¦–å…ˆæ·»åŠ æ‰€æœ‰èšç±»åçš„ç«¯ç‚¹ä½œä¸ºå…³é”®èŠ‚ç‚¹
        for endpoint_id, endpoint_info in self.clustered_endpoint_nodes.items():
            key_node = KeyNode(
                node_id=endpoint_id,
                position=endpoint_info['position'],
                cluster_center=endpoint_info['position'],
                node_type=NodeType.ENDPOINT,
                is_endpoint=True
            )
            
            key_node.endpoint_info = endpoint_info
            key_node.path_memberships = endpoint_info['paths']
            
            # âœ¨ è®¾ç½®ç«¯ç‚¹é‡è¦æ€§ï¼ˆåŸºäºåˆå¹¶çš„è·¯å¾„æ•°é‡ï¼‰
            if 'cluster_info' in endpoint_info:
                cluster_info = endpoint_info['cluster_info']
                merged_paths_count = len(cluster_info.get('merged_path_ids', []))
                key_node.node_importance = max(10.0, merged_paths_count * 5.0)  # èšç±»ç«¯ç‚¹æ›´é‡è¦
                key_node.cluster_info = cluster_info
            else:
                key_node.node_importance = 10.0
            
            key_node.road_class = RoadClass.PRIMARY
            key_node.traffic_capacity = 150
            
            self.key_nodes[endpoint_id] = key_node
        
        print(f"   æ·»åŠ äº† {len(self.clustered_endpoint_nodes)} ä¸ªèšç±»ç«¯ç‚¹ä½œä¸ºå…³é”®èŠ‚ç‚¹")
        
        # 2. å¤„ç†èšç±»ç”Ÿæˆçš„å…³é”®èŠ‚ç‚¹
        for cluster_idx, cluster in enumerate(self.node_clusters):
            # è®¡ç®—èšç±»ä¸­å¿ƒ
            cluster_center = self._calculate_weighted_cluster_center(cluster['nodes'])
            
            # åˆ›å»ºå…³é”®èŠ‚ç‚¹
            key_node_id = f"key_node_{node_id_counter}"
            key_node = KeyNode(
                node_id=key_node_id,
                position=cluster_center,
                cluster_center=cluster_center,
                node_type=NodeType.KEY_NODE
            )
            
            # è¯¦ç»†ä¿¡æ¯ç»§æ‰¿
            unique_paths = set()
            all_original_positions = []
            
            for node in cluster['nodes']:
                original_pos = node['position']
                all_original_positions.append(original_pos)
                
                for path_id in node['paths']:
                    unique_paths.add(path_id)
                    key_node.add_original_node(original_pos, path_id, is_endpoint=False)
            
            # æ·»åŠ èšç±»ç»Ÿè®¡ä¿¡æ¯
            key_node.cluster_info = {
                'original_node_count': len(all_original_positions),
                'path_count': len(unique_paths),
                'cluster_size': len(cluster['nodes']),
                'is_intersection': len(unique_paths) > 1
            }
            
            self.key_nodes[key_node_id] = key_node
            node_id_counter += 1
            
            if len(unique_paths) > 1:
                print(f"     äº¤å‰å…³é”®èŠ‚ç‚¹ {key_node_id}: {len(unique_paths)}æ¡è·¯å¾„äº¤æ±‡")
        
        self.consolidation_stats['key_nodes_count'] = len(self.key_nodes)
        
        # è®¡ç®—èŠ‚ç‚¹å‡å°‘æ¯”ä¾‹
        original_count = self.consolidation_stats['original_nodes_count']
        key_count = len(self.key_nodes)
        
        if original_count > 0:
            self.consolidation_stats['node_reduction_ratio'] = (
                1.0 - key_count / original_count
            )
        
        print(f"   âœ… å…³é”®èŠ‚ç‚¹ç”Ÿæˆå®Œæˆ: {len(self.key_nodes)} ä¸ª")
        print(f"   èŠ‚ç‚¹å‡å°‘: {original_count} -> {key_count} "
              f"({self.consolidation_stats['node_reduction_ratio']:.1%})")
        
        return len(self.key_nodes) > 0
    
    def _preserve_original_path_as_consolidated(self, path_id: str, path_info: Dict):
        """å°†åŸè·¯å¾„ä¿ç•™ä¸ºæ•´åˆè·¯å¾„"""
        consolidated_path = EnhancedConsolidatedBackbonePath(
            path_id=f"preserved_{path_id}",
            original_path_id=path_id,
            key_nodes=[],
            original_endpoints=path_info['endpoints'],
            original_quality=path_info['quality']
        )
        
        # ç›´æ¥ä½¿ç”¨åŸè·¯å¾„
        consolidated_path.reconstructed_path = path_info['nodes']
        consolidated_path.reconstruction_success = True
        consolidated_path.curve_quality_score = path_info['quality']
        consolidated_path.curve_fitting_method = "preserved_original"
        consolidated_path.path_length = self._calculate_path_length(path_info['nodes'])
        
        self.consolidated_paths[consolidated_path.path_id] = consolidated_path
    
    def _apply_consolidation_to_backbone(self, backbone_network) -> bool:
        """åº”ç”¨æ•´åˆç»“æœåˆ°éª¨å¹²ç½‘ç»œ"""
        print(f"   åº”ç”¨æ•´åˆç»“æœåˆ°éª¨å¹²ç½‘ç»œ...")
        
        # åˆ›å»ºæ–°çš„éª¨å¹²è·¯å¾„å­—å…¸
        new_bidirectional_paths = {}
        
        for consolidated_path in self.consolidated_paths.values():
            if not consolidated_path.reconstructed_path:
                continue
            
            # åˆ›å»ºæ–°çš„éª¨å¹²è·¯å¾„å¯¹è±¡
            new_path_object = self._create_enhanced_backbone_path_object(consolidated_path)
            
            if new_path_object:
                new_bidirectional_paths[consolidated_path.path_id] = new_path_object
                print(f"     âœ… æ•´åˆè·¯å¾„: {consolidated_path.path_id} "
                      f"({len(consolidated_path.reconstructed_path)} èŠ‚ç‚¹, "
                      f"æ–¹æ³•: {consolidated_path.curve_fitting_method})")
        
        # æ›´æ–°éª¨å¹²ç½‘ç»œ
        backbone_network.bidirectional_paths = new_bidirectional_paths
        
        # æ·»åŠ æ•´åˆä¿¡æ¯åˆ°éª¨å¹²ç½‘ç»œ
        backbone_network.consolidation_info = {
            'consolidation_applied': True,
            'consolidation_type': 'enhanced_node_clustering_professional',
            'key_nodes': self.key_nodes,
            'consolidation_stats': self.consolidation_stats,
            'enhanced_curve_fitting': self.config['enable_enhanced_curve_fitting'],
            'vehicle_dynamics_config': self.config['vehicle_dynamics'],
            'original_paths_count': len(self.original_paths),
            'consolidated_paths_count': len(new_bidirectional_paths),
            
            # âœ¨ æ–°å¢ï¼šç«¯ç‚¹èšç±»ä¿¡æ¯
            'endpoint_clustering_applied': self.config['enable_endpoint_clustering'],
            'endpoint_clusters': self.endpoint_clusters,
            'clustered_endpoint_nodes': self.clustered_endpoint_nodes,
            'endpoint_cluster_mapping': self.endpoint_cluster_mapping,
        }
        
        print(f"   âœ… æ•´åˆç»“æœå·²åº”ç”¨: {len(new_bidirectional_paths)} æ¡å¢å¼ºè·¯å¾„")
        return True
    
    def _create_enhanced_backbone_path_object(self, consolidated_path: EnhancedConsolidatedBackbonePath):
        """åˆ›å»ºå¢å¼ºçš„éª¨å¹²è·¯å¾„å¯¹è±¡"""
        
        class EnhancedConsolidatedBackbonePathObject:
            def __init__(self, consolidated_path, key_nodes_dict):
                self.path_id = consolidated_path.path_id
                self.original_path_id = consolidated_path.original_path_id
                
                # è·¯å¾„æ•°æ®
                self.forward_path = consolidated_path.reconstructed_path
                self.reverse_path = list(reversed(consolidated_path.reconstructed_path))
                
                # å±æ€§
                self.length = consolidated_path.path_length
                self.quality = consolidated_path.curve_quality_score
                self.planner_used = 'enhanced_node_clustering_professional_consolidator'
                self.created_time = time.time()
                
                # å¢å¼ºå±æ€§
                self.is_professional_design = True
                self.is_consolidated = True
                self.is_enhanced_curve_fitted = True
                self.consolidation_method = 'enhanced_multi_round_node_clustering'
                self.curve_fitting_method = consolidated_path.curve_fitting_method
                self.key_nodes = consolidated_path.key_nodes
                self.road_class = consolidated_path.road_class.value
                
                # è½¦è¾†åŠ¨åŠ›å­¦å±æ€§
                self.dynamics_compliance_rate = consolidated_path.dynamics_compliance_rate
                self.max_curvature = consolidated_path.max_curvature
                self.avg_curvature = consolidated_path.avg_curvature
                self.max_grade = consolidated_path.max_grade
                self.turning_radius_compliance = consolidated_path.turning_radius_compliance
                self.grade_compliance = consolidated_path.grade_compliance
                
                # è´¨é‡æŒ‡æ ‡
                self.curve_quality_score = consolidated_path.curve_quality_score
                self.smoothness_score = consolidated_path.smoothness_score
                self.safety_score = consolidated_path.safety_score
                self.efficiency_score = consolidated_path.efficiency_score
                
                # ç«¯ç‚¹ä¿¡æ¯
                if consolidated_path.original_endpoints:
                    start_pos, end_pos = consolidated_path.original_endpoints
                    self.point_a = {'type': 'loading', 'id': 0, 'position': start_pos}
                    self.point_b = {'type': 'unloading', 'id': 0, 'position': end_pos}
                else:
                    self.point_a = {'type': 'loading', 'id': 0, 'position': self.forward_path[0]}
                    self.point_b = {'type': 'unloading', 'id': 0, 'position': self.forward_path[-1]}
                
                # è´Ÿè½½ç®¡ç†
                self.usage_count = 0
                self.current_load = 0
                self.max_capacity = 5
                
                # è´¨é‡è¿½è¸ª
                self.quality_history = [self.quality]
                self.last_quality_update = time.time()
            
            def get_path(self, from_point_type, from_point_id, to_point_type, to_point_id):
                return self.forward_path
            
            def increment_usage(self):
                self.usage_count += 1
            
            def add_vehicle(self, vehicle_id):
                self.current_load += 1
            
            def remove_vehicle(self, vehicle_id):
                self.current_load = max(0, self.current_load - 1)
            
            def get_load_factor(self):
                return self.current_load / self.max_capacity if self.max_capacity > 0 else 0
        
        return EnhancedConsolidatedBackbonePathObject(consolidated_path, self.key_nodes)
    
    def _generate_enhanced_consolidation_report(self, total_time: float):
        """ç”Ÿæˆå¢å¼ºæ•´åˆæŠ¥å‘Š"""
        print(f"\nğŸ“Š ç”Ÿæˆå¢å¼ºæ•´åˆæŠ¥å‘Š...")
        
        # è®¡ç®—å¢å¼ºç»Ÿè®¡
        if self.consolidated_paths:
            # åŠ¨åŠ›å­¦åˆè§„æ€§ç»Ÿè®¡
            total_dynamics_compliance = sum(
                p.dynamics_compliance_rate for p in self.consolidated_paths.values()
            )
            self.consolidation_stats['dynamics_compliance_rate'] = (
                total_dynamics_compliance / len(self.consolidated_paths)
            )
            
            # è½¬å¼¯åŠå¾„è¿è§„ç»Ÿè®¡
            self.consolidation_stats['turning_radius_violations'] = sum(
                1 for p in self.consolidated_paths.values() 
                if not p.turning_radius_compliance
            )
            
            # å¡åº¦è¿è§„ç»Ÿè®¡
            self.consolidation_stats['grade_violations'] = sum(
                1 for p in self.consolidated_paths.values() 
                if not p.grade_compliance
            )
        
        # é“è·¯ç­‰çº§ç»Ÿè®¡
        road_class_dist = {'primary': 0, 'secondary': 0, 'service': 0}
        endpoint_count = 0
        
        for key_node in self.key_nodes.values():
            road_class_dist[key_node.road_class.value] += 1
            if key_node.is_endpoint:
                endpoint_count += 1
        
        # æ›´æ–°ç»Ÿè®¡
        self.consolidation_stats.update({
            'total_time': total_time,
            'road_class_distribution': road_class_dist,
            'protected_endpoints': endpoint_count,
            'clustering_rounds': len(self.config['clustering_rounds']),
            
            # å¢å¼ºç»Ÿè®¡
            'enhanced_fitting_enabled': self.config['enable_enhanced_curve_fitting'],
            'vehicle_dynamics_enforced': self.config['force_vehicle_dynamics'],
            'curve_fitting_success_rate': (
                (self.consolidation_stats['enhanced_curve_fitting_used'] + 
                 self.consolidation_stats['segmented_curve_success']) / 
                max(1, len(self.consolidated_paths))
            ),
            
            # âœ¨ ç«¯ç‚¹èšç±»ç»Ÿè®¡
            'endpoint_clustering_enabled': self.config['enable_endpoint_clustering'],
            'endpoint_clustering_radius': self.config['endpoint_clustering_radius'],
        })
        
        print(f"   âœ… å¢å¼ºæ•´åˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        print(f"      æ€»è€—æ—¶: {total_time:.2f}s")
        print(f"      âœ¨ ç«¯ç‚¹å‡å°‘: {self.consolidation_stats['original_endpoints_count']} -> {self.consolidation_stats['endpoint_clusters_count']}")
        print(f"      âœ¨ ç«¯ç‚¹åˆå¹¶: {self.consolidation_stats['merged_endpoint_paths']} æ¡è·¯å¾„")
        print(f"      å¢å¼ºæ‹Ÿåˆä½¿ç”¨: {self.consolidation_stats['enhanced_curve_fitting_used']} æ¬¡")
        print(f"      åŠ¨åŠ›å­¦åˆè§„ç‡: {self.consolidation_stats['dynamics_compliance_rate']:.1%}")
        print(f"      è½¬å¼¯åŠå¾„è¿è§„: {self.consolidation_stats['turning_radius_violations']} æ¡è·¯å¾„")
        print(f"      å¡åº¦è¿è§„: {self.consolidation_stats['grade_violations']} æ¡è·¯å¾„")
    
    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def _calculate_distance(self, pos1: Tuple, pos2: Tuple) -> float:
        """è®¡ç®—ä¸¤ç‚¹é—´è·ç¦»"""
        return math.sqrt((pos2[0] - pos1[0])**2 + (pos2[1] - pos1[1])**2)
    
    def _calculate_path_length(self, path: List[Tuple]) -> float:
        """è®¡ç®—è·¯å¾„é•¿åº¦"""
        if not path or len(path) < 2:
            return 0.0
        
        length = 0.0
        for i in range(len(path) - 1):
            length += self._calculate_distance(path[i], path[i + 1])
        return length
    
    # ==================== å¢å¼ºç‰ˆå…¬å…±æ¥å£æ–¹æ³• ====================
    
    def get_consolidation_stats(self) -> Dict:
        """è·å–æ•´åˆç»Ÿè®¡ä¿¡æ¯"""
        return self.consolidation_stats.copy()
    
    def get_key_nodes_info(self) -> Dict:
        """è·å–å…³é”®èŠ‚ç‚¹ä¿¡æ¯"""
        nodes_info = {}
        for node_id, key_node in self.key_nodes.items():
            nodes_info[node_id] = {
                'position': key_node.position,
                'importance': key_node.node_importance,
                'road_class': key_node.road_class.value,
                'path_memberships': list(key_node.path_memberships),
                'traffic_capacity': key_node.traffic_capacity,
                'original_nodes_count': len(key_node.original_nodes),
                'is_endpoint': key_node.is_endpoint,
                'node_type': key_node.node_type.value,
                'cluster_info': getattr(key_node, 'cluster_info', {}),
                
                # å¢å¼ºå±æ€§
                'curve_fitting_quality': getattr(key_node, 'curve_fitting_quality', 0.0),
                'dynamics_compliance': getattr(key_node, 'dynamics_compliance', True),
                'smoothness_score': getattr(key_node, 'smoothness_score', 0.0),
                
                # âœ¨ ç«¯ç‚¹èšç±»ä¿¡æ¯
                'endpoint_info': getattr(key_node, 'endpoint_info', {}),
            }
        return nodes_info
    
    def get_consolidated_paths_info(self) -> Dict:
        """è·å–æ•´åˆè·¯å¾„ä¿¡æ¯"""
        paths_info = {}
        for path_id, consolidated_path in self.consolidated_paths.items():
            paths_info[path_id] = {
                'original_path_id': consolidated_path.original_path_id,
                'key_nodes': consolidated_path.key_nodes,
                'path_length': consolidated_path.path_length,
                'road_class': consolidated_path.road_class.value,
                'reconstruction_success': consolidated_path.reconstruction_success,
                'node_count': len(consolidated_path.reconstructed_path),
                
                # å¢å¼ºå±æ€§
                'curve_fitting_method': consolidated_path.curve_fitting_method,
                'curve_quality_score': consolidated_path.curve_quality_score,
                'dynamics_compliance_rate': consolidated_path.dynamics_compliance_rate,
                'smoothness_score': consolidated_path.smoothness_score,
                'max_curvature': consolidated_path.max_curvature,
                'avg_curvature': consolidated_path.avg_curvature,
                'turning_radius_compliance': consolidated_path.turning_radius_compliance,
                'grade_compliance': consolidated_path.grade_compliance,
            }
        return paths_info
    
    def get_endpoint_clustering_info(self) -> Dict:
        """âœ¨ è·å–ç«¯ç‚¹èšç±»ä¿¡æ¯"""
        return {
            'endpoint_clustering_enabled': self.config['enable_endpoint_clustering'],
            'endpoint_clustering_radius': self.config['endpoint_clustering_radius'],
            'original_endpoints_count': self.consolidation_stats['original_endpoints_count'],
            'endpoint_clusters_count': self.consolidation_stats['endpoint_clusters_count'],
            'endpoint_reduction_ratio': self.consolidation_stats['endpoint_reduction_ratio'],
            'merged_endpoint_paths': self.consolidation_stats['merged_endpoint_paths'],
            'endpoint_clusters': {
                cluster_id: {
                    'representative_position': cluster.representative_position,
                    'endpoint_type': cluster.endpoint_type,
                    'original_endpoints_count': len(cluster.original_endpoints),
                    'cluster_importance': cluster.cluster_importance,
                    'merged_path_ids': list(cluster.merged_path_ids)
                }
                for cluster_id, cluster in self.endpoint_clusters.items()
            },
            'clustered_endpoint_nodes': self.clustered_endpoint_nodes,
            'endpoint_cluster_mapping': self.endpoint_cluster_mapping
        }
    
    def get_curve_fitting_statistics(self) -> Dict:
        """è·å–æ›²çº¿æ‹Ÿåˆç»Ÿè®¡ä¿¡æ¯"""
        if self.enhanced_curve_fitter:
            return self.enhanced_curve_fitter.get_fitting_statistics()
        else:
            return {}

# å‘åå…¼å®¹æ€§
NodeClusteringConsolidator = EnhancedNodeClusteringConsolidator

# ä¾¿æ·åˆ›å»ºå‡½æ•°
def create_enhanced_node_clustering_consolidator(env, config=None):
    """åˆ›å»ºå¢å¼ºç‰ˆåŸºäºèŠ‚ç‚¹èšç±»çš„ä¸“ä¸šé“è·¯æ•´åˆå™¨"""
    default_config = {
        'enable_enhanced_curve_fitting': True,
        'force_vehicle_dynamics': True,
        'curve_fitting_quality_threshold': 0.7,
        'prefer_complete_curve': True,
        
        # âœ¨ ç«¯ç‚¹èšç±»é…ç½®
        'enable_endpoint_clustering': True,
        'endpoint_clustering_radius': 2.0,
        'endpoint_merge_threshold': 2.5,
        
        'vehicle_dynamics': {
            'turning_radius': 8.0,
            'max_grade': 0.15,
            'safety_margin': 1.5
        }
    }
    
    if config:
        default_config.update(config)
    
    return EnhancedNodeClusteringConsolidator(env, default_config)

def apply_enhanced_consolidation(backbone_network, env, mode='professional'):
    """åº”ç”¨å¢å¼ºç‰ˆæ•´åˆåˆ°éª¨å¹²ç½‘ç»œ"""
    mode_configs = {
        'professional': {
            'curve_fitting_quality_threshold': 0.7,
            'force_vehicle_dynamics': True,
            'prefer_complete_curve': True,
            'enable_endpoint_clustering': True,
            'endpoint_clustering_radius': 2.0,
            'vehicle_dynamics': {
                'turning_radius': 8.0,
                'max_grade': 0.12,  # æ›´ä¸¥æ ¼çš„å¡åº¦è¦æ±‚
                'safety_margin': 2.0
            }
        },
        'balanced': {
            'curve_fitting_quality_threshold': 0.7,
            'force_vehicle_dynamics': True,
            'prefer_complete_curve': True,
            'enable_endpoint_clustering': True,
            'endpoint_clustering_radius': 2.0,
            'vehicle_dynamics': {
                'turning_radius': 8.0,
                'max_grade': 0.15,
                'safety_margin': 1.5
            }
        },
        'performance': {
            'curve_fitting_quality_threshold': 0.6,
            'force_vehicle_dynamics': True,
            'prefer_complete_curve': False,
            'enable_endpoint_clustering': True,
            'endpoint_clustering_radius': 2.5,
            'vehicle_dynamics': {
                'turning_radius': 7.0,
                'max_grade': 0.18,
                'safety_margin': 1.2
            }
        }
    }
    
    config = mode_configs.get(mode, mode_configs['balanced'])
    
    consolidator = EnhancedNodeClusteringConsolidator(env, config)
    success = consolidator.consolidate_backbone_network_professional(backbone_network)
    
    if success:
        stats = consolidator.get_consolidation_stats()
        endpoint_info = consolidator.get_endpoint_clustering_info()
        
        print(f"âœ… å¢å¼ºç‰ˆä¸“ä¸šé“è·¯æ•´åˆæˆåŠŸ (æ¨¡å¼: {mode})")
        print(f"   èŠ‚ç‚¹å‡å°‘: {stats['node_reduction_ratio']:.1%}")
        print(f"   âœ¨ ç«¯ç‚¹å‡å°‘: {endpoint_info['endpoint_reduction_ratio']:.1%}")
        print(f"   æ›²çº¿æ‹ŸåˆæˆåŠŸç‡: {stats.get('curve_fitting_success_rate', 0):.1%}")
        print(f"   åŠ¨åŠ›å­¦åˆè§„ç‡: {stats.get('dynamics_compliance_rate', 0):.1%}")
        return consolidator
    else:
        print(f"âŒ å¢å¼ºç‰ˆä¸“ä¸šé“è·¯æ•´åˆå¤±è´¥")
        return None

# æ›¿æ¢åŸæœ‰çš„æ•´åˆå™¨
OptimizedProfessionalMiningRoadConsolidator = EnhancedNodeClusteringConsolidator