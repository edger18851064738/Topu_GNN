#!/usr/bin/env python3
"""
MAGECè¯„ä¼°å’Œå¯è§†åŒ–å·¥å…·
å®ç°ä¸è®ºæ–‡åŸºå‡†ç®—æ³•çš„å¯¹æ¯”ï¼šAHPA, SEBS, CBLS
æä¾›å®Œæ•´çš„æ€§èƒ½è¯„ä¼°å’Œå¯è§†åŒ–

åŸºäºè®ºæ–‡: "Graph Neural Network-based Multi-agent Reinforcement Learning 
for Resilient Distributed Coordination of Multi-Robot Systems"

ä½¿ç”¨æ–¹æ³•:
    1. äº¤äº’å¼æ¨¡å¼ (æ¨èæ–°æ‰‹ç”¨æˆ·):
       python visualize.py
    
    2. å‘½ä»¤è¡Œæ¨¡å¼:
       python visualize.py --magec_model path/to/model.pth --output_dir results/
    
    3. æ‰¹å¤„ç†æ¨¡å¼:
       python visualize.py --batch --magec_model path/to/model.pth
    
    4. å¿«é€Ÿæµ‹è¯•:
       python visualize.py --quick_test --animate

ç‰¹æ€§:
    âœ… è‡ªåŠ¨å‘ç°è®­ç»ƒå¥½çš„æ¨¡å‹
    âœ… äº¤äº’å¼é…ç½®å‘å¯¼
    âœ… å¤šç§åŸºå‡†ç®—æ³•å¯¹æ¯”
    âœ… åŠ¨ç”»å¯è§†åŒ–å·¡é€»è¿‡ç¨‹
    âœ… è¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š
    âœ… å¤šç§å¹²æ‰°åœºæ™¯æµ‹è¯•
"""

import sys
import os
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)
import time
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib.patches import Circle, FancyBboxPatch
import networkx as nx
import torch
import argparse
import logging
import json
import random
import time
from collections import deque
from typing import Dict, List, Tuple, Optional
import copy

# å¯¼å…¥æˆ‘ä»¬çš„MAGECå®ç°
try:
    from demo import (
        MAGECActor, MAGECCritic, OptimizedPatrollingEnvironment,
        MAGECTrainer, create_official_config
    )
except ImportError:
    print("è¯·ç¡®ä¿demo.pyåœ¨åŒä¸€ç›®å½•ä¸‹")
    sys.exit(1)

logger = logging.getLogger(__name__)

class BaselinePatrollingAlgorithm:
    """åŸºå‡†å·¡é€»ç®—æ³•åŸºç±»"""
    
    def __init__(self, env, name="Baseline"):
        self.env = env
        self.name = name
        self.reset()
    
    def reset(self):
        """é‡ç½®ç®—æ³•çŠ¶æ€"""
        pass
    
    def select_actions(self, observations):
        """é€‰æ‹©åŠ¨ä½œ - å­ç±»å®ç°"""
        raise NotImplementedError
    
    def update(self, observations, actions, rewards, next_observations, dones):
        """æ›´æ–°ç®—æ³•çŠ¶æ€ - å­ç±»å¯é€‰å®ç°"""
        pass

class AHPAAlgorithm(BaselinePatrollingAlgorithm):
    """
    AHPA (Adaptive Heuristic-based Patrolling Algorithm)
    è®ºæ–‡åŸºå‡†ç®—æ³•ä¹‹ä¸€
    """
    
    def __init__(self, env):
        super().__init__(env, "AHPA")
        self.agent_plans = [deque() for _ in range(env.num_agents)]
        self.plan_horizons = [5] * env.num_agents
    
    def reset(self):
        """é‡ç½®AHPAçŠ¶æ€"""
        self.agent_plans = [deque() for _ in range(self.env.num_agents)]
    
    def select_actions(self, observations):
        """AHPAåŠ¨ä½œé€‰æ‹©"""
        actions = []
        
        for agent_id in range(self.env.num_agents):
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è§„åˆ’
            if len(self.agent_plans[agent_id]) == 0:
                self._plan_for_agent(agent_id)
            
            # æ‰§è¡Œè®¡åˆ’ä¸­çš„ä¸‹ä¸€ä¸ªåŠ¨ä½œ
            if len(self.agent_plans[agent_id]) > 0:
                target_node = self.agent_plans[agent_id].popleft()
                action = self._get_action_to_node(agent_id, target_node)
            else:
                action = 0
            
            actions.append(action)
        
        return np.array(actions)
    
    def _plan_for_agent(self, agent_id):
        """ä¸ºæ™ºèƒ½ä½“ç”Ÿæˆå·¡é€»è®¡åˆ’"""
        current_pos = self.env.agent_positions[agent_id]
        
        # é€‰æ‹©æœ€é«˜é—²ç½®æ—¶é—´çš„æœªè®¿é—®èŠ‚ç‚¹
        unvisited_nodes = []
        for node in range(self.env.num_nodes):
            if node not in self.env.agent_positions:
                idleness = self.env.node_idleness[node]
                unvisited_nodes.append((idleness, node))
        
        # æŒ‰é—²ç½®æ—¶é—´æ’åº
        unvisited_nodes.sort(reverse=True)
        
        # é€‰æ‹©å‰å‡ ä¸ªèŠ‚ç‚¹ä½œä¸ºç›®æ ‡
        plan = []
        for _, node in unvisited_nodes[:self.plan_horizons[agent_id]]:
            plan.append(node)
        
        self.agent_plans[agent_id] = deque(plan)
    
    def _get_action_to_node(self, agent_id, target_node):
        """è®¡ç®—åˆ°è¾¾ç›®æ ‡èŠ‚ç‚¹çš„åŠ¨ä½œ"""
        current_pos = self.env.agent_positions[agent_id]
        neighbors = self.env.neighbor_dict.get(current_pos, [])
        
        if target_node in neighbors:
            return neighbors.index(target_node)
        elif neighbors:
            # ä½¿ç”¨æœ€çŸ­è·¯å¾„å¯¼èˆª
            try:
                path = nx.shortest_path(self.env.graph, current_pos, target_node)
                if len(path) > 1:
                    next_node = path[1]
                    if next_node in neighbors:
                        return neighbors.index(next_node)
            except:
                pass
            
            # éšæœºé€‰æ‹©é‚»å±…
            return random.randint(0, len(neighbors) - 1)
        
        return 0

class SEBSAlgorithm(BaselinePatrollingAlgorithm):
    """
    SEBS (State Exchange Bayesian Strategy)
    è®ºæ–‡åŸºå‡†ç®—æ³•ä¹‹ä¸€
    """
    
    def __init__(self, env):
        super().__init__(env, "SEBS")
        self.agent_beliefs = [{} for _ in range(env.num_agents)]
        self.communication_range = 2
    
    def reset(self):
        """é‡ç½®SEBSçŠ¶æ€"""
        self.agent_beliefs = [{} for _ in range(self.env.num_agents)]
    
    def select_actions(self, observations):
        """SEBSåŠ¨ä½œé€‰æ‹©"""
        # æ›´æ–°ä¿¡å¿µ
        self._update_beliefs()
        
        # æ™ºèƒ½ä½“é—´ä¿¡æ¯äº¤æ¢
        self._exchange_information()
        
        actions = []
        for agent_id in range(self.env.num_agents):
            action = self._select_action_for_agent(agent_id)
            actions.append(action)
        
        return np.array(actions)
    
    def _update_beliefs(self):
        """æ›´æ–°æ™ºèƒ½ä½“ä¿¡å¿µ"""
        for agent_id in range(self.env.num_agents):
            agent_pos = self.env.agent_positions[agent_id]
            
            # æ›´æ–°å¯è§‚å¯ŸèŠ‚ç‚¹çš„ä¿¡å¿µ
            for node in range(self.env.num_nodes):
                if node == agent_pos:
                    self.agent_beliefs[agent_id][node] = {
                        'idleness': self.env.node_idleness[node],
                        'last_update': self.env.current_step,
                        'confidence': 1.0
                    }
    
    def _exchange_information(self):
        """æ™ºèƒ½ä½“é—´ä¿¡æ¯äº¤æ¢"""
        for i in range(self.env.num_agents):
            for j in range(i + 1, self.env.num_agents):
                # æ£€æŸ¥æ˜¯å¦åœ¨é€šä¿¡èŒƒå›´å†…
                pos_i = self.env.agent_positions[i]
                pos_j = self.env.agent_positions[j]
                
                try:
                    distance = nx.shortest_path_length(self.env.graph, pos_i, pos_j)
                    if distance <= self.communication_range:
                        # äº¤æ¢ä¿¡å¿µ
                        self._merge_beliefs(i, j)
                        self._merge_beliefs(j, i)
                except:
                    continue
    
    def _merge_beliefs(self, receiver_id, sender_id):
        """åˆå¹¶ä¿¡å¿µ"""
        receiver_beliefs = self.agent_beliefs[receiver_id]
        sender_beliefs = self.agent_beliefs[sender_id]
        
        for node, belief in sender_beliefs.items():
            if node not in receiver_beliefs:
                receiver_beliefs[node] = belief.copy()
            else:
                if belief['last_update'] > receiver_beliefs[node]['last_update']:
                    receiver_beliefs[node] = belief.copy()
    
    def _select_action_for_agent(self, agent_id):
        """ä¸ºæ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ"""
        current_pos = self.env.agent_positions[agent_id]
        neighbors = self.env.neighbor_dict.get(current_pos, [])
        
        if not neighbors:
            return 0
        
        # åŸºäºä¿¡å¿µé€‰æ‹©æœ€ä½³é‚»å±…
        best_action = 0
        best_utility = -float('inf')
        
        for i, neighbor in enumerate(neighbors):
            utility = self._calculate_utility(agent_id, neighbor)
            if utility > best_utility:
                best_utility = utility
                best_action = i
        
        return best_action
    
    def _calculate_utility(self, agent_id, node):
        """è®¡ç®—èŠ‚ç‚¹æ•ˆç”¨"""
        beliefs = self.agent_beliefs[agent_id]
        
        if node in beliefs:
            believed_idleness = beliefs[node]['idleness']
            confidence = beliefs[node]['confidence']
            return believed_idleness * confidence
        else:
            return self.env.node_idleness[node]

class CBLSAlgorithm(BaselinePatrollingAlgorithm):
    """
    CBLS (Concurrent Bayesian Learning Strategy)
    è®ºæ–‡åŸºå‡†ç®—æ³•ä¹‹ä¸€
    """
    
    def __init__(self, env):
        super().__init__(env, "CBLS")
        self.q_values = np.zeros((env.num_agents, env.num_nodes, env.num_nodes))
        self.learning_rate = 0.1
        self.epsilon = 0.1
        self.decay_rate = 0.99
    
    def reset(self):
        """é‡ç½®CBLSçŠ¶æ€"""
        self.q_values = np.zeros((self.env.num_agents, self.env.num_nodes, self.env.num_nodes))
        self.epsilon = 0.1
    
    def select_actions(self, observations):
        """CBLSåŠ¨ä½œé€‰æ‹©"""
        actions = []
        
        for agent_id in range(self.env.num_agents):
            action = self._select_action_for_agent(agent_id)
            actions.append(action)
        
        return np.array(actions)
    
    def _select_action_for_agent(self, agent_id):
        """epsilon-greedyåŠ¨ä½œé€‰æ‹©"""
        current_pos = self.env.agent_positions[agent_id]
        neighbors = self.env.neighbor_dict.get(current_pos, [])
        
        if not neighbors:
            return 0
        
        if random.random() < self.epsilon:
            return random.randint(0, len(neighbors) - 1)
        else:
            best_action = 0
            best_value = -float('inf')
            
            for i, neighbor in enumerate(neighbors):
                q_value = self.q_values[agent_id, current_pos, neighbor]
                if q_value > best_value:
                    best_value = q_value
                    best_action = i
            
            return best_action
    
    def update(self, observations, actions, rewards, next_observations, dones):
        """æ›´æ–°Qå€¼"""
        for agent_id in range(self.env.num_agents):
            if agent_id < len(actions):
                current_pos = self.env.agent_positions[agent_id]
                neighbors = self.env.neighbor_dict.get(current_pos, [])
                
                if actions[agent_id] < len(neighbors):
                    next_pos = neighbors[actions[agent_id]]
                    reward = rewards[agent_id] if agent_id < len(rewards) else 0
                    
                    old_q = self.q_values[agent_id, current_pos, next_pos]
                    
                    future_neighbors = self.env.neighbor_dict.get(next_pos, [])
                    max_future_q = 0
                    if future_neighbors:
                        max_future_q = max(self.q_values[agent_id, next_pos, n] 
                                         for n in future_neighbors)
                    
                    new_q = old_q + self.learning_rate * (reward + 0.9 * max_future_q - old_q)
                    self.q_values[agent_id, current_pos, next_pos] = new_q
        
        self.epsilon *= self.decay_rate

class RandomAlgorithm(BaselinePatrollingAlgorithm):
    """éšæœºç®—æ³•ä½œä¸ºåŸºå‡†"""
    
    def __init__(self, env):
        super().__init__(env, "Random")
    
    def select_actions(self, observations):
        """éšæœºé€‰æ‹©åŠ¨ä½œ"""
        actions = []
        
        for agent_id in range(self.env.num_agents):
            current_pos = self.env.agent_positions[agent_id]
            neighbors = self.env.neighbor_dict.get(current_pos, [])
            
            if neighbors:
                action = random.randint(0, len(neighbors) - 1)
            else:
                action = 0
            
            actions.append(action)
        
        return np.array(actions)

class MAGECEvaluator:
    """MAGECè¯„ä¼°å™¨ - å¯¹æ¯”ä¸åŒç®—æ³•çš„æ€§èƒ½"""
    
    def __init__(self, env_config):
        self.env_config = env_config
        self.algorithms = {}
        self.results = {}
    
    def register_algorithm(self, name, algorithm):
        """æ³¨å†Œç®—æ³•"""
        self.algorithms[name] = algorithm
        logger.info(f"æ³¨å†Œç®—æ³•: {name}")
    
    def load_magec_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„MAGECæ¨¡å‹"""
        try:
            checkpoint = torch.load(model_path, map_location='cpu')
            config = checkpoint.get('config', create_official_config())
            
            # åˆ›å»ºç¯å¢ƒ
            env = OptimizedPatrollingEnvironment(
                graph_name=config['environment']['graph_name'],
                num_agents=config['environment']['num_agents'],
                observation_radius=config['environment']['observation_radius'],
                max_cycles=config['environment']['max_cycles'],
                agent_speed=config['environment']['agent_speed'],
                action_method=config['environment']['action_method']
            )
            
            # åˆ›å»ºç½‘ç»œ
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            actor = MAGECActor(
                node_features=config['network']['node_features'],
                edge_features=config['network']['edge_features'],
                hidden_size=config['network']['gnn_hidden_size'],
                num_layers=config['network']['gnn_layers'],
                max_neighbors=env.get_max_neighbors(),
                dropout=config['network']['gnn_dropout'],
                use_skip_connections=config['network']['gnn_skip_connections']
            ).to(device)
            
            # åŠ è½½æƒé‡
            actor.load_state_dict(checkpoint['actor_state_dict'])
            actor.eval()
            
            # åˆ›å»ºMAGECç®—æ³•åŒ…è£…å™¨
            class MAGECAlgorithmWrapper(BaselinePatrollingAlgorithm):
                def __init__(self, actor, device):
                    self.actor = actor
                    self.device = device
                    self.name = "MAGEC"
                    self.step_count = 0
                    self.env = None  # ç¯å¢ƒå¼•ç”¨ï¼Œç¨åè®¾ç½®                
                def select_actions(self, observations):
                    """é€‰æ‹©åŠ¨ä½œ - ä¿®å¤ç‰ˆ"""
                    try:
                        self.step_count += 1
                        debug = (self.step_count <= 3)  # å‰3æ¬¡æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
                        
                        if debug:
                            print(f"ğŸ¤– MAGECåŠ¨ä½œé€‰æ‹© #{self.step_count}")
                        
                        with torch.no_grad():
                            # è·å–æ™ºèƒ½ä½“ä½ç½®
                            agent_indices = []
                            for i, obs in enumerate(observations):
                                if hasattr(obs, 'agent_pos'):
                                    agent_pos = obs.agent_pos.item()
                                    agent_indices.append(agent_pos)
                                else:
                                    agent_indices.append(0)
                            
                            if debug:
                                print(f"   æ™ºèƒ½ä½“ä½ç½®: {agent_indices}")
                            
                            # ç§»åŠ¨åˆ°è®¾å¤‡
                            observations_device = []
                            for obs in observations:
                                obs_device = obs.clone() if hasattr(obs, 'clone') else obs
                                obs_device.x = obs_device.x.to(self.device)
                                obs_device.edge_index = obs_device.edge_index.to(self.device)
                                if hasattr(obs_device, 'edge_attr') and obs_device.edge_attr is not None:
                                    obs_device.edge_attr = obs_device.edge_attr.to(self.device)
                                if hasattr(obs_device, 'agent_pos'):
                                    obs_device.agent_pos = obs_device.agent_pos.to(self.device)
                                observations_device.append(obs_device)
                            
                            # å‰å‘ä¼ æ’­
                            action_logits = self.actor(observations_device, agent_indices)
                            
                            if debug:
                                print(f"   Logitså½¢çŠ¶: {action_logits.shape}")
                                print(f"   LogitsèŒƒå›´: [{action_logits.min().item():.3f}, {action_logits.max().item():.3f}]")
                            
                            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿åŠ¨ä½œæœ‰æ•ˆæ€§
                            actions = []
                            for i in range(len(observations)):
                                # ğŸ”¥ ä¿®å¤ï¼šè·å–å®é™…å¯ç”¨çš„é‚»å±…æ•°é‡
                                if hasattr(self, 'env') and self.env and hasattr(self.env, 'agent_positions'):
                                    # ä»ç¯å¢ƒè·å–å®é™…é‚»å±…æ•°
                                    if i < len(self.env.agent_positions):
                                        agent_pos = self.env.agent_positions[i]
                                        available_neighbors = len(getattr(self.env, 'neighbor_dict', {}).get(agent_pos, [1]))
                                    else:
                                        available_neighbors = 1
                                else:
                                    # å›é€€ï¼šä»è§‚å¯Ÿä¸­ä¼°è®¡
                                    if hasattr(observations[i], 'num_nodes'):
                                        available_neighbors = min(self.max_neighbors, observations[i].num_nodes.item())
                                    else:
                                        available_neighbors = min(self.max_neighbors, 5)
                                
                                # ç¡®ä¿è‡³å°‘æœ‰1ä¸ªåŠ¨ä½œ
                                available_neighbors = max(1, available_neighbors)
                                
                                # è·å–è¯¥æ™ºèƒ½ä½“çš„logits
                                if action_logits.dim() == 1:
                                    logits = action_logits
                                else:
                                    logits = action_logits[i] if i < action_logits.size(0) else action_logits[0]
                                
                                # ğŸ”¥ å…³é”®ï¼šé™åˆ¶åˆ°å¯ç”¨é‚»å±…æ•°é‡
                                if logits.size(0) > available_neighbors:
                                    valid_logits = logits[:available_neighbors]
                                else:
                                    valid_logits = logits
                                
                                # é€‰æ‹©åŠ¨ä½œ
                                if valid_logits.size(0) > 0:
                                    action = torch.argmax(valid_logits).item()
                                    action = min(action, available_neighbors - 1)  # åŒé‡ä¿é™©
                                else:
                                    action = 0
                                
                                actions.append(action)
                                
                                if debug:
                                    print(f"   æ™ºèƒ½ä½“{i}: å¯ç”¨é‚»å±…{available_neighbors}, é€‰æ‹©åŠ¨ä½œ{action}")
                            
                            return np.array(actions)
                            
                    except Exception as e:
                        print(f"âš ï¸ MAGECåŠ¨ä½œé€‰æ‹©å¤±è´¥: {e}")
                        # å›é€€åˆ°å®‰å…¨çš„éšæœºåŠ¨ä½œ
                        actions = []
                        for i in range(len(observations)):
                            # ç¡®ä¿åŠ¨ä½œåœ¨å®‰å…¨èŒƒå›´å†…
                            if hasattr(observations[i], 'num_nodes'):
                                max_action = min(self.max_neighbors, observations[i].num_nodes.item()) - 1
                            else:
                                max_action = min(self.max_neighbors, 3) - 1
                            
                            action = np.random.randint(0, max(1, max_action + 1))
                            actions.append(action)
                        
                        return np.array(actions)
            
            magec_wrapper = MAGECAlgorithmWrapper(actor, device)
            self.register_algorithm("MAGEC", magec_wrapper)
            
            logger.info(f"MAGECæ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
            return env
            
        except Exception as e:
            logger.error(f"åŠ è½½MAGECæ¨¡å‹å¤±è´¥: {e}")
            return None
    
    def evaluate_algorithms(self, num_episodes=50, episode_length=100, 
                          test_scenarios=None):
        """è¯„ä¼°æ‰€æœ‰ç®—æ³•"""
        if test_scenarios is None:
            test_scenarios = [
                {'name': 'normal', 'attrition': False, 'comm_loss': 0.0},
                {'name': 'attrition', 'attrition': True, 'comm_loss': 0.0},
                {'name': 'comm_loss', 'attrition': False, 'comm_loss': 0.5},
                {'name': 'both', 'attrition': True, 'comm_loss': 0.5}
            ]
        
        logger.info(f"å¼€å§‹è¯„ä¼° {len(self.algorithms)} ä¸ªç®—æ³•")
        logger.info(f"æµ‹è¯•åœºæ™¯: {[s['name'] for s in test_scenarios]}")
        
        for scenario in test_scenarios:
            logger.info(f"æµ‹è¯•åœºæ™¯: {scenario['name']}")
            print(f"ğŸ­ æ­£åœ¨æµ‹è¯•åœºæ™¯: {scenario['name']}")
            scenario_results = {}
            
            for alg_idx, (alg_name, algorithm) in enumerate(self.algorithms.items(), 1):
                logger.info(f"  è¯„ä¼°ç®—æ³•: {alg_name}")
                print(f"  ğŸ¤– ({alg_idx}/{len(self.algorithms)}) è¯„ä¼°ç®—æ³•: {alg_name}")
                
                # è¿è¡Œå¤šä¸ªå›åˆ
                episode_rewards = []
                episode_idleness = []
                episode_steps = []
                
                # æ·»åŠ è¿›åº¦æ¡
                from tqdm import tqdm
                progress_bar = tqdm(range(num_episodes), 
                                  desc=f"    {alg_name}", 
                                  unit="ep", 
                                  leave=False,
                                  ncols=80)
                
                for episode in progress_bar:
                    # åˆ›å»ºç¯å¢ƒ
                    env = OptimizedPatrollingEnvironment(**self.env_config)
                    
                    # æ¨¡æ‹Ÿæ™ºèƒ½ä½“æŸå¤±
                    if scenario['attrition'] and episode > num_episodes // 3:
                        if env.num_agents > 1:
                            env.num_agents -= 1
                            env.agent_positions = env.agent_positions[:-1]
                    
                    observations = env.reset()
                    algorithm.reset()
                    
                    episode_reward = []
                    done = False
                    step = 0
                    
                    while not done and step < episode_length:
                        # é€‰æ‹©åŠ¨ä½œ
                        actions = algorithm.select_actions(observations)
                        
                        # æ¨¡æ‹Ÿé€šä¿¡æŸå¤±
                        if scenario['comm_loss'] > 0:
                            for i in range(len(actions)):
                                if random.random() < scenario['comm_loss']:
                                    actions[i] = 0
                        
                        # æ‰§è¡ŒåŠ¨ä½œ
                        next_observations, rewards, done = env.step(actions)
                        
                        # æ›´æ–°ç®—æ³•
                        if hasattr(algorithm, 'update'):
                            algorithm.update(observations, actions, rewards, 
                                           next_observations, done)
                        
                        episode_reward.extend(rewards)
                        observations = next_observations
                        step += 1
                    
                    # è®°å½•ç»“æœ
                    episode_rewards.append(np.mean(episode_reward))
                    episode_idleness.append(np.mean(env.node_idleness))
                    episode_steps.append(step)
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    progress_bar.set_postfix({
                        'Reward': f'{np.mean(episode_reward):.2f}',
                        'Idleness': f'{np.mean(env.node_idleness):.1f}'
                    })
                
                progress_bar.close()
                
                # è®¡ç®—ç»Ÿè®¡
                scenario_results[alg_name] = {
                    'avg_reward': np.mean(episode_rewards),
                    'std_reward': np.std(episode_rewards),
                    'avg_idleness': np.mean(episode_idleness),
                    'std_idleness': np.std(episode_idleness),
                    'avg_steps': np.mean(episode_steps),
                    'episode_rewards': episode_rewards,
                    'episode_idleness': episode_idleness
                }
                
                logger.info(f"    å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.3f} Â± {np.std(episode_rewards):.3f}")
                logger.info(f"    å¹³å‡é—²ç½®: {np.mean(episode_idleness):.3f} Â± {np.std(episode_idleness):.3f}")
                print(f"    âœ… {alg_name}: å¥–åŠ±={np.mean(episode_rewards):.3f}Â±{np.std(episode_rewards):.3f}, "
                      f"é—²ç½®={np.mean(episode_idleness):.3f}Â±{np.std(episode_idleness):.3f}")
            
            self.results[scenario['name']] = scenario_results
            print(f"âœ… åœºæ™¯ '{scenario['name']}' è¯„ä¼°å®Œæˆ\n")
        
        logger.info("è¯„ä¼°å®Œæˆï¼")
        return self.results
    
    def plot_comparison_results(self, save_path=None):
        """ç»˜åˆ¶å¯¹æ¯”ç»“æœ"""
        if not self.results:
            logger.warning("æ²¡æœ‰ç»“æœå¯ç»˜åˆ¶")
            return
        
        scenarios = list(self.results.keys())
        algorithms = list(self.algorithms.keys())
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('MAGEC vs Baseline Algorithms Comparison', fontsize=16, fontweight='bold')
        
        # å¹³å‡å¥–åŠ±å¯¹æ¯”
        ax = axes[0, 0]
        x = np.arange(len(scenarios))
        width = 0.15
        
        for i, alg in enumerate(algorithms):
            rewards = [self.results[s][alg]['avg_reward'] for s in scenarios]
            errors = [self.results[s][alg]['std_reward'] for s in scenarios]
            ax.bar(x + i * width, rewards, width, label=alg, yerr=errors, capsize=3)
        
        ax.set_xlabel('Test Scenarios')
        ax.set_ylabel('Average Reward')
        ax.set_title('Average Reward Comparison')
        ax.set_xticks(x + width * (len(algorithms) - 1) / 2)
        ax.set_xticklabels(scenarios, rotation=45)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # å¹³å‡é—²ç½®æ—¶é—´å¯¹æ¯”
        ax = axes[0, 1]
        for i, alg in enumerate(algorithms):
            idleness = [self.results[s][alg]['avg_idleness'] for s in scenarios]
            errors = [self.results[s][alg]['std_idleness'] for s in scenarios]
            ax.bar(x + i * width, idleness, width, label=alg, yerr=errors, capsize=3)
        
        ax.set_xlabel('Test Scenarios')
        ax.set_ylabel('Average Idleness')
        ax.set_title('Average Idleness Comparison (Lower is Better)')
        ax.set_xticks(x + width * (len(algorithms) - 1) / 2)
        ax.set_xticklabels(scenarios, rotation=45)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # å¥–åŠ±åˆ†å¸ƒç®±çº¿å›¾ï¼ˆnormalåœºæ™¯ï¼‰
        ax = axes[1, 0]
        if 'normal' in self.results:
            reward_data = [self.results['normal'][alg]['episode_rewards'] for alg in algorithms]
            box_plot = ax.boxplot(reward_data, labels=algorithms, patch_artist=True)
            
            colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink']
            for patch, color in zip(box_plot['boxes'], colors[:len(algorithms)]):
                patch.set_facecolor(color)
        
        ax.set_xlabel('Algorithms')
        ax.set_ylabel('Episode Rewards')
        ax.set_title('Reward Distribution (Normal Scenario)')
        ax.grid(True, alpha=0.3)
        
        # æ€§èƒ½çƒ­åŠ›å›¾
        ax = axes[1, 1]
        
        perf_matrix = np.zeros((len(algorithms), len(scenarios)))
        for i, alg in enumerate(algorithms):
            for j, scenario in enumerate(scenarios):
                perf_matrix[i, j] = -self.results[scenario][alg]['avg_idleness']
        
        perf_matrix = (perf_matrix - perf_matrix.min()) / (perf_matrix.max() - perf_matrix.min())
        
        im = ax.imshow(perf_matrix, cmap='RdYlGn', aspect='auto')
        ax.set_xticks(range(len(scenarios)))
        ax.set_yticks(range(len(algorithms)))
        ax.set_xticklabels(scenarios, rotation=45)
        ax.set_yticklabels(algorithms)
        ax.set_title('Performance Heatmap (Green=Better)')
        
        for i in range(len(algorithms)):
            for j in range(len(scenarios)):
                text = ax.text(j, i, f'{perf_matrix[i, j]:.2f}',
                             ha="center", va="center", color="black", fontweight='bold')
        
        plt.colorbar(im, ax=ax)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            logger.info(f"å¯¹æ¯”ç»“æœå·²ä¿å­˜: {save_path}")
        else:
            plt.show()
        
        plt.close()
    
    def visualize_patrolling_animation(self, algorithm_name, save_path=None, 
                                     episode_length=100):
        """åˆ›å»ºå·¡é€»è¿‡ç¨‹çš„åŠ¨ç”»å¯è§†åŒ–"""
        if algorithm_name not in self.algorithms:
            logger.error(f"ç®—æ³• {algorithm_name} æœªæ³¨å†Œ")
            return
        
        # åˆ›å»ºç¯å¢ƒå’Œç®—æ³•
        env = OptimizedPatrollingEnvironment(**self.env_config)
        algorithm = self.algorithms[algorithm_name]
        
        observations = env.reset()
        algorithm.reset()
        
        # è®°å½•å·¡é€»è½¨è¿¹
        trajectory = []
        idleness_history = []
        
        for step in range(episode_length):
            # è®°å½•å½“å‰çŠ¶æ€
            current_state = {
                'agent_positions': env.agent_positions.copy(),
                'node_idleness': env.node_idleness.copy(),
                'step': step
            }
            trajectory.append(current_state)
            idleness_history.append(np.mean(env.node_idleness))
            
            # é€‰æ‹©å¹¶æ‰§è¡ŒåŠ¨ä½œ
            actions = algorithm.select_actions(observations)
            next_observations, rewards, done = env.step(actions)
            
            if hasattr(algorithm, 'update'):
                algorithm.update(observations, actions, rewards, next_observations, done)
            
            observations = next_observations
            
            if done:
                break
        
        # åˆ›å»ºåŠ¨ç”»
        self._create_patrolling_animation(env, trajectory, idleness_history, 
                                        algorithm_name, save_path)
    
    def _create_patrolling_animation(self, env, trajectory, idleness_history, 
                                   algorithm_name, save_path):
        """åˆ›å»ºå·¡é€»åŠ¨ç”»"""
        try:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # ç»˜åˆ¶å›¾ç»“æ„
            pos = env.node_positions
            
            def animate(frame):
                ax1.clear()
                ax2.clear()
                
                if frame >= len(trajectory):
                    return
                
                state = trajectory[frame]
                
                # å·¦å›¾ï¼šå›¾å¯è§†åŒ–
                ax1.set_title(f'{algorithm_name} Patrolling - Step {state["step"]}')
                
                # ç»˜åˆ¶è¾¹
                for edge in env.graph.edges():
                    x_coords = [pos[edge[0]][0], pos[edge[1]][0]]
                    y_coords = [pos[edge[0]][1], pos[edge[1]][1]]
                    ax1.plot(x_coords, y_coords, 'k-', alpha=0.3, linewidth=1)
                
                # ç»˜åˆ¶èŠ‚ç‚¹ï¼ˆé¢œè‰²è¡¨ç¤ºé—²ç½®æ—¶é—´ï¼‰
                node_colors = []
                for node in range(env.num_nodes):
                    idleness = state['node_idleness'][node]
                    max_idleness = max(state['node_idleness']) if max(state['node_idleness']) > 0 else 1
                    normalized_idleness = idleness / max_idleness
                    node_colors.append(plt.cm.Reds(normalized_idleness))
                
                for node in range(env.num_nodes):
                    circle = Circle(pos[node], 0.05, color=node_colors[node], alpha=0.8)
                    ax1.add_patch(circle)
                    ax1.text(pos[node][0], pos[node][1], str(node), 
                            ha='center', va='center', fontsize=8, fontweight='bold')
                
                # ç»˜åˆ¶æ™ºèƒ½ä½“
                colors = ['blue', 'red', 'green', 'orange', 'purple']
                for i, agent_pos in enumerate(state['agent_positions']):
                    if i < len(colors) and agent_pos < len(pos):
                        circle = Circle(pos[agent_pos], 0.08, 
                                      color=colors[i], alpha=0.9, linewidth=2)
                        ax1.add_patch(circle)
                        ax1.text(pos[agent_pos][0], pos[agent_pos][1] + 0.12, 
                               f'A{i}', ha='center', va='center', 
                               fontsize=10, fontweight='bold', color=colors[i])
                
                ax1.set_xlim(-0.2, 1.2)
                ax1.set_ylim(-0.2, 1.2)
                ax1.set_aspect('equal')
                ax1.axis('off')
                
                # æ·»åŠ é¢œè‰²æ¡è¯´æ˜
                ax1.text(1.1, 1.1, 'Idleness:', fontsize=10, fontweight='bold')
                ax1.text(1.1, 1.05, 'Low', fontsize=8, color='white')
                ax1.text(1.1, 0.95, 'High', fontsize=8, color='red')
                
                # å³å›¾ï¼šæ€§èƒ½æŒ‡æ ‡
                steps = list(range(len(idleness_history[:frame+1])))
                ax2.plot(steps, idleness_history[:frame+1], 'b-', linewidth=2)
                ax2.set_xlabel('Step')
                ax2.set_ylabel('Average Idleness')
                ax2.set_title('Average Idleness Over Time')
                ax2.grid(True, alpha=0.3)
                
                if len(idleness_history) > 0:
                    ax2.set_ylim(0, max(idleness_history) * 1.1)
                    ax2.set_xlim(0, len(idleness_history))
                
                plt.tight_layout()
            
            # åˆ›å»ºåŠ¨ç”»
            ani = animation.FuncAnimation(fig, animate, frames=len(trajectory),
                                        interval=200, repeat=True, blit=False)
            
            if save_path:
                ani.save(save_path, writer='pillow', fps=5)
                logger.info(f"åŠ¨ç”»å·²ä¿å­˜: {save_path}")
            else:
                plt.show()
            
            plt.close()
            
        except Exception as e:
            logger.error(f"åˆ›å»ºåŠ¨ç”»å¤±è´¥: {e}")
    
    def save_results_report(self, save_path="results/evaluation_report.json"):
        """ä¿å­˜è¯„ä¼°æŠ¥å‘Š"""
        try:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            report = {
                'algorithms': list(self.algorithms.keys()),
                'env_config': self.env_config,
                'results': self.results,
                'summary': self._generate_summary(),
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            with open(save_path, 'w') as f:
                json.dump(report, f, indent=2, default=lambda x: float(x) if isinstance(x, np.float64) else x)
            
            logger.info(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {save_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
    
    def _generate_summary(self):
        """ç”Ÿæˆè¯„ä¼°æ‘˜è¦"""
        if not self.results:
            return {}
        
        summary = {}
        
        for scenario, scenario_results in self.results.items():
            best_reward_alg = max(scenario_results.keys(), 
                                key=lambda x: scenario_results[x]['avg_reward'])
            best_idleness_alg = min(scenario_results.keys(), 
                                  key=lambda x: scenario_results[x]['avg_idleness'])
            
            summary[scenario] = {
                'best_reward_algorithm': best_reward_alg,
                'best_reward_value': scenario_results[best_reward_alg]['avg_reward'],
                'best_idleness_algorithm': best_idleness_alg,
                'best_idleness_value': scenario_results[best_idleness_alg]['avg_idleness'],
                'magec_performance': scenario_results.get('MAGEC', {})
            }
        
        return summary

def interactive_input():
    """äº¤äº’å¼è¾“å…¥é…ç½®"""
    print("ğŸš€ " + "=" * 76)
    print("ğŸš€ MAGEC ç®—æ³•è¯„ä¼°å’Œå¯è§†åŒ–å·¥å…· - äº¤äº’å¼é…ç½®")
    print("ğŸš€ " + "=" * 76)
    print("ğŸ’¡ æç¤ºï¼šç›´æ¥æŒ‰å›è½¦ä½¿ç”¨é»˜è®¤å€¼ï¼Œè¾“å…¥ 'q' é€€å‡º")
    print()
    
    config = {}
    
    # 1. æ¨¡å‹è·¯å¾„é…ç½®
    print("ğŸ“‚ æ¨¡å‹è·¯å¾„é…ç½®")
    print("-" * 50)
    
    # æœç´¢å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶
    possible_models = []
    if os.path.exists("experiments"):
        for root, dirs, files in os.walk("experiments"):
            for file in files:
                if file.endswith(('.pth', '.pt')):
                    possible_models.append(os.path.join(root, file))
    
    if possible_models:
        print("ğŸ” å‘ç°ä»¥ä¸‹è®­ç»ƒå¥½çš„æ¨¡å‹:")
        for i, model in enumerate(possible_models, 1):
            # æ˜¾ç¤ºæ–‡ä»¶å¤§å°å’Œä¿®æ”¹æ—¶é—´
            try:
                stat = os.stat(model)
                size_mb = stat.st_size / (1024 * 1024)
                mtime = time.strftime('%Y-%m-%d %H:%M', time.localtime(stat.st_mtime))
                print(f"  {i}. {model}")
                print(f"      ğŸ“Š å¤§å°: {size_mb:.1f}MB, ğŸ“… ä¿®æ”¹: {mtime}")
            except:
                print(f"  {i}. {model}")
        print()
        
        while True:
            choice = input("è¯·é€‰æ‹©æ¨¡å‹ (è¾“å…¥åºå·æˆ–å®Œæ•´è·¯å¾„): ").strip()
            if choice.lower() == 'q':
                print("ğŸ‘‹ é€€å‡ºç¨‹åº")
                sys.exit(0)
            
            if choice.isdigit() and 1 <= int(choice) <= len(possible_models):
                config['magec_model'] = possible_models[int(choice) - 1]
                break
            elif os.path.exists(choice):
                config['magec_model'] = choice
                break
            else:
                print("âŒ æ— æ•ˆçš„é€‰æ‹©æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°è¾“å…¥")
                if choice and not choice.isdigit():
                    print("ğŸ’¡ æç¤º: å¯ä»¥è¾“å…¥å®Œæ•´çš„æ–‡ä»¶è·¯å¾„ï¼Œæˆ–è€…è¾“å…¥åºå·é€‰æ‹©ä¸Šé¢åˆ—å‡ºçš„æ¨¡å‹")
    else:
        print("âš ï¸ æœªå‘ç°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶")
        print("ğŸ’¡ æç¤º: è¯·å…ˆè¿è¡Œ demo.py è®­ç»ƒæ¨¡å‹ï¼Œæˆ–è€…æ‰‹åŠ¨è¾“å…¥æ¨¡å‹è·¯å¾„")
        print()
        
        while True:
            model_path = input("è¯·è¾“å…¥MAGECæ¨¡å‹è·¯å¾„: ").strip()
            if model_path.lower() == 'q':
                print("ğŸ‘‹ é€€å‡ºç¨‹åº")
                sys.exit(0)
            if model_path and os.path.exists(model_path):
                config['magec_model'] = model_path
                break
            else:
                print("âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„")
                print("ğŸ’¡ æç¤º: æ¨¡å‹æ–‡ä»¶é€šå¸¸ä»¥ .pth æˆ– .pt ç»“å°¾")
    
    print(f"âœ… å·²é€‰æ‹©æ¨¡å‹: {config['magec_model']}")
    print()
    
    # 2. è¾“å‡ºç›®å½•é…ç½®
    print("ğŸ“ è¾“å‡ºç›®å½•é…ç½®")
    print("-" * 50)
    default_output = f"results/evaluation_{time.strftime('%Y%m%d_%H%M%S')}"
    output_dir = input(f"è¾“å‡ºç›®å½• [é»˜è®¤: {default_output}]: ").strip()
    config['output_dir'] = output_dir if output_dir else default_output
    print(f"âœ… è¾“å‡ºç›®å½•: {config['output_dir']}")
    print()
    
    # 3. æµ‹è¯•ç¯å¢ƒé…ç½®
    print("ğŸŒ æµ‹è¯•ç¯å¢ƒé…ç½®")
    print("-" * 50)
    
    # å›¾ç±»å‹
    graphs = ['milwaukee', '4nodes']
    print("å¯é€‰å›¾ç±»å‹:")
    for i, graph in enumerate(graphs, 1):
        print(f"  {i}. {graph}")
    
    while True:
        graph_choice = input("é€‰æ‹©å›¾ç±»å‹ [é»˜è®¤: 1-milwaukee]: ").strip()
        if not graph_choice:
            config['graph_name'] = 'milwaukee'
            break
        elif graph_choice.isdigit() and 1 <= int(graph_choice) <= len(graphs):
            config['graph_name'] = graphs[int(graph_choice) - 1]
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥1-2")
    
    # æ™ºèƒ½ä½“æ•°é‡
    while True:
        agents_input = input("æ™ºèƒ½ä½“æ•°é‡ [é»˜è®¤: 4]: ").strip()
        if not agents_input:
            config['num_agents'] = 4
            break
        try:
            num_agents = int(agents_input)
            if 1 <= num_agents <= 10:
                config['num_agents'] = num_agents
                break
            else:
                print("âŒ æ™ºèƒ½ä½“æ•°é‡åº”åœ¨1-10ä¹‹é—´")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
    
    print(f"âœ… æµ‹è¯•ç¯å¢ƒ: {config['graph_name']}, {config['num_agents']} agents")
    print()
    
    # 4. æµ‹è¯•å‚æ•°é…ç½®
    print("âš™ï¸ æµ‹è¯•å‚æ•°é…ç½®")
    print("-" * 50)
    
    # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
    quick_test = input("å¯ç”¨å¿«é€Ÿæµ‹è¯•æ¨¡å¼? (y/N) [é»˜è®¤: N]: ").strip().lower()
    config['quick_test'] = quick_test in ['y', 'yes', '1', 'true']
    
    if config['quick_test']:
        config['num_episodes'] = 10
        config['episode_length'] = 50
        print("âœ… å¿«é€Ÿæµ‹è¯•æ¨¡å¼: 10å›åˆ Ã— 50æ­¥")
    else:
        # å›åˆæ•°
        while True:
            episodes_input = input("æµ‹è¯•å›åˆæ•° [é»˜è®¤: 50]: ").strip()
            if not episodes_input:
                config['num_episodes'] = 50
                break
            try:
                episodes = int(episodes_input)
                if episodes > 0:
                    config['num_episodes'] = episodes
                    break
                else:
                    print("âŒ å›åˆæ•°å¿…é¡»å¤§äº0")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
        
        # æ¯å›åˆæ­¥æ•°
        while True:
            steps_input = input("æ¯å›åˆæ­¥æ•° [é»˜è®¤: 100]: ").strip()
            if not steps_input:
                config['episode_length'] = 100
                break
            try:
                steps = int(steps_input)
                if steps > 0:
                    config['episode_length'] = steps
                    break
                else:
                    print("âŒ æ­¥æ•°å¿…é¡»å¤§äº0")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
        
        print(f"âœ… æµ‹è¯•å‚æ•°: {config['num_episodes']}å›åˆ Ã— {config['episode_length']}æ­¥")
    
    print()
    
    # 5. ç®—æ³•é€‰æ‹©
    print("ğŸ¤– ç®—æ³•é€‰æ‹©")
    print("-" * 50)
    available_algorithms = ['MAGEC', 'AHPA', 'SEBS', 'CBLS', 'Random']
    print("å¯é€‰ç®—æ³•:")
    for i, alg in enumerate(available_algorithms, 1):
        print(f"  {i}. {alg}")
    
    print("è¯·é€‰æ‹©è¦æµ‹è¯•çš„ç®—æ³• (ç”¨ç©ºæ ¼åˆ†éš”å¤šä¸ªé€‰æ‹©ï¼Œå¦‚: 1 2 3)")
    while True:
        alg_input = input("[é»˜è®¤: 1 2 3 4 5 (å…¨éƒ¨)]: ").strip()
        if not alg_input:
            config['algorithms'] = available_algorithms.copy()
            break
        
        try:
            choices = [int(x) for x in alg_input.split()]
            if all(1 <= choice <= len(available_algorithms) for choice in choices):
                config['algorithms'] = [available_algorithms[i-1] for i in choices]
                break
            else:
                print("âŒ é€‰æ‹©è¶…å‡ºèŒƒå›´ï¼Œè¯·è¾“å…¥1-5")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—åºåˆ—")
    
    print(f"âœ… é€‰æ‹©ç®—æ³•: {', '.join(config['algorithms'])}")
    print()
    
    # 6. æµ‹è¯•åœºæ™¯
    print("ğŸ­ æµ‹è¯•åœºæ™¯")
    print("-" * 50)
    available_scenarios = [
        ('normal', 'æ­£å¸¸åœºæ™¯'),
        ('attrition', 'æ™ºèƒ½ä½“æŸå¤±'),
        ('comm_loss', 'é€šä¿¡å¹²æ‰°'),
        ('both', 'å¤åˆå¹²æ‰°')
    ]
    
    print("å¯é€‰æµ‹è¯•åœºæ™¯:")
    for i, (name, desc) in enumerate(available_scenarios, 1):
        print(f"  {i}. {desc} ({name})")
    
    print("è¯·é€‰æ‹©æµ‹è¯•åœºæ™¯ (ç”¨ç©ºæ ¼åˆ†éš”å¤šä¸ªé€‰æ‹©)")
    while True:
        scenario_input = input("[é»˜è®¤: 1 2 3 4 (å…¨éƒ¨)]: ").strip()
        if not scenario_input:
            config['scenarios'] = [name for name, _ in available_scenarios]
            break
        
        try:
            choices = [int(x) for x in scenario_input.split()]
            if all(1 <= choice <= len(available_scenarios) for choice in choices):
                config['scenarios'] = [available_scenarios[i-1][0] for i in choices]
                break
            else:
                print("âŒ é€‰æ‹©è¶…å‡ºèŒƒå›´ï¼Œè¯·è¾“å…¥1-4")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—åºåˆ—")
    
    print(f"âœ… æµ‹è¯•åœºæ™¯: {', '.join(config['scenarios'])}")
    print()
    
    # 7. å¯è§†åŒ–é€‰é¡¹
    print("ğŸ¬ å¯è§†åŒ–é€‰é¡¹")
    print("-" * 50)
    
    animate = input("ç”ŸæˆåŠ¨ç”»å¯è§†åŒ–? (y/N) [é»˜è®¤: N]: ").strip().lower()
    config['animate'] = animate in ['y', 'yes', '1', 'true']
    
    if config['animate']:
        print("é€‰æ‹©è¦ç”ŸæˆåŠ¨ç”»çš„ç®—æ³•:")
        for i, alg in enumerate(config['algorithms'], 1):
            print(f"  {i}. {alg}")
        
        while True:
            anim_choice = input("[é»˜è®¤: 1-ç¬¬ä¸€ä¸ªç®—æ³•]: ").strip()
            if not anim_choice:
                config['animate_algorithm'] = config['algorithms'][0]
                break
            try:
                choice = int(anim_choice)
                if 1 <= choice <= len(config['algorithms']):
                    config['animate_algorithm'] = config['algorithms'][choice-1]
                    break
                else:
                    print("âŒ é€‰æ‹©è¶…å‡ºèŒƒå›´")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
        
        print(f"âœ… åŠ¨ç”»ç®—æ³•: {config['animate_algorithm']}")
    else:
        config['animate_algorithm'] = 'MAGEC'
    
    print()
    
    # 8. å…¶ä»–è®¾ç½®
    config['seed'] = 42
    
    # æ˜¾ç¤ºæœ€ç»ˆé…ç½®
    print("ğŸ“‹ " + "=" * 76)
    print("ğŸ“‹ æœ€ç»ˆé…ç½®ç¡®è®¤")
    print("ğŸ“‹ " + "=" * 76)
    print(f"ğŸ”¹ æ¨¡å‹è·¯å¾„: {config['magec_model']}")
    print(f"ğŸ”¹ è¾“å‡ºç›®å½•: {config['output_dir']}")
    print(f"ğŸ”¹ æµ‹è¯•ç¯å¢ƒ: {config['graph_name']} ({config['num_agents']} agents)")
    print(f"ğŸ”¹ æµ‹è¯•å‚æ•°: {config['num_episodes']}å›åˆ Ã— {config['episode_length']}æ­¥")
    print(f"ğŸ”¹ æµ‹è¯•ç®—æ³•: {', '.join(config['algorithms'])}")
    print(f"ğŸ”¹ æµ‹è¯•åœºæ™¯: {', '.join(config['scenarios'])}")
    print(f"ğŸ”¹ ç”ŸæˆåŠ¨ç”»: {'æ˜¯' if config['animate'] else 'å¦'}")
    if config['animate']:
        print(f"ğŸ”¹ åŠ¨ç”»ç®—æ³•: {config['animate_algorithm']}")
    print()
    
    confirm = input("ç¡®è®¤å¼€å§‹è¯„ä¼°? (Y/n) [é»˜è®¤: Y]: ").strip().lower()
    if confirm in ['n', 'no', '0', 'false']:
        print("ğŸ‘‹ å·²å–æ¶ˆè¯„ä¼°")
        sys.exit(0)
    
    return config

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ˜¾ç¤ºå¸®åŠ©

    
    parser = argparse.ArgumentParser(
        description='MAGECç®—æ³•è¯„ä¼°å’Œå¯è§†åŒ–',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python visualize.py                                    # äº¤äº’å¼æ¨¡å¼
  python visualize.py --quick_test --animate             # å¿«é€Ÿæµ‹è¯•
  python visualize.py --magec_model model.pth           # æŒ‡å®šæ¨¡å‹
  python visualize.py --batch --magec_model model.pth   # æ‰¹å¤„ç†æ¨¡å¼
        """
    )
    parser.add_argument('--magec_model', type=str, help='è®­ç»ƒå¥½çš„MAGECæ¨¡å‹è·¯å¾„')
    parser.add_argument('--graph_name', type=str, default='milwaukee',
                       choices=['milwaukee', '4nodes'], help='æµ‹è¯•å›¾ç±»å‹')
    parser.add_argument('--num_agents', type=int, default=4, help='æ™ºèƒ½ä½“æ•°é‡')
    parser.add_argument('--num_episodes', type=int, default=50, help='æ¯ä¸ªç®—æ³•çš„æµ‹è¯•å›åˆæ•°')
    parser.add_argument('--episode_length', type=int, default=100, help='æ¯å›åˆæ­¥æ•°')
    parser.add_argument('--output_dir', type=str, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--algorithms', nargs='+', 
                       default=['MAGEC', 'AHPA', 'SEBS', 'CBLS', 'Random'],
                       help='è¦æµ‹è¯•çš„ç®—æ³•')
    parser.add_argument('--scenarios', nargs='+',
                       default=['normal', 'attrition', 'comm_loss', 'both'],
                       help='æµ‹è¯•åœºæ™¯')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--quick_test', action='store_true', help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼')
    parser.add_argument('--animate', action='store_true', help='ç”ŸæˆåŠ¨ç”»å¯è§†åŒ–')
    parser.add_argument('--animate_algorithm', type=str, default='MAGEC',
                       help='è¦ç”ŸæˆåŠ¨ç”»çš„ç®—æ³•')
    parser.add_argument('--interactive', action='store_true', help='äº¤äº’å¼é…ç½®æ¨¡å¼')
    parser.add_argument('--batch', action='store_true', help='æ‰¹å¤„ç†æ¨¡å¼ï¼ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼‰')
    
    args = parser.parse_args()
    
    # å†³å®šä½¿ç”¨äº¤äº’å¼è¿˜æ˜¯å‘½ä»¤è¡Œæ¨¡å¼
    if args.batch:
        # æ‰¹å¤„ç†æ¨¡å¼ï¼šå¿…é¡»æä¾›æ¨¡å‹è·¯å¾„
        if not args.magec_model:
            print("âŒ æ‰¹å¤„ç†æ¨¡å¼ä¸‹å¿…é¡»æŒ‡å®š --magec_model å‚æ•°")
            sys.exit(1)
        
        config = {
            'magec_model': args.magec_model,
            'graph_name': args.graph_name,
            'num_agents': args.num_agents,
            'num_episodes': args.num_episodes,
            'episode_length': args.episode_length,
            'output_dir': args.output_dir or 'results/evaluation',
            'algorithms': args.algorithms,
            'scenarios': args.scenarios,
            'seed': args.seed,
            'quick_test': args.quick_test,
            'animate': args.animate,
            'animate_algorithm': args.animate_algorithm
        }
        
        print("ğŸ¤– æ‰¹å¤„ç†æ¨¡å¼")
        
    elif args.interactive or not args.magec_model:
        # äº¤äº’å¼æ¨¡å¼ï¼šå¦‚æœæ²¡æœ‰æä¾›æ¨¡å‹è·¯å¾„æˆ–è€…æ˜ç¡®æŒ‡å®šäº¤äº’å¼
        config = interactive_input()
    else:
        # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
        config = {
            'magec_model': args.magec_model,
            'graph_name': args.graph_name,
            'num_agents': args.num_agents,
            'num_episodes': args.num_episodes,
            'episode_length': args.episode_length,
            'output_dir': args.output_dir or 'results/evaluation',
            'algorithms': args.algorithms,
            'scenarios': args.scenarios,
            'seed': args.seed,
            'quick_test': args.quick_test,
            'animate': args.animate,
            'animate_algorithm': args.animate_algorithm
        }
    
    # è®¾ç½®éšæœºç§å­
    random.seed(config['seed'])
    np.random.seed(config['seed'])
    torch.manual_seed(config['seed'])
    
    # å¿«é€Ÿæµ‹è¯•æ¨¡å¼è°ƒæ•´
    if config['quick_test']:
        config['num_episodes'] = 10
        config['episode_length'] = 50
        config['graph_name'] = '4nodes'
        config['num_agents'] = 2
        logger.info("å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config['output_dir'], exist_ok=True)
    
    # ç¯å¢ƒé…ç½®
    env_config = {
        'graph_name': config['graph_name'],
        'num_agents': config['num_agents'],
        'observation_radius': 400.0,
        'max_cycles': config['episode_length'],
        'agent_speed': 40.0,
        'action_method': 'neighbors'
    }
    
    print("\n" + "ğŸ¯ " + "=" * 76)
    print("ğŸ¯ å¼€å§‹MAGECç®—æ³•è¯„ä¼°")
    print("ğŸ¯ " + "=" * 76)
    print(f"ğŸ“ MAGECæ¨¡å‹: {config['magec_model']}")
    print(f"ğŸŒ æµ‹è¯•ç¯å¢ƒ: {config['graph_name']} ({config['num_agents']} agents)")
    print(f"ğŸ¤– æµ‹è¯•ç®—æ³•: {', '.join(config['algorithms'])}")
    print(f"ğŸ­ æµ‹è¯•åœºæ™¯: {', '.join(config['scenarios'])}")
    print(f"âš™ï¸ æµ‹è¯•è®¾ç½®: {config['num_episodes']} episodes Ã— {config['episode_length']} steps")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {config['output_dir']}")
    print("ğŸ¯ " + "=" * 76)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = MAGECEvaluator(env_config)
    
    # åŠ è½½MAGECæ¨¡å‹
    if 'MAGEC' in config['algorithms']:
        if not os.path.exists(config['magec_model']):
            logger.error(f"MAGECæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {config['magec_model']}")
            return
        
        print("ğŸ“¥ æ­£åœ¨åŠ è½½MAGECæ¨¡å‹...")
        env = evaluator.load_magec_model(config['magec_model'])
        if env is None:
            logger.error("MAGECæ¨¡å‹åŠ è½½å¤±è´¥")
            return
        print("âœ… MAGECæ¨¡å‹åŠ è½½æˆåŠŸ")
    else:
        env = OptimizedPatrollingEnvironment(**env_config)
    
    # æ³¨å†ŒåŸºå‡†ç®—æ³•
    print("ğŸ”§ æ³¨å†ŒåŸºå‡†ç®—æ³•...")
    if 'AHPA' in config['algorithms']:
        evaluator.register_algorithm('AHPA', AHPAAlgorithm(env))
    
    if 'SEBS' in config['algorithms']:
        evaluator.register_algorithm('SEBS', SEBSAlgorithm(env))
    
    if 'CBLS' in config['algorithms']:
        evaluator.register_algorithm('CBLS', CBLSAlgorithm(env))
    
    if 'Random' in config['algorithms']:
        evaluator.register_algorithm('Random', RandomAlgorithm(env))
    
    print(f"âœ… å·²æ³¨å†Œ {len(evaluator.algorithms)} ä¸ªç®—æ³•")
    
    # å®šä¹‰æµ‹è¯•åœºæ™¯
    test_scenarios = []
    scenario_configs = {
        'normal': {'name': 'Normal', 'attrition': False, 'comm_loss': 0.0},
        'attrition': {'name': 'Agent Attrition', 'attrition': True, 'comm_loss': 0.0},
        'comm_loss': {'name': 'Communication Loss', 'attrition': False, 'comm_loss': 0.5},
        'both': {'name': 'Both Disturbances', 'attrition': True, 'comm_loss': 0.5}
    }
    
    for scenario_name in config['scenarios']:
        if scenario_name in scenario_configs:
            test_scenarios.append(scenario_configs[scenario_name])
    
    try:
        # è¿è¡Œè¯„ä¼°
        print(f"\nğŸ å¼€å§‹ç®—æ³•è¯„ä¼° ({len(test_scenarios)}ä¸ªåœºæ™¯)...")
        results = evaluator.evaluate_algorithms(
            num_episodes=config['num_episodes'],
            episode_length=config['episode_length'],
            test_scenarios=test_scenarios
        )
        
        print("ğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ...")
        # ä¿å­˜ç»“æœ
        evaluator.save_results_report(f"{config['output_dir']}/evaluation_report.json")
        print(f"âœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {config['output_dir']}/evaluation_report.json")
        
        # ç»˜åˆ¶å¯¹æ¯”å›¾
        evaluator.plot_comparison_results(f"{config['output_dir']}/comparison_results.png")
        print(f"âœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {config['output_dir']}/comparison_results.png")
        
        # ç”ŸæˆåŠ¨ç”»ï¼ˆå¦‚æœè¯·æ±‚ï¼‰
        if config['animate'] and config['animate_algorithm'] in evaluator.algorithms:
            print(f"ğŸ¬ æ­£åœ¨ç”Ÿæˆ {config['animate_algorithm']} ç®—æ³•çš„åŠ¨ç”»...")
            animation_path = f"{config['output_dir']}/{config['animate_algorithm']}_animation.gif"
            evaluator.visualize_patrolling_animation(
                config['animate_algorithm'], animation_path, config['episode_length']
            )
            print(f"âœ… åŠ¨ç”»å·²ä¿å­˜: {animation_path}")
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "ğŸ“Š " + "=" * 76)
        print("ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦")
        print("ğŸ“Š " + "=" * 76)
        
        summary = evaluator._generate_summary()
        for scenario, data in summary.items():
            print(f"\nğŸ­ {scenario.upper()} åœºæ™¯:")
            print(f"  ğŸ† æœ€ä½³å¥–åŠ±ç®—æ³•: {data['best_reward_algorithm']} ({data['best_reward_value']:.3f})")
            print(f"  âš¡ æœ€ä½³é—²ç½®ç®—æ³•: {data['best_idleness_algorithm']} ({data['best_idleness_value']:.3f})")
            
            if 'MAGEC' in data['magec_performance']:
                magec_perf = data['magec_performance']
                print(f"  ğŸ¤– MAGECæ€§èƒ½: å¥–åŠ±={magec_perf['avg_reward']:.3f}, é—²ç½®={magec_perf['avg_idleness']:.3f}")
        
        print("\n" + "ğŸ‰ " + "=" * 76)
        print("ğŸ‰ è¯„ä¼°å®Œæˆï¼")
        print("ğŸ‰ " + "=" * 76)
        print(f"ğŸ“ è¯¦ç»†ç»“æœä¿å­˜åœ¨: {config['output_dir']}/")
        print("ğŸ“‹ ä¸»è¦æ–‡ä»¶:")
        print(f"  ğŸ“„ è¯„ä¼°æŠ¥å‘Š: evaluation_report.json")
        print(f"  ğŸ“ˆ å¯¹æ¯”å›¾è¡¨: comparison_results.png")
        if config['animate']:
            print(f"  ğŸ¬ åŠ¨ç”»å¯è§†åŒ–: {config['animate_algorithm']}_animation.gif")
        print("ğŸ‰ " + "=" * 76)
        
        # è¯¢é—®æ˜¯å¦æ‰“å¼€ç»“æœç›®å½•
        if not config.get('batch', False):
            open_dir = input("\nğŸ“‚ æ˜¯å¦æ‰“å¼€ç»“æœç›®å½•? (y/N): ").strip().lower()
            if open_dir in ['y', 'yes', '1', 'true']:
                try:
                    import subprocess
                    import platform
                    
                    system = platform.system()
                    if system == 'Windows':
                        subprocess.run(['explorer', config['output_dir']])
                    elif system == 'Darwin':  # macOS
                        subprocess.run(['open', config['output_dir']])
                    else:  # Linux
                        subprocess.run(['xdg-open', config['output_dir']])
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•è‡ªåŠ¨æ‰“å¼€ç›®å½•: {e}")
                    print(f"ğŸ“ è¯·æ‰‹åŠ¨è®¿é—®: {config['output_dir']}")
        
    except KeyboardInterrupt:
        print("\nâ›” è¯„ä¼°è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    
    try:
        main()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥è¾“å…¥å‚æ•°æˆ–æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯: python visualize.py --help")
        
# å¿«é€Ÿå¼€å§‹ç¤ºä¾‹
"""
ğŸš€ å¿«é€Ÿå¼€å§‹ç¤ºä¾‹:

1. æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼ (æ¨èæ–°æ‰‹):
   python visualize.py

2. å¦‚æœä½ å·²ç»æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹:
   python visualize.py --magec_model experiments/magec_official/final_model.pth

3. å¿«é€Ÿæµ‹è¯• (åªéœ€å‡ åˆ†é’Ÿ):
   python visualize.py --quick_test --animate

4. æ‰¹é‡å¤„ç†å¤šä¸ªæ¨¡å‹:
   for model in experiments/*/final_model.pth; do
       python visualize.py --batch --magec_model "$model" --output_dir "results/$(basename $(dirname $model))"
   done

5. åªæµ‹è¯•ç‰¹å®šç®—æ³•å’Œåœºæ™¯:
   python visualize.py --algorithms MAGEC AHPA --scenarios normal attrition --animate

ğŸ’¡ æç¤º: ç¬¬ä¸€æ¬¡ä½¿ç”¨å»ºè®®é€‰æ‹©äº¤äº’å¼æ¨¡å¼ï¼Œç¨‹åºä¼šå¼•å¯¼ä½ å®Œæˆæ‰€æœ‰é…ç½®ï¼
"""