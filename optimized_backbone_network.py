"""
optimized_backbone_network.py - é›†æˆå¢å¼ºç‰ˆClothoidCubicå’ŒEnhancedNodeClusteringConsolidator
âœ¨ ä¿®å¤ç‰ˆæœ¬ï¼šè§£å†³äº†ç«¯ç‚¹èšç±»æ–¹æ³•è°ƒç”¨é—®é¢˜
"""

import math
import time
from collections import defaultdict, OrderedDict
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import threading

# ç®€åŒ–çš„ç®¡ç†å™¨ç±» - é¿å…ä¾èµ–é—®é¢˜
class SimpleInterfaceManager:
    """ç®€åŒ–çš„æ¥å£ç®¡ç†å™¨"""
    def __init__(self):
        self.reservations = {}
    
    def reserve_interface(self, interface_id, vehicle_id, current_time, duration):
        self.reservations[interface_id] = {
            'vehicle_id': vehicle_id,
            'start_time': current_time,
            'duration': duration
        }
        return True
    
    def release_interface(self, interface_id, vehicle_id):
        if interface_id in self.reservations:
            del self.reservations[interface_id]
    
    def cleanup_expired_reservations(self, current_time):
        expired = []
        for interface_id, reservation in self.reservations.items():
            if current_time - reservation['start_time'] > reservation['duration']:
                expired.append(interface_id)
        
        for interface_id in expired:
            del self.reservations[interface_id]
    
    def get_interface_congestion_factor(self, interface_id):
        return 1.2 if interface_id in self.reservations else 1.0

class SimpleStabilityManager:
    """ç®€åŒ–çš„ç¨³å®šæ€§ç®¡ç†å™¨"""
    def __init__(self):
        self.vehicle_switches = {}
    
    def can_vehicle_switch(self, vehicle_id):
        return True  # ç®€åŒ–ç‰ˆæ€»æ˜¯å…è®¸åˆ‡æ¢
    
    def record_vehicle_switch(self, vehicle_id, path_id):
        self.vehicle_switches[vehicle_id] = path_id
    
    def get_stability_report(self):
        return {
            'overall_stability': 0.9,
            'vehicle_switches': len(self.vehicle_switches)
        }

class SimpleSafetyManager:
    """ç®€åŒ–çš„å®‰å…¨ç®¡ç†å™¨"""
    def __init__(self):
        self.vehicle_safety_params = {}
    
    def is_interface_safe_for_vehicle(self, position, vehicle_id):
        return True  # ç®€åŒ–ç‰ˆæ€»æ˜¯å®‰å…¨
    
    def calculate_interface_safety_score(self, position, vehicle_id):
        return 0.9  # ç®€åŒ–ç‰ˆè¿”å›å›ºå®šåˆ†æ•°

@dataclass
class BiDirectionalPath:
    """åŒå‘è·¯å¾„æ•°æ®ç»“æ„ - å¢å¼ºç‰ˆ"""
    path_id: str
    point_a: Dict  # èµ·ç‚¹ä¿¡æ¯
    point_b: Dict  # ç»ˆç‚¹ä¿¡æ¯
    forward_path: List[Tuple]  # A->Bè·¯å¾„
    reverse_path: List[Tuple]  # B->Aè·¯å¾„ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
    length: float
    quality: float
    planner_used: str
    created_time: float
    usage_count: int = 0
    current_load: int = 0  # å½“å‰ä½¿ç”¨è¯¥è·¯å¾„çš„è½¦è¾†æ•°
    max_capacity: int = 5  # æœ€å¤§å®¹é‡
    
    # æ–°å¢ï¼šè´¨é‡å†å²è¿½è¸ª
    quality_history: List[float] = None
    last_quality_update: float = 0.0
    
    # æ–°å¢ï¼šå¢å¼ºç‰ˆä¸“ä¸šè®¾è®¡ç›¸å…³æ ‡è®°
    is_professional_design: bool = False
    is_optimized_design: bool = False
    road_class: Optional[str] = None  # "primary", "secondary", "service"
    design_class: str = "standard"
    consolidation_level: str = "original"  # "original", "consolidated", "enhanced_node_clustering_professional"
    
    # æ–°å¢ï¼šèŠ‚ç‚¹èšç±»ç›¸å…³å±æ€§
    key_nodes: List[str] = None  # å…³é”®èŠ‚ç‚¹IDåˆ—è¡¨
    is_node_clustered: bool = False
    node_reduction_ratio: float = 0.0
    
    # æ–°å¢ï¼šå·¥ç¨‹æ ‡å‡†
    engineering_standards: Dict = None
    node_spacing: float = 15.0
    safety_rating: float = 1.0
    meets_standards: bool = True
    
    def __post_init__(self):
        if self.quality_history is None:
            self.quality_history = [self.quality]
        if self.engineering_standards is None:
            self.engineering_standards = {}
        if self.key_nodes is None:
            self.key_nodes = []
    
    def get_path(self, from_point_type: str, from_point_id: int, 
                to_point_type: str, to_point_id: int) -> Optional[List[Tuple]]:
        """è·å–æŒ‡å®šæ–¹å‘çš„è·¯å¾„"""
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…A->Bæ–¹å‘
        if (self.point_a['type'] == from_point_type and self.point_a['id'] == from_point_id and
            self.point_b['type'] == to_point_type and self.point_b['id'] == to_point_id):
            return self.forward_path
        
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…B->Aæ–¹å‘
        if (self.point_b['type'] == from_point_type and self.point_b['id'] == from_point_id and
            self.point_a['type'] == to_point_type and self.point_a['id'] == to_point_id):
            return self.reverse_path
        
        return None
    
    def increment_usage(self):
        """å¢åŠ ä½¿ç”¨è®¡æ•°"""
        self.usage_count += 1
    
    def add_vehicle(self, vehicle_id: str):
        """æ·»åŠ è½¦è¾†åˆ°è·¯å¾„"""
        self.current_load += 1
    
    def remove_vehicle(self, vehicle_id: str):
        """ä»è·¯å¾„ç§»é™¤è½¦è¾†"""
        self.current_load = max(0, self.current_load - 1)
    
    def get_load_factor(self) -> float:
        """è·å–è´Ÿè½½å› å­"""
        return self.current_load / self.max_capacity
    
    def update_quality_history(self, new_quality: float):
        """æ›´æ–°è´¨é‡å†å²"""
        self.quality_history.append(new_quality)
        self.last_quality_update = time.time()
        
        # é™åˆ¶å†å²é•¿åº¦
        if len(self.quality_history) > 20:
            self.quality_history = self.quality_history[-10:]
    
    def get_average_quality(self) -> float:
        """è·å–å¹³å‡è´¨é‡"""
        if not self.quality_history:
            return self.quality
        return sum(self.quality_history) / len(self.quality_history)

# å¢å¼ºç‰ˆä¸“ä¸šé“è·¯æ•´åˆæ¨¡å—å¯¼å…¥
try:
    from node_clustering_professional_consolidator import (
        EnhancedNodeClusteringConsolidator,
        NodeClusteringConsolidator,  # å‘åå…¼å®¹
        RoadClass,
        KeyNode,
        EnhancedConsolidatedBackbonePath,
        create_enhanced_node_clustering_consolidator,
        apply_enhanced_consolidation
    )
    ENHANCED_PROFESSIONAL_CONSOLIDATION_AVAILABLE = True
    print("âœ… å¢å¼ºç‰ˆä¸“ä¸šé“è·¯æ•´åˆæ¨¡å—åŠ è½½æˆåŠŸ")
except ImportError as e:
    # å›é€€åˆ°åŸºç¡€ç‰ˆæœ¬
    try:
        from node_clustering_professional_consolidator import (
            NodeClusteringConsolidator,
            RoadClass,
            KeyNode
        )
        ENHANCED_PROFESSIONAL_CONSOLIDATION_AVAILABLE = False
        print(f"âš ï¸ å¢å¼ºç‰ˆä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€ç‰ˆæœ¬: {e}")
    except ImportError:
        ENHANCED_PROFESSIONAL_CONSOLIDATION_AVAILABLE = False
        print(f"âŒ ä¸“ä¸šé“è·¯æ•´åˆæ¨¡å—å®Œå…¨ä¸å¯ç”¨: {e}")

class OptimizedBackboneNetwork:
    """é›†æˆå¢å¼ºç‰ˆä¸“ä¸šé“è·¯æ•´åˆçš„ä¼˜åŒ–éª¨å¹²è·¯å¾„ç½‘ç»œ"""
    
    def __init__(self, env):
        self.env = env
        self.path_planner = None
        
        # æ ¸å¿ƒæ•°æ®ç»“æ„
        self.bidirectional_paths = {}
        self.special_points = {'loading': [], 'unloading': [], 'parking': []}
        
        # ç¬¬ä¸€é˜¶æ®µçŠ¶æ€è·Ÿè¸ª - å¢å¼ºç‰ˆ
        self.stage1_progress = {
            'current_step': 0,
            'total_steps': 5,
            'step_names': [
                'åŒå‘è·¯å¾„æ™ºèƒ½è§„åˆ’',
                'åŠ¨æ€èŠ‚ç‚¹å¯†åº¦æ§åˆ¶', 
                'å…³é”®èŠ‚ç‚¹èšç±»æå–',
                'è½¦è¾†åŠ¨åŠ›å­¦çº¦æŸæ‹Ÿåˆ',
                'å›¾æ‹“æ‰‘æ ‡å‡†åŒ–è¾“å‡º'
            ],
            'completed_steps': [],
            'step_details': {},
            'step_status': {},  # æ–°å¢ï¼šæ¯ä¸ªæ­¥éª¤çš„çŠ¶æ€
            'can_execute_manually': {  # æ–°å¢ï¼šæ‰‹åŠ¨æ‰§è¡Œèƒ½åŠ›
                'åŒå‘è·¯å¾„æ™ºèƒ½è§„åˆ’': True,
                'åŠ¨æ€èŠ‚ç‚¹å¯†åº¦æ§åˆ¶': True,
                'å…³é”®èŠ‚ç‚¹èšç±»æå–': False,  # éœ€è¦å‰ä¸¤æ­¥å®Œæˆ
                'è½¦è¾†åŠ¨åŠ›å­¦çº¦æŸæ‹Ÿåˆ': False,  # éœ€è¦èšç±»å®Œæˆ
                'å›¾æ‹“æ‰‘æ ‡å‡†åŒ–è¾“å‡º': False     # éœ€è¦æ‹Ÿåˆå®Œæˆ
            }
        }
        
        # åŸå§‹è·¯å¾„çŠ¶æ€ï¼ˆèšç±»å‰ï¼‰
        self.raw_paths_state = {
            'generated': False,
            'paths_count': 0,
            'total_nodes': 0,
            'avg_path_length': 0.0,
            'avg_quality': 0.0,
            'generation_time': 0.0,
            'detailed_info': {}  # æ–°å¢ï¼šè¯¦ç»†ä¿¡æ¯
        }
        
        # å¢å¼ºç‰ˆä¸“ä¸šæ•´åˆå™¨
        self.professional_consolidator = None
        self.enhanced_professional_consolidator = None  # æ–°å¢ï¼šå¢å¼ºç‰ˆæ•´åˆå™¨
        self.professional_design_applied = False
        self.original_paths_backup = {}
        self.professional_design_info = {}
        
        # æ¥å£ç³»ç»Ÿ
        self.backbone_interfaces = {}
        self.path_interfaces = defaultdict(list)
        
        # è·¯å¾„æŸ¥æ‰¾ç´¢å¼•
        self.connection_index = {}
        
        # è´Ÿè½½å‡è¡¡è¿½è¸ª
        self.vehicle_path_assignments = {}
        self.path_load_history = defaultdict(list)
        
        # ç®€åŒ–çš„ç®¡ç†å™¨
        self.interface_manager = SimpleInterfaceManager()
        self.stability_manager = SimpleStabilityManager()
        self.safe_interface_manager = SimpleSafetyManager()
        
        # å¢å¼ºé…ç½®
        self.config = {
            'primary_quality_threshold': 0.7,
            'fallback_quality_threshold': 0.4,
            'max_planning_time_per_path': 20.0,
            'enable_progressive_fallback': True,
            'interface_spacing': 8,
            'retry_with_relaxed_params': True,
            'load_balancing_weight': 0.3,
            'path_switching_threshold': 0.8,
            'interface_reservation_duration': 60.0,
            
            # å¢å¼ºç‰ˆé…ç½®
            'enable_enhanced_professional_consolidation': ENHANCED_PROFESSIONAL_CONSOLIDATION_AVAILABLE,
            'enhanced_design_mode': 'professional',  # 'professional', 'balanced', 'performance'
            'enable_enhanced_curve_fitting': True,
            'force_vehicle_dynamics': True,
            'curve_fitting_quality_threshold': 0.7,
            'preserve_intermediate_states': True,  # ä¿ç•™ä¸­é—´çŠ¶æ€ç”¨äºå¯è§†åŒ–
            
            # ä¼ ç»Ÿé…ç½®ä¿æŒå…¼å®¹
            'professional_design_mode': 'balanced',
            'clustering_radius': 12.0,
            'enable_path_reconstruction': True,
            'enable_node_optimization': True,
            'preserve_original_backup': True,
            'auto_consolidate_after_generation': True,
            
            'enable_safety_optimization': True,
            'enable_stability_management': True,
            'safety_margin': 3.0,
        }
        
        # å¢å¼ºç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_path_pairs': 0,
            'successful_paths': 0,
            'astar_success': 0,
            'rrt_success': 0,
            'direct_fallback': 0,
            'generation_time': 0,
            'loading_to_unloading': 0,
            'loading_to_parking': 0,
            'unloading_to_parking': 0,
            'load_balancing_decisions': 0,
            'path_switches': 0,
            'interface_reservations': 0,
            'safety_optimizations': 0,
            
            # å¢å¼ºç‰ˆç»Ÿè®¡
            'enhanced_consolidation_applied': False,
            'enhanced_curve_fitting_used': False,
            'original_paths_count': 0,
            'consolidated_paths_count': 0,
            'consolidation_time': 0.0,
            'enhanced_professional_design_applied': False,
            
            # è¯¦ç»†çš„ç¬¬ä¸€é˜¶æ®µç»Ÿè®¡
            'stage1_step_times': {},
            'stage1_step_quality': {},
            'stage1_intermediate_states': {},
            
            # èŠ‚ç‚¹èšç±»ç»Ÿè®¡ - å¢å¼ºç‰ˆ
            'original_nodes_count': 0,
            'key_nodes_count': 0,
            'node_reduction_ratio': 0.0,
            'reconstruction_success_rate': 0.0,
            'enhanced_reconstruction_success_rate': 0.0,
            
            # æ›²çº¿æ‹Ÿåˆç»Ÿè®¡ - å¢å¼ºç‰ˆ
            'enhanced_curve_fitting_success': 0,
            'complete_curve_success': 0,
            'segmented_curve_success': 0,
            'fallback_reconstruction': 0,
            'avg_curve_quality': 0.0,
            'dynamics_compliance_rate': 0.0,
            'turning_radius_violations': 0,
            'grade_violations': 0,
            
            # ä¸“ä¸šè®¾è®¡ç»Ÿè®¡
            'primary_roads_count': 0,
            'secondary_roads_count': 0,
            'service_roads_count': 0,
            'average_safety_rating': 0.0,
            'standards_compliance_rate': 0.0,
            'total_construction_cost': 0.0,
        }
        
        print("åˆå§‹åŒ–å¢å¼ºç‰ˆé›†æˆçš„ä¼˜åŒ–éª¨å¹²è·¯å¾„ç½‘ç»œ")
        print(f"  å¢å¼ºç‰ˆä¸“ä¸šæ•´åˆåŠŸèƒ½: {'âœ…' if ENHANCED_PROFESSIONAL_CONSOLIDATION_AVAILABLE else 'âŒ'}")
    
    # ==================== å¢å¼ºç‰ˆç¬¬ä¸€é˜¶æ®µæ ¸å¿ƒå®ç° ====================
    
    def generate_backbone_network(self, quality_threshold: float = None, 
                                enable_consolidation: bool = None) -> bool:
        """
        ç”Ÿæˆéª¨å¹²ç½‘ç»œ - å¢å¼ºç‰ˆç¬¬ä¸€é˜¶æ®µï¼šæ™ºèƒ½æ‹“æ‰‘æ„å»º
        """
        start_time = time.time()
        print("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆç¬¬ä¸€é˜¶æ®µï¼šæ™ºèƒ½æ‹“æ‰‘æ„å»º")
        
        # æ›´æ–°é…ç½®
        if quality_threshold is not None:
            self.config['primary_quality_threshold'] = quality_threshold
        
        if enable_consolidation is not None:
            self.config['auto_consolidate_after_generation'] = enable_consolidation
        
        try:
            # === æ­¥éª¤1-2: åŒå‘è·¯å¾„æ™ºèƒ½è§„åˆ’ + åŠ¨æ€èŠ‚ç‚¹å¯†åº¦æ§åˆ¶ ===
            success = self._execute_step1_and_step2()
            if not success:
                return False
            
            # è®°å½•åŸå§‹è·¯å¾„çŠ¶æ€ï¼ˆèšç±»å‰çš„çŠ¶æ€ï¼‰
            self._record_enhanced_raw_paths_state(start_time)
            
            # === å¯é€‰ï¼šæ‰§è¡Œå¢å¼ºç‰ˆä¸“ä¸šæ•´åˆï¼ˆæ­¥éª¤3-5ï¼‰===
            if self.config['auto_consolidate_after_generation']:
                consolidation_success = self._execute_enhanced_remaining_stage1_steps()
                if not consolidation_success:
                    print("âš ï¸ å¢å¼ºç‰ˆä¸“ä¸šæ•´åˆå¤±è´¥ï¼Œä½†åŸå§‹è·¯å¾„ä»å¯ç”¨")
            else:
                print("ğŸ“‹ å¢å¼ºç‰ˆä¸“ä¸šæ•´åˆå·²ç¦ç”¨ï¼Œå¯æ‰‹åŠ¨æ‰§è¡Œå‰©ä½™æ­¥éª¤")
                self._mark_steps_ready_for_manual_execution()
            
            # æ›´æ–°æœ€ç»ˆç»Ÿè®¡
            generation_time = time.time() - start_time
            self.stats.update({
                'successful_paths': len(self.bidirectional_paths),
                'generation_time': generation_time
            })
            
            success_rate = len(self.bidirectional_paths) / max(1, self.stats['total_path_pairs'])
            
            print(f"\nğŸ‰ å¢å¼ºç‰ˆç¬¬ä¸€é˜¶æ®µï¼šæ™ºèƒ½æ‹“æ‰‘æ„å»ºå®Œæˆ!")
            print(f"  æ‰§è¡Œæ­¥éª¤: {len(self.stage1_progress['completed_steps'])}/{self.stage1_progress['total_steps']}")
            print(f"  åŒå‘è·¯å¾„: {len(self.bidirectional_paths)} æ¡")
            print(f"  æˆåŠŸç‡: {success_rate:.1%}")
            print(f"  æ€»è€—æ—¶: {generation_time:.2f}s")
            print(f"  å¢å¼ºç‰ˆæ•´åˆ: {'âœ…' if self.stats['enhanced_consolidation_applied'] else 'âŒ'}")
            
            return True
        
        except Exception as e:
            print(f"âŒ å¢å¼ºç‰ˆç¬¬ä¸€é˜¶æ®µæ„å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _execute_step1_and_step2(self) -> bool:
        """æ‰§è¡Œæ­¥éª¤1å’Œ2ï¼šåŒå‘è·¯å¾„æ™ºèƒ½è§„åˆ’ + åŠ¨æ€èŠ‚ç‚¹å¯†åº¦æ§åˆ¶"""
        step_start = time.time()
        
        # === æ­¥éª¤1: åŒå‘è·¯å¾„æ™ºèƒ½è§„åˆ’ ===
        self._update_enhanced_stage1_progress(1, "åŒå‘è·¯å¾„æ™ºèƒ½è§„åˆ’", "æ‰§è¡Œä¸­")
        
        # åŠ è½½ç‰¹æ®Šç‚¹
        self._load_special_points()
        if not self._validate_special_points():
            self._update_enhanced_stage1_progress(1, "åŒå‘è·¯å¾„æ™ºèƒ½è§„åˆ’", "å¤±è´¥")
            return False
        
        # ç”Ÿæˆæ‰€æœ‰è·¯å¾„ç»„åˆ
        success_count = self._generate_complete_bidirectional_paths()
        
        if success_count == 0:
            print("âŒ æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•éª¨å¹²è·¯å¾„")
            self._update_enhanced_stage1_progress(1, "åŒå‘è·¯å¾„æ™ºèƒ½è§„åˆ’", "å¤±è´¥")
            return False
        
        step1_time = time.time() - step_start
        self.stats['stage1_step_times']['åŒå‘è·¯å¾„æ™ºèƒ½è§„åˆ’'] = step1_time
        self._update_enhanced_stage1_progress(1, "åŒå‘è·¯å¾„æ™ºèƒ½è§„åˆ’", "å®Œæˆ")
        
        # === æ­¥éª¤2: åŠ¨æ€èŠ‚ç‚¹å¯†åº¦æ§åˆ¶ ===
        step2_start = time.time()
        self._update_enhanced_stage1_progress(2, "åŠ¨æ€èŠ‚ç‚¹å¯†åº¦æ§åˆ¶", "æ‰§è¡Œä¸­")
        
        total_interfaces = self._generate_safe_interfaces()
        
        # å»ºç«‹è¿æ¥ç´¢å¼•
        self._build_connection_index()
        
        # åˆå§‹åŒ–è´¨é‡è¿½è¸ª
        self._initialize_quality_tracking()
        
        step2_time = time.time() - step2_start
        self.stats['stage1_step_times']['åŠ¨æ€èŠ‚ç‚¹å¯†åº¦æ§åˆ¶'] = step2_time
        self._update_enhanced_stage1_progress(2, "åŠ¨æ€èŠ‚ç‚¹å¯†åº¦æ§åˆ¶", "å®Œæˆ")
        
        print(f"\nâœ… ç¬¬ä¸€é˜¶æ®µå‰ä¸¤æ­¥å®Œæˆï¼")
        print(f"  åŒå‘è·¯å¾„: {len(self.bidirectional_paths)} æ¡")
        print(f"  æ€»èŠ‚ç‚¹æ•°: è®¡ç®—ä¸­...")
        print(f"  å®‰å…¨æ¥å£æ•°é‡: {total_interfaces} ä¸ª")
        print(f"  æ­¥éª¤1è€—æ—¶: {step1_time:.2f}s, æ­¥éª¤2è€—æ—¶: {step2_time:.2f}s")
        
        return True
    
    def _execute_enhanced_remaining_stage1_steps(self) -> bool:
        """æ‰§è¡Œå¢å¼ºç‰ˆç¬¬ä¸€é˜¶æ®µå‰©ä½™æ­¥éª¤ï¼ˆæ­¥éª¤3-5ï¼‰"""
        try:
            if len(self.bidirectional_paths) < 2:
                print("è·¯å¾„æ•°é‡è¿‡å°‘ï¼Œè·³è¿‡å¢å¼ºç‰ˆä¸“ä¸šæ•´åˆ...")
                return True
            
            print(f"\nğŸ”§ å¼€å§‹åº”ç”¨å¢å¼ºç‰ˆä¸“ä¸šé“è·¯æ•´åˆ...")
            consolidation_start = time.time()
            
            # å¤‡ä»½åŸå§‹è·¯å¾„
            if self.config['preserve_original_backup']:
                self.original_paths_backup = self.bidirectional_paths.copy()
            
            # === æ­¥éª¤3: å…³é”®èŠ‚ç‚¹èšç±»æå– ===
            step3_success = self._execute_enhanced_step3()
            if not step3_success:
                return False
            
            # === æ­¥éª¤4: è½¦è¾†åŠ¨åŠ›å­¦çº¦æŸæ‹Ÿåˆ ===
            step4_success = self._execute_enhanced_step4()
            if not step4_success:
                return False
            
            # === æ­¥éª¤5: å›¾æ‹“æ‰‘æ ‡å‡†åŒ–è¾“å‡º ===
            step5_success = self._execute_enhanced_step5()
            if not step5_success:
                return False
            
            # æ›´æ–°æ•´åˆç»Ÿè®¡ä¿¡æ¯
            consolidation_time = time.time() - consolidation_start
            self._update_enhanced_consolidation_stats(consolidation_time)
            
            print(f"âœ… å¢å¼ºç‰ˆç¬¬ä¸€é˜¶æ®µå®Œæ•´æ„å»ºæˆåŠŸ")
            print(f"   å¢å¼ºç‰ˆæ•´åˆè€—æ—¶: {consolidation_time:.2f}s")
            
            return True
        
        except Exception as e:
            print(f"âŒ å¢å¼ºç‰ˆç¬¬ä¸€é˜¶æ®µå‰©ä½™æ­¥éª¤æ‰§è¡Œå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _execute_enhanced_step3(self) -> bool:
        """å¢å¼ºç‰ˆæ­¥éª¤3: å…³é”®èŠ‚ç‚¹èšç±»æå–"""
        step_start = time.time()
        self._update_enhanced_stage1_progress(3, "å…³é”®èŠ‚ç‚¹èšç±»æå–", "æ‰§è¡Œä¸­")
        
        try:
            # åˆ›å»ºå¢å¼ºç‰ˆæ•´åˆå™¨
            design_mode = self.config['enhanced_design_mode']
            enhanced_config = self._get_enhanced_professional_config_by_mode(design_mode)
            
            if ENHANCED_PROFESSIONAL_CONSOLIDATION_AVAILABLE:
                self.enhanced_professional_consolidator = EnhancedNodeClusteringConsolidator(
                    self.env, enhanced_config
                )
                # ä¿æŒä¼ ç»Ÿæ¥å£å…¼å®¹
                self.professional_consolidator = self.enhanced_professional_consolidator
                
                # âœ¨ è°ƒè¯•ï¼šéªŒè¯æ–¹æ³•å­˜åœ¨
                required_methods = [
                    '_extract_original_paths',
                    '_identify_and_cluster_endpoints_optimized',  # âœ… ä¿®å¤åçš„æ–¹æ³•å
                    '_perform_multi_round_clustering', 
                    '_generate_key_nodes',
                    '_enhanced_reconstruct_backbone_paths',
                    '_apply_consolidation_to_backbone'
                ]
                
                for method_name in required_methods:
                    if not hasattr(self.professional_consolidator, method_name):
                        print(f"âŒ ç¼ºå°‘æ–¹æ³•: {method_name}")
                        return False
                    else:
                        print(f"âœ… æ–¹æ³•å­˜åœ¨: {method_name}")
                        
            else:
                # å›é€€åˆ°åŸºç¡€ç‰ˆæœ¬
                self.professional_consolidator = NodeClusteringConsolidator(
                    self.env, enhanced_config
                )
            
            # æ‰§è¡Œèšç±»é˜¶æ®µ
            success = self._execute_clustering_phase()
            
            if success:
                step3_time = time.time() - step_start
                self.stats['stage1_step_times']['å…³é”®èŠ‚ç‚¹èšç±»æå–'] = step3_time
                self._update_enhanced_stage1_progress(3, "å…³é”®èŠ‚ç‚¹èšç±»æå–", "å®Œæˆ")
                print(f"âœ… æ­¥éª¤3å®Œæˆï¼Œè€—æ—¶: {step3_time:.2f}s")
                return True
            else:
                self._update_enhanced_stage1_progress(3, "å…³é”®èŠ‚ç‚¹èšç±»æå–", "å¤±è´¥")
                return False
        
        except Exception as e:
            print(f"âŒ å¢å¼ºç‰ˆæ­¥éª¤3å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            self._update_enhanced_stage1_progress(3, "å…³é”®èŠ‚ç‚¹èšç±»æå–", "å¤±è´¥")
            return False
    
    def _execute_enhanced_step4(self) -> bool:
        """å¢å¼ºç‰ˆæ­¥éª¤4: è½¦è¾†åŠ¨åŠ›å­¦çº¦æŸæ‹Ÿåˆ"""
        step_start = time.time()
        self._update_enhanced_stage1_progress(4, "è½¦è¾†åŠ¨åŠ›å­¦çº¦æŸæ‹Ÿåˆ", "æ‰§è¡Œä¸­")
        
        try:
            # æ‰§è¡Œå¢å¼ºç‰ˆè·¯å¾„é‡å»º
            success = self._execute_enhanced_curve_fitting_phase()
            
            if success:
                step4_time = time.time() - step_start
                self.stats['stage1_step_times']['è½¦è¾†åŠ¨åŠ›å­¦çº¦æŸæ‹Ÿåˆ'] = step4_time
                self._update_enhanced_stage1_progress(4, "è½¦è¾†åŠ¨åŠ›å­¦çº¦æŸæ‹Ÿåˆ", "å®Œæˆ")
                print(f"âœ… æ­¥éª¤4å®Œæˆï¼Œè€—æ—¶: {step4_time:.2f}s")
                return True
            else:
                self._update_enhanced_stage1_progress(4, "è½¦è¾†åŠ¨åŠ›å­¦çº¦æŸæ‹Ÿåˆ", "å¤±è´¥")
                return False
        
        except Exception as e:
            print(f"âŒ å¢å¼ºç‰ˆæ­¥éª¤4å¼‚å¸¸: {e}")
            self._update_enhanced_stage1_progress(4, "è½¦è¾†åŠ¨åŠ›å­¦çº¦æŸæ‹Ÿåˆ", "å¤±è´¥")
            return False
    
    def _execute_enhanced_step5(self) -> bool:
        """å¢å¼ºç‰ˆæ­¥éª¤5: å›¾æ‹“æ‰‘æ ‡å‡†åŒ–è¾“å‡º"""
        step_start = time.time()
        self._update_enhanced_stage1_progress(5, "å›¾æ‹“æ‰‘æ ‡å‡†åŒ–è¾“å‡º", "æ‰§è¡Œä¸­")
        
        try:
            # åº”ç”¨æ•´åˆç»“æœåˆ°éª¨å¹²ç½‘ç»œ
            success = self._apply_enhanced_consolidation_results()
            
            if success:
                step5_time = time.time() - step_start
                self.stats['stage1_step_times']['å›¾æ‹“æ‰‘æ ‡å‡†åŒ–è¾“å‡º'] = step5_time
                self._update_enhanced_stage1_progress(5, "å›¾æ‹“æ‰‘æ ‡å‡†åŒ–è¾“å‡º", "å®Œæˆ")
                print(f"âœ… æ­¥éª¤5å®Œæˆï¼Œè€—æ—¶: {step5_time:.2f}s")
                return True
            else:
                self._update_enhanced_stage1_progress(5, "å›¾æ‹“æ‰‘æ ‡å‡†åŒ–è¾“å‡º", "å¤±è´¥")
                return False
        
        except Exception as e:
            print(f"âŒ å¢å¼ºç‰ˆæ­¥éª¤5å¼‚å¸¸: {e}")
            self._update_enhanced_stage1_progress(5, "å›¾æ‹“æ‰‘æ ‡å‡†åŒ–è¾“å‡º", "å¤±è´¥")
            return False
    
    def _execute_clustering_phase(self) -> bool:
        """âœ… ä¿®å¤ï¼šæ‰§è¡Œèšç±»é˜¶æ®µ"""
        if not self.professional_consolidator:
            return False
        
        # æå–åŸå§‹è·¯å¾„
        success = self.professional_consolidator._extract_original_paths(self)
        if not success:
            return False
        
        # âœ¨ ä¿®å¤ï¼šä½¿ç”¨æ–°çš„ä¼˜åŒ–ç«¯ç‚¹èšç±»æ–¹æ³•
        success = self.professional_consolidator._identify_and_cluster_endpoints_optimized()
        if not success:
            return False
        
        # æ‰§è¡Œå¤šè½®èšç±»
        success = self.professional_consolidator._perform_multi_round_clustering()
        if not success:
            return False
        
        # ç”Ÿæˆå…³é”®èŠ‚ç‚¹
        success = self.professional_consolidator._generate_key_nodes()
        return success
    
    def _execute_enhanced_curve_fitting_phase(self) -> bool:
        """æ‰§è¡Œå¢å¼ºç‰ˆæ›²çº¿æ‹Ÿåˆé˜¶æ®µ"""
        if not self.professional_consolidator:
            return False
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¢å¼ºç‰ˆæ•´åˆå™¨
        if hasattr(self.professional_consolidator, '_enhanced_reconstruct_backbone_paths'):
            # ä½¿ç”¨å¢å¼ºç‰ˆé‡å»ºæ–¹æ³•
            success = self.professional_consolidator._enhanced_reconstruct_backbone_paths()
            if success:
                self.stats['enhanced_curve_fitting_used'] = True
                return True
        
        # å¦‚æœæ²¡æœ‰å¢å¼ºç‰ˆæ–¹æ³•ï¼Œè¿”å›False
        print("       âš ï¸ å¢å¼ºç‰ˆæ›²çº¿æ‹Ÿåˆæ–¹æ³•ä¸å¯ç”¨")
        return False
    
    def _apply_enhanced_consolidation_results(self) -> bool:
        """åº”ç”¨å¢å¼ºç‰ˆæ•´åˆç»“æœ"""
        if not self.professional_consolidator:
            return False
        
        # åº”ç”¨æ•´åˆç»“æœåˆ°éª¨å¹²ç½‘ç»œ
        if hasattr(self.professional_consolidator, '_apply_consolidation_to_backbone'):
            return self.professional_consolidator._apply_consolidation_to_backbone(self)
        
        return False
    
    # ==================== æ‰‹åŠ¨æ‰§è¡Œå•ä¸ªæ­¥éª¤æ¥å£ ====================
    
    def execute_single_stage1_step(self, step_name: str) -> bool:
        """æ‰§è¡Œå•ä¸ªç¬¬ä¸€é˜¶æ®µæ­¥éª¤ - å¢å¼ºç‰ˆ"""
        print(f"ğŸ¯ æ‰‹åŠ¨æ‰§è¡Œæ­¥éª¤: {step_name}")
        
        # æ£€æŸ¥æ­¥éª¤æ˜¯å¦å¯ä»¥æ‰§è¡Œ
        if not self._can_execute_step(step_name):
            print(f"âŒ æ­¥éª¤ '{step_name}' å½“å‰ä¸å¯æ‰§è¡Œï¼Œè¯·å…ˆå®Œæˆå‰ç½®æ­¥éª¤")
            return False
        
        try:
            if step_name == "åŒå‘è·¯å¾„æ™ºèƒ½è§„åˆ’":
                return self._manual_execute_step1_2()
            
            elif step_name == "åŠ¨æ€èŠ‚ç‚¹å¯†åº¦æ§åˆ¶":
                return self._manual_execute_step1_2()
            
            elif step_name == "å…³é”®èŠ‚ç‚¹èšç±»æå–":
                return self._execute_enhanced_step3()
            
            elif step_name == "è½¦è¾†åŠ¨åŠ›å­¦çº¦æŸæ‹Ÿåˆ":
                return self._execute_enhanced_step4()
            
            elif step_name == "å›¾æ‹“æ‰‘æ ‡å‡†åŒ–è¾“å‡º":
                return self._execute_enhanced_step5()
            
            else:
                print(f"âŒ æœªçŸ¥æ­¥éª¤: {step_name}")
                return False
        
        except Exception as e:
            print(f"âŒ æ‰‹åŠ¨æ‰§è¡Œæ­¥éª¤å¤±è´¥: {e}")
            return False
    
    def _manual_execute_step1_2(self) -> bool:
        """æ‰‹åŠ¨æ‰§è¡Œæ­¥éª¤1å’Œ2"""
        if not self.raw_paths_state['generated']:
            return self._execute_step1_and_step2()
        else:
            print("âœ… æ­¥éª¤1-2å·²å®Œæˆ")
            return True
    
    def _can_execute_step(self, step_name: str) -> bool:
        """æ£€æŸ¥æ­¥éª¤æ˜¯å¦å¯ä»¥æ‰§è¡Œ"""
        if step_name in ["åŒå‘è·¯å¾„æ™ºèƒ½è§„åˆ’", "åŠ¨æ€èŠ‚ç‚¹å¯†åº¦æ§åˆ¶"]:
            return True
        
        if step_name == "å…³é”®èŠ‚ç‚¹èšç±»æå–":
            return self.raw_paths_state['generated']
        
        if step_name == "è½¦è¾†åŠ¨åŠ›å­¦çº¦æŸæ‹Ÿåˆ":
            return (self.professional_consolidator and 
                    hasattr(self.professional_consolidator, 'key_nodes') and
                    self.professional_consolidator.key_nodes)
        
        if step_name == "å›¾æ‹“æ‰‘æ ‡å‡†åŒ–è¾“å‡º":
            return (self.professional_consolidator and 
                    hasattr(self.professional_consolidator, 'consolidated_paths') and
                    self.professional_consolidator.consolidated_paths)
        
        return False
    
    # ==================== å¢å¼ºç‰ˆçŠ¶æ€ç®¡ç†å’ŒæŸ¥è¯¢æ¥å£ ====================
    
    def get_raw_backbone_paths_info(self) -> Dict:
        """è·å–åŸå§‹éª¨å¹²è·¯å¾„ä¿¡æ¯ï¼ˆèšç±»å‰çŠ¶æ€ï¼‰- å¢å¼ºç‰ˆ"""
        if not self.raw_paths_state['generated']:
            return {'status': 'not_generated'}
        
        # æå–è·¯å¾„è¯¦ç»†ä¿¡æ¯
        paths_info = {}
        for path_id, path_data in self.bidirectional_paths.items():
            paths_info[path_id] = {
                'path_id': path_id,
                'start_point': path_data.point_a,
                'end_point': path_data.point_b,
                'forward_path': path_data.forward_path,
                'reverse_path': path_data.reverse_path,
                'length': path_data.length,
                'quality': path_data.quality,
                'planner_used': path_data.planner_used,
                'node_count': len(path_data.forward_path),
                'is_professional': getattr(path_data, 'is_professional_design', False),
                'consolidation_level': getattr(path_data, 'consolidation_level', 'original'),
                'created_time': path_data.created_time
            }
        
        return {
            'status': 'generated',
            'raw_state': self.raw_paths_state,
            'paths_info': paths_info,
            'interfaces_info': self.backbone_interfaces,
            'special_points': self.special_points,
            'generation_stats': {
                'total_path_pairs': self.stats['total_path_pairs'],
                'successful_paths': self.stats['successful_paths'],
                'success_rate': self.stats['successful_paths'] / max(1, self.stats['total_path_pairs']),
                'astar_success': self.stats['astar_success'],
                'rrt_success': self.stats['rrt_success'],
                'direct_fallback': self.stats['direct_fallback'],
                'generation_time': self.raw_paths_state['generation_time']
            }
        }
    
    def get_topology_construction_summary(self) -> Dict:
        """è·å–æ‹“æ‰‘æ„å»ºæ‘˜è¦ä¿¡æ¯ - å¢å¼ºç‰ˆ"""
        summary = {
            'stage1_progress': self.stage1_progress,
            'construction_stats': {
                'paths_generated': len(self.bidirectional_paths),
                'total_nodes': self.raw_paths_state.get('total_nodes', 0),
                'avg_quality': self.raw_paths_state.get('avg_quality', 0.0),
                'interfaces_count': len(self.backbone_interfaces),
                'step_times': self.stats.get('stage1_step_times', {})
            },
            'ready_for_stage2': False,
            'gnn_input_ready': False,
            'enhanced_features': {
                'enhanced_consolidation_available': ENHANCED_PROFESSIONAL_CONSOLIDATION_AVAILABLE,
                'enhanced_consolidation_applied': self.stats['enhanced_consolidation_applied'],
                'enhanced_curve_fitting_used': self.stats['enhanced_curve_fitting_used']
            }
        }
        
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆå¢å¼ºç‰ˆä¸“ä¸šæ•´åˆï¼ˆå‡†å¤‡è¿›å…¥ç¬¬äºŒé˜¶æ®µï¼‰
        if self.stats['enhanced_consolidation_applied']:
            summary.update({
                'ready_for_stage2': True,
                'gnn_input_ready': True,
                'consolidation_stats': self._get_enhanced_consolidation_summary()
            })
        
        return summary
    
    def _get_enhanced_consolidation_summary(self) -> Dict:
        """è·å–å¢å¼ºç‰ˆæ•´åˆæ‘˜è¦"""
        if not self.professional_consolidator:
            return {}
        
        # ä»å¢å¼ºç‰ˆæ•´åˆå™¨è·å–ç»Ÿè®¡
        if hasattr(self.professional_consolidator, 'get_consolidation_stats'):
            stats = self.professional_consolidator.get_consolidation_stats()
            
            summary = {
                'key_nodes_count': stats.get('key_nodes_count', 0),
                'node_reduction_ratio': stats.get('node_reduction_ratio', 0.0),
                'reconstruction_success_rate': stats.get('reconstruction_success_rate', 0.0),
                'consolidation_time': self.stats.get('consolidation_time', 0.0)
            }
            
            # å¢å¼ºç‰ˆç‰¹æœ‰ç»Ÿè®¡
            if ENHANCED_PROFESSIONAL_CONSOLIDATION_AVAILABLE:
                summary.update({
                    'enhanced_curve_fitting_success': stats.get('enhanced_curve_fitting_used', 0),
                    'complete_curve_success': stats.get('complete_curve_success', 0),
                    'avg_curve_quality': stats.get('avg_curve_quality', 0.0),
                    'dynamics_compliance_rate': stats.get('dynamics_compliance_rate', 0.0),
                    'turning_radius_violations': stats.get('turning_radius_violations', 0),
                    'grade_violations': stats.get('grade_violations', 0),
                    
                    # âœ¨ ç«¯ç‚¹èšç±»ç»Ÿè®¡
                    'endpoint_reduction_ratio': stats.get('endpoint_reduction_ratio', 0.0),
                    'endpoint_clusters_count': stats.get('endpoint_clusters_count', 0),
                    'merged_endpoint_paths': stats.get('merged_endpoint_paths', 0)
                })
            
            return summary
        
        return {}
    
    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def _update_enhanced_stage1_progress(self, step_num: int, step_description: str, status: str):
        """æ›´æ–°å¢å¼ºç‰ˆç¬¬ä¸€é˜¶æ®µè¿›åº¦"""
        self.stage1_progress['current_step'] = step_num
        step_name = self.stage1_progress['step_names'][step_num - 1]
        
        # æ›´æ–°çŠ¶æ€
        self.stage1_progress['step_status'][step_name] = status
        
        if status == "å®Œæˆ" and step_name not in self.stage1_progress['completed_steps']:
            self.stage1_progress['completed_steps'].append(step_name)
        
        self.stage1_progress['step_details'][step_name] = {
            'description': step_description,
            'timestamp': time.time(),
            'status': status
        }
        
        # æ›´æ–°æ‰‹åŠ¨æ‰§è¡Œèƒ½åŠ›
        self._update_manual_execution_capabilities()
        
        print(f"ğŸ“‹ ç¬¬ä¸€é˜¶æ®µ æ­¥éª¤{step_num}: {step_name} - {status}")
    
    def _update_manual_execution_capabilities(self):
        """æ›´æ–°æ‰‹åŠ¨æ‰§è¡Œèƒ½åŠ›"""
        completed = set(self.stage1_progress['completed_steps'])
        
        # æ­¥éª¤3éœ€è¦æ­¥éª¤1-2å®Œæˆ
        if 'åŒå‘è·¯å¾„æ™ºèƒ½è§„åˆ’' in completed and 'åŠ¨æ€èŠ‚ç‚¹å¯†åº¦æ§åˆ¶' in completed:
            self.stage1_progress['can_execute_manually']['å…³é”®èŠ‚ç‚¹èšç±»æå–'] = True
        
        # æ­¥éª¤4éœ€è¦æ­¥éª¤3å®Œæˆ
        if 'å…³é”®èŠ‚ç‚¹èšç±»æå–' in completed:
            self.stage1_progress['can_execute_manually']['è½¦è¾†åŠ¨åŠ›å­¦çº¦æŸæ‹Ÿåˆ'] = True
        
        # æ­¥éª¤5éœ€è¦æ­¥éª¤4å®Œæˆ
        if 'è½¦è¾†åŠ¨åŠ›å­¦çº¦æŸæ‹Ÿåˆ' in completed:
            self.stage1_progress['can_execute_manually']['å›¾æ‹“æ‰‘æ ‡å‡†åŒ–è¾“å‡º'] = True
    
    def _mark_steps_ready_for_manual_execution(self):
        """æ ‡è®°æ­¥éª¤ä¸ºå¯æ‰‹åŠ¨æ‰§è¡Œ"""
        self._update_manual_execution_capabilities()
        print("ğŸ“‹ å‰©ä½™æ­¥éª¤å·²å‡†å¤‡å¥½æ‰‹åŠ¨æ‰§è¡Œ")
    
    def _record_enhanced_raw_paths_state(self, start_time):
        """è®°å½•å¢å¼ºç‰ˆåŸå§‹è·¯å¾„çŠ¶æ€ï¼ˆèšç±»å‰ï¼‰"""
        # ç»Ÿè®¡èŠ‚ç‚¹æ•°
        total_nodes = 0
        total_length = 0
        total_quality = 0
        detailed_info = {}
        
        for path_id, path_data in self.bidirectional_paths.items():
            if path_data.forward_path:
                nodes_count = len(path_data.forward_path)
                total_nodes += nodes_count
                total_length += path_data.length
                total_quality += path_data.quality
                
                detailed_info[path_id] = {
                    'nodes_count': nodes_count,
                    'length': path_data.length,
                    'quality': path_data.quality,
                    'planner_used': path_data.planner_used
                }
        
        paths_count = len(self.bidirectional_paths)
        
        self.raw_paths_state = {
            'generated': True,
            'paths_count': paths_count,
            'total_nodes': total_nodes,
            'avg_path_length': total_length / max(1, paths_count),
            'avg_quality': total_quality / max(1, paths_count),
            'generation_time': time.time() - start_time,
            'detailed_info': detailed_info
        }
        
        # è®°å½•åŸå§‹è·¯å¾„ç»Ÿè®¡
        self.stats['original_paths_count'] = paths_count
        self.stats['original_nodes_count'] = total_nodes
        
        print(f"ğŸ“Š å¢å¼ºç‰ˆåŸå§‹è·¯å¾„çŠ¶æ€å·²è®°å½•: {paths_count}æ¡è·¯å¾„, {total_nodes}ä¸ªèŠ‚ç‚¹")
    
    def _get_enhanced_professional_config_by_mode(self, design_mode: str) -> Dict:
        """æ ¹æ®è®¾è®¡æ¨¡å¼è·å–å¢å¼ºç‰ˆä¸“ä¸šé…ç½®"""
        mode_configs = {
            'professional': {
                'enable_enhanced_curve_fitting': True,
                'curve_fitting_quality_threshold': 0.8,
                'force_vehicle_dynamics': True,
                'prefer_complete_curve': True,
                'enable_endpoint_clustering': True,  # âœ¨ å¯ç”¨ç«¯ç‚¹èšç±»
                'endpoint_clustering_radius': 2.0,    # âœ¨ ç«¯ç‚¹èšç±»åŠå¾„
                'vehicle_dynamics': {
                    'turning_radius': 8.0,
                    'max_grade': 0.12,  # æ›´ä¸¥æ ¼
                    'safety_margin': 2.0
                },
                'clustering_rounds': [
                    {'radius': 8.0, 'name': 'ç¬¬ä¸€è½®'},
                    {'radius': 6.0, 'name': 'ç¬¬äºŒè½®'},
                    {'radius': 3.0, 'name': 'ç¬¬ä¸‰è½®'}
                ]
            },
            'balanced': {
                'enable_enhanced_curve_fitting': True,
                'curve_fitting_quality_threshold': 0.7,
                'force_vehicle_dynamics': True,
                'prefer_complete_curve': True,
                'enable_endpoint_clustering': True,  # âœ¨ å¯ç”¨ç«¯ç‚¹èšç±»
                'endpoint_clustering_radius': 2.0,    # âœ¨ ç«¯ç‚¹èšç±»åŠå¾„
                'vehicle_dynamics': {
                    'turning_radius': 8.0,
                    'max_grade': 0.15,
                    'safety_margin': 1.5
                },
                'clustering_rounds': [
                    {'radius': 6.0, 'name': 'ç¬¬ä¸€è½®'},
                    {'radius': 6.0, 'name': 'ç¬¬äºŒè½®'},
                    {'radius': 3.0, 'name': 'ç¬¬ä¸‰è½®'}
                ]
            },
            'performance': {
                'enable_enhanced_curve_fitting': True,
                'curve_fitting_quality_threshold': 0.6,
                'force_vehicle_dynamics': True,
                'prefer_complete_curve': False,
                'enable_endpoint_clustering': True,  # âœ¨ å¯ç”¨ç«¯ç‚¹èšç±»
                'endpoint_clustering_radius': 2.5,    # âœ¨ ç«¯ç‚¹èšç±»åŠå¾„ï¼ˆæ›´å¤§ï¼‰
                'vehicle_dynamics': {
                    'turning_radius': 7.0,
                    'max_grade': 0.18,
                    'safety_margin': 1.2
                },
                'clustering_rounds': [
                    {'radius': 6.0, 'name': 'ç¬¬ä¸€è½®'},
                    {'radius': 3.0, 'name': 'ç¬¬äºŒè½®'}
                ]
            }
        }
        
        config = mode_configs.get(design_mode, mode_configs['balanced'])
        
        # æ·»åŠ é€šç”¨é…ç½®
        config.update({
            'protect_endpoints': True,
            'endpoint_buffer_radius': 3.0,
            'min_cluster_size': 1,
            'importance_threshold': 1.5,
            'preserve_original_on_failure': True,
            'enable_quality_reporting': True,
        })
        
        return config
    
    def _update_enhanced_consolidation_stats(self, consolidation_time: float):
        """æ›´æ–°å¢å¼ºç‰ˆæ•´åˆç»Ÿè®¡ä¿¡æ¯"""
        if not self.professional_consolidator:
            return
        
        # è·å–æ•´åˆç»Ÿè®¡
        if hasattr(self.professional_consolidator, 'get_consolidation_stats'):
            consolidation_stats = self.professional_consolidator.get_consolidation_stats()
            
            self.stats.update({
                'enhanced_consolidation_applied': True,
                'consolidated_paths_count': len(self.bidirectional_paths),
                'consolidation_time': consolidation_time,
                'enhanced_professional_design_applied': True,
                'key_nodes_count': consolidation_stats.get('key_nodes_count', 0),
                'node_reduction_ratio': consolidation_stats.get('node_reduction_ratio', 0.0),
                'reconstruction_success_rate': consolidation_stats.get('reconstruction_success_rate', 0.0),
            })
            
            # å¢å¼ºç‰ˆç‰¹æœ‰ç»Ÿè®¡
            if ENHANCED_PROFESSIONAL_CONSOLIDATION_AVAILABLE:
                self.stats.update({
                    'enhanced_curve_fitting_success': consolidation_stats.get('enhanced_curve_fitting_used', 0),
                    'complete_curve_success': consolidation_stats.get('complete_curve_success', 0),
                    'segmented_curve_success': consolidation_stats.get('segmented_curve_success', 0),
                    'fallback_reconstruction': consolidation_stats.get('fallback_reconstruction', 0),
                    'avg_curve_quality': consolidation_stats.get('avg_curve_quality', 0.0),
                    'dynamics_compliance_rate': consolidation_stats.get('dynamics_compliance_rate', 0.0),
                    'turning_radius_violations': consolidation_stats.get('turning_radius_violations', 0),
                    'grade_violations': consolidation_stats.get('grade_violations', 0),
                })
        
        # å­˜å‚¨ä¸“ä¸šè®¾è®¡ä¿¡æ¯
        self.professional_design_info = {
            'consolidation_stats': consolidation_stats if 'consolidation_stats' in locals() else {},
            'is_enhanced_professional_design': ENHANCED_PROFESSIONAL_CONSOLIDATION_AVAILABLE,
            'design_type': 'enhanced_node_clustering_professional_consolidation',
            'consolidation_time': consolidation_time,
            'design_mode': self.config['enhanced_design_mode'],
        }
        
        # è·å–è¯¦ç»†ä¿¡æ¯
        if hasattr(self.professional_consolidator, 'get_key_nodes_info'):
            self.professional_design_info['key_nodes_info'] = self.professional_consolidator.get_key_nodes_info()
        
        if hasattr(self.professional_consolidator, 'get_consolidated_paths_info'):
            self.professional_design_info['consolidated_paths_info'] = self.professional_consolidator.get_consolidated_paths_info()
        
        # âœ¨ è·å–ç«¯ç‚¹èšç±»ä¿¡æ¯
        if hasattr(self.professional_consolidator, 'get_endpoint_clustering_info'):
            self.professional_design_info['endpoint_clustering_info'] = self.professional_consolidator.get_endpoint_clustering_info()
        
        # æ ‡è®°æ‰€æœ‰è·¯å¾„ä¸ºå¢å¼ºä¸“ä¸šè®¾è®¡
        for path_data in self.bidirectional_paths.values():
            path_data.is_professional_design = True
            path_data.is_optimized_design = True
            path_data.consolidation_level = "enhanced_node_clustering_professional"
            if hasattr(path_data, 'key_nodes'):
                path_data.is_node_clustered = True
                path_data.node_reduction_ratio = consolidation_stats.get('node_reduction_ratio', 0.0)
        
        self.professional_design_applied = True
        
        print(f"ğŸ“Š å¢å¼ºç‰ˆæ•´åˆç»Ÿè®¡å·²æ›´æ–°")
        if ENHANCED_PROFESSIONAL_CONSOLIDATION_AVAILABLE:
            print(f"   åŸå§‹èŠ‚ç‚¹: {consolidation_stats.get('original_nodes_count', 0)} -> å…³é”®èŠ‚ç‚¹: {consolidation_stats.get('key_nodes_count', 0)}")
            print(f"   èŠ‚ç‚¹å‡å°‘: {consolidation_stats.get('node_reduction_ratio', 0.0):.1%}")
            print(f"   âœ¨ ç«¯ç‚¹å‡å°‘: {consolidation_stats.get('endpoint_reduction_ratio', 0.0):.1%}")
            print(f"   å¢å¼ºæ‹ŸåˆæˆåŠŸ: {consolidation_stats.get('enhanced_curve_fitting_used', 0)} æ¬¡")
            print(f"   åŠ¨åŠ›å­¦åˆè§„ç‡: {consolidation_stats.get('dynamics_compliance_rate', 0.0):.1%}")
    
    # ==================== ä¿æŒåŸæœ‰æ–¹æ³•ï¼ˆå‘åå…¼å®¹ï¼‰ ====================
    
    def _load_special_points(self):
        """åŠ è½½ç‰¹æ®Šç‚¹ - ä¿æŒåŸæœ‰å®ç°"""
        # è£…è½½ç‚¹
        self.special_points['loading'] = []
        for i, point in enumerate(self.env.loading_points):
            self.special_points['loading'].append({
                'id': i, 'type': 'loading', 'position': self._ensure_3d_point(point)
            })
        
        # å¸è½½ç‚¹
        self.special_points['unloading'] = []
        for i, point in enumerate(self.env.unloading_points):
            self.special_points['unloading'].append({
                'id': i, 'type': 'unloading', 'position': self._ensure_3d_point(point)
            })
        
        # åœè½¦ç‚¹
        self.special_points['parking'] = []
        parking_areas = getattr(self.env, 'parking_areas', [])
        for i, point in enumerate(parking_areas):
            self.special_points['parking'].append({
                'id': i, 'type': 'parking', 'position': self._ensure_3d_point(point)
            })
        
        print(f"åŠ è½½ç‰¹æ®Šç‚¹: è£…è½½{len(self.special_points['loading'])}ä¸ª, "
              f"å¸è½½{len(self.special_points['unloading'])}ä¸ª, "
              f"åœè½¦{len(self.special_points['parking'])}ä¸ª")
    
    def _validate_special_points(self) -> bool:
        """éªŒè¯ç‰¹æ®Šç‚¹"""
        if not self.special_points['loading'] or not self.special_points['unloading']:
            print("âŒ ç¼ºå°‘å¿…è¦çš„è£…è½½ç‚¹æˆ–å¸è½½ç‚¹")
            return False
        return True
    
    def _generate_complete_bidirectional_paths(self) -> int:
        """ç”Ÿæˆå®Œæ•´çš„åŒå‘è·¯å¾„ç»„åˆ - ä¿æŒåŸæœ‰å®ç°ä½†ç®€åŒ–"""
        if not self.path_planner:
            print("âŒ æœªè®¾ç½®è·¯å¾„è§„åˆ’å™¨")
            return 0
        
        success_count = 0
        path_pairs = []
        
        # å®šä¹‰éœ€è¦è¿æ¥çš„ç‚¹ç±»å‹ç»„åˆ
        connection_types = [
            ('loading', 'unloading'),
            ('loading', 'parking'),
            ('unloading', 'parking')
        ]
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦è¿æ¥çš„ç‚¹å¯¹
        for type_a, type_b in connection_types:
            if (len(self.special_points[type_a]) == 0 or 
                len(self.special_points[type_b]) == 0):
                continue
                
            for point_a in self.special_points[type_a]:
                for point_b in self.special_points[type_b]:
                    path_pairs.append((point_a, point_b, f"{type_a}_to_{type_b}"))
        
        self.stats['total_path_pairs'] = len(path_pairs)
        print(f"éœ€è¦ç”Ÿæˆ {len(path_pairs)} æ¡åŒå‘è·¯å¾„")
        
        # ç”Ÿæˆæ¯æ¡åŒå‘è·¯å¾„ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        for i, (point_a, point_b, connection_type) in enumerate(path_pairs, 1):
            print(f"[{i}/{len(path_pairs)}] ç”Ÿæˆè·¯å¾„: {point_a['type'][0].upper()}{point_a['id']} â†” {point_b['type'][0].upper()}{point_b['id']}")
            
            path_result = self._generate_single_bidirectional_path_simplified(point_a, point_b)
            
            if path_result:
                success_count += 1
                self.stats[connection_type] += 1
                print(f"  âœ… æˆåŠŸ: é•¿åº¦{len(path_result.forward_path)}, è´¨é‡{path_result.quality:.2f}")
            else:
                print(f"  âŒ å¤±è´¥")
        
        return success_count
    
    def _generate_single_bidirectional_path_simplified(self, point_a: Dict, point_b: Dict) -> Optional['BiDirectionalPath']:
        """ç”Ÿæˆå•æ¡åŒå‘è·¯å¾„ - ç®€åŒ–ç‰ˆæœ¬"""
        path_id = f"{point_a['type'][0].upper()}{point_a['id']}_to_{point_b['type'][0].upper()}{point_b['id']}"
        
        start_pos = point_a['position']
        end_pos = point_b['position']
        
        # å°è¯•ä½¿ç”¨è·¯å¾„è§„åˆ’å™¨
        try:
            result = self.path_planner.plan_path(
                vehicle_id=f"backbone_{path_id}",
                start=start_pos,
                goal=end_pos,
                use_backbone=False,
                context='backbone',
                return_object=True
            )
            
            if result and hasattr(result, 'path') and result.path:
                forward_path = result.path
                reverse_path = self._reverse_path(forward_path)
                quality = getattr(result, 'quality_score', 0.7)
                planner_used = getattr(result, 'planner_used', 'unknown')
                
                # æ›´æ–°ç»Ÿè®¡
                if 'astar' in planner_used.lower():
                    self.stats['astar_success'] += 1
                elif 'rrt' in planner_used.lower():
                    self.stats['rrt_success'] += 1
                
            else:
                # å›é€€åˆ°ç›´çº¿è·¯å¾„
                forward_path = self._create_direct_path(start_pos, end_pos)
                reverse_path = self._reverse_path(forward_path)
                quality = 0.5
                planner_used = 'direct_fallback'
                self.stats['direct_fallback'] += 1
        
        except Exception as e:
            print(f"  è·¯å¾„è§„åˆ’å¼‚å¸¸: {e}")
            # å›é€€åˆ°ç›´çº¿è·¯å¾„
            forward_path = self._create_direct_path(start_pos, end_pos)
            reverse_path = self._reverse_path(forward_path)
            quality = 0.4
            planner_used = 'direct_fallback'
            self.stats['direct_fallback'] += 1
        
        if forward_path and len(forward_path) >= 2:
            bidirectional_path = BiDirectionalPath(
                path_id=path_id,
                point_a=point_a,
                point_b=point_b,
                forward_path=forward_path,
                reverse_path=reverse_path,
                length=self._calculate_path_length(forward_path),
                quality=quality,
                planner_used=planner_used,
                created_time=time.time()
            )
            
            self.bidirectional_paths[path_id] = bidirectional_path
            return bidirectional_path
        
        return None
    
    def _create_direct_path(self, start: Tuple, end: Tuple) -> List[Tuple]:
        """åˆ›å»ºç›´çº¿è·¯å¾„"""
        distance = math.sqrt((end[0] - start[0])**2 + (end[1] - start[1])**2)
        steps = max(3, int(distance / 2.0))
        
        path = []
        for i in range(steps + 1):
            t = i / steps
            x = start[0] + t * (end[0] - start[0])
            y = start[1] + t * (end[1] - start[1])
            z = start[2] if len(start) > 2 else 0
            theta = math.atan2(end[1] - start[1], end[0] - start[0])
            path.append((x, y, z, theta))
        
        return path
    
    def _reverse_path(self, path: List[Tuple]) -> List[Tuple]:
        """åè½¬è·¯å¾„æ–¹å‘"""
        if not path:
            return []
        
        reversed_path = []
        for point in reversed(path):
            if len(point) >= 3:
                x, y, theta = point[0], point[1], point[2]
                reverse_theta = (theta + math.pi) % (2 * math.pi)
                reversed_path.append((x, y, reverse_theta))
            else:
                reversed_path.append(point)
        
        return reversed_path
    
    def _generate_safe_interfaces(self) -> int:
        """ä¸ºåŒå‘è·¯å¾„ç”Ÿæˆå®‰å…¨æ¥å£"""
        total_interfaces = 0
        spacing = self.config['interface_spacing']
        
        for path_id, path_data in self.bidirectional_paths.items():
            forward_path = path_data.forward_path
            
            if len(forward_path) < 2:
                continue
            
            interface_count = 0
            
            # åœ¨è·¯å¾„ä¸Šç­‰é—´è·ç”Ÿæˆæ¥å£
            for i in range(0, len(forward_path), spacing):
                if i >= len(forward_path):
                    break
                
                interface_id = f"{path_id}_if_{interface_count}"
                
                self.backbone_interfaces[interface_id] = {
                    'id': interface_id,
                    'position': forward_path[i],
                    'path_id': path_id,
                    'path_index': i,
                    'is_occupied': False,
                    'reservation_count': 0,
                    'usage_history': [],
                    'consolidation_level': getattr(path_data, 'consolidation_level', 'original'),
                    'is_professional_design': getattr(path_data, 'is_professional_design', False),
                    'spacing_used': spacing
                }
                
                self.path_interfaces[path_id].append(interface_id)
                interface_count += 1
                total_interfaces += 1
        
        return total_interfaces
    
    def _build_connection_index(self):
        """å»ºç«‹è¿æ¥ç´¢å¼•"""
        self.connection_index.clear()
        
        for path_id, path_data in self.bidirectional_paths.items():
            point_a = path_data.point_a
            point_b = path_data.point_b
            
            # åŒå‘ç´¢å¼•
            key_ab = (point_a['type'], point_a['id'], point_b['type'], point_b['id'])
            key_ba = (point_b['type'], point_b['id'], point_a['type'], point_a['id'])
            
            self.connection_index[key_ab] = path_id
            self.connection_index[key_ba] = path_id
    
    def _initialize_quality_tracking(self):
        """åˆå§‹åŒ–è´¨é‡è¿½è¸ª"""
        for path_data in self.bidirectional_paths.values():
            if hasattr(path_data, 'update_quality_history'):
                path_data.update_quality_history(path_data.quality)
    
    def _calculate_distance(self, p1: Tuple, p2: Tuple) -> float:
        """è®¡ç®—ä¸¤ç‚¹é—´è·ç¦»"""
        return math.sqrt((p2[0] - p1[0])**2 + (p2[1] - p1[1])**2)
    
    def _calculate_path_length(self, path: List[Tuple]) -> float:
        """è®¡ç®—è·¯å¾„æ€»é•¿åº¦"""
        if not path or len(path) < 2:
            return 0.0
        
        length = 0.0
        for i in range(len(path) - 1):
            length += self._calculate_distance(path[i], path[i + 1])
        return length
    
    def get_network_status(self) -> Dict:
        """è·å–ç½‘ç»œçŠ¶æ€"""
        return {
            'bidirectional_paths': len(self.bidirectional_paths),
            'total_interfaces': len(self.backbone_interfaces),
            'generation_stats': self.stats,
            'special_points': {
                'loading': len(self.special_points['loading']),
                'unloading': len(self.special_points['unloading']),
                'parking': len(self.special_points['parking'])
            },
            'enhanced_features': {
                'enhanced_consolidation_available': ENHANCED_PROFESSIONAL_CONSOLIDATION_AVAILABLE,
                'enhanced_consolidation_applied': self.stats['enhanced_consolidation_applied']
            }
        }
    
    def set_path_planner(self, path_planner):
        """è®¾ç½®è·¯å¾„è§„åˆ’å™¨"""
        self.path_planner = path_planner
        print("âœ… å·²è®¾ç½®è·¯å¾„è§„åˆ’å™¨")
    
    def _ensure_3d_point(self, point) -> Tuple[float, float, float]:
        """ç¡®ä¿ç‚¹åæ ‡ä¸º3D"""
        if not point:
            return (0.0, 0.0, 0.0)
        elif len(point) >= 3:
            return (float(point[0]), float(point[1]), float(point[2]))
        elif len(point) == 2:
            return (float(point[0]), float(point[1]), 0.0)
        else:
            return (0.0, 0.0, 0.0)


# ==================== ä¾¿æ·å‡½æ•° ====================

def create_enhanced_backbone_network(env, config_mode='balanced'):
    """åˆ›å»ºå¢å¼ºç‰ˆéª¨å¹²ç½‘ç»œ"""
    network = OptimizedBackboneNetwork(env)
    
    # è®¾ç½®å¢å¼ºé…ç½®
    if config_mode == 'professional':
        network.config.update({
            'enhanced_design_mode': 'professional',
            'curve_fitting_quality_threshold': 0.8,
            'force_vehicle_dynamics': True
        })
    elif config_mode == 'performance':
        network.config.update({
            'enhanced_design_mode': 'performance',
            'curve_fitting_quality_threshold': 0.6,
            'auto_consolidate_after_generation': True
        })
    
    return network

# å‘åå…¼å®¹æ€§
SimplifiedBackboneNetwork = OptimizedBackboneNetwork